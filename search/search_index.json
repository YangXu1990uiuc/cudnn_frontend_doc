{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to cuDNN Frontend","text":""},{"location":"#what-is-cudnn-frontend","title":"What is cuDNN Frontend?","text":"<p>cuDNN Frontend is your gateway to GPU-accelerated deep learning. It provides a simple, intuitive interface to harness the power of NVIDIA's cuDNN library - the same technology powering the world's fastest AI systems.</p> <p>Think of it like this</p> <p>If cuDNN is the engine of a race car, cuDNN Frontend is the steering wheel and dashboard that lets you drive it effortlessly.</p>"},{"location":"#why-should-you-care","title":"Why Should You Care?","text":"Without cuDNN Frontend With cuDNN Frontend Write hundreds of lines of boilerplate code Write 10-20 lines of clean code Manually manage tensor layouts and memory Automatic layout inference Figure out optimal algorithms yourself Built-in autotuning Debug cryptic error messages Clear, actionable feedback"},{"location":"#the-big-picture","title":"The Big Picture","text":"<pre><code>graph LR\n    A[Your Code] --&gt; B[cuDNN Frontend]\n    B --&gt; C[cuDNN Backend]\n    C --&gt; D[NVIDIA GPU]\n\n    style A fill:#e1f5fe\n    style B fill:#c8e6c9\n    style C fill:#fff9c4\n    style D fill:#76ff03</code></pre> <p>cuDNN Frontend sits between your code and the powerful cuDNN backend, translating your high-level intentions into optimized GPU operations.</p>"},{"location":"#key-features-at-a-glance","title":"Key Features at a Glance","text":"<ul> <li> <p> Graph-Based API</p> <p>Build once, run many times. Describe your operations as a graph and let cuDNN optimize execution.</p> </li> <li> <p> Python &amp; C++ Support</p> <p>Choose your language. Both APIs are functionally equivalent and easy to use.</p> </li> <li> <p> Automatic Optimization</p> <p>Built-in heuristics and autotuning find the fastest implementation for your hardware.</p> </li> <li> <p> Smart Memory Management</p> <p>Automatic workspace allocation and tensor management.</p> </li> </ul>"},{"location":"#who-is-this-guide-for","title":"Who Is This Guide For?","text":"<p>This guide is designed for:</p> <ul> <li>ML Engineers who want to squeeze every drop of performance from their models</li> <li>Researchers exploring custom operations and architectures</li> <li>Developers building production AI systems</li> <li>Students learning about GPU computing and deep learning internals</li> </ul> <p>Prerequisites</p> <ul> <li>Basic Python or C++ knowledge</li> <li>Understanding of basic deep learning concepts (tensors, convolutions, attention)</li> <li>Access to an NVIDIA GPU (Ampere or newer recommended)</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Here's a taste of how simple cuDNN Frontend can be:</p> PythonC++ <pre><code>import cudnn\nimport torch\n\n# Create input tensors\nx = torch.randn(8, 64, 56, 56, device=\"cuda\", dtype=torch.float16)\nw = torch.randn(32, 64, 3, 3, device=\"cuda\", dtype=torch.float16)\n\n# Build and execute a convolution graph\nwith cudnn.Graph() as graph:\n    output = graph.conv_fprop(image=x, weight=w, padding=[1,1])\n    output.set_output(True)\n\nresult = graph(x, w)  # That's it!\n</code></pre> <pre><code>#include &lt;cudnn_frontend.h&gt;\n\n// Create a graph\nauto graph = cudnn_frontend::graph::Graph();\ngraph.set_io_data_type(cudnn::DataType_t::HALF);\n\n// Add convolution operation\nauto conv = graph.conv_fprop(X, W, Conv_options);\nconv-&gt;set_output(true);\n\n// Build and execute\ngraph.build();\ngraph.execute(handle, variant_pack, workspace);\n</code></pre>"},{"location":"#ready-to-start","title":"Ready to Start?","text":"<ul> <li> <p> Getting Started</p> <p>New to cuDNN Frontend? Start here!</p> </li> <li> <p> Core Concepts</p> <p>Understand the fundamentals.</p> </li> <li> <p> Tutorials</p> <p>Learn by building real operations.</p> </li> </ul> <p>cuDNN Frontend is open source. View on GitHub</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Common questions and answers about cuDNN Frontend.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-the-difference-between-cudnn-and-cudnn-frontend","title":"What is the difference between cuDNN and cuDNN Frontend?","text":"<p>cuDNN is NVIDIA's core library with highly optimized GPU kernels for deep learning operations (convolutions, attention, normalization, etc.).</p> <p>cuDNN Frontend is a modern API layer that makes cuDNN easier to use:</p> Aspect cuDNN (Backend) cuDNN Frontend API style Descriptor-based C API Graph-based Python/C++ Code required Hundreds of lines Tens of lines Optimization Manual Automatic Learning curve Steep Gentle"},{"location":"faq/#when-should-i-use-cudnn-frontend-directly-vs-pytorch","title":"When should I use cuDNN Frontend directly vs PyTorch?","text":"Scenario Recommendation Standard models (ResNet, BERT) Use PyTorch - it uses cuDNN internally Custom fused operations Use cuDNN Frontend directly Maximum performance tuning Use cuDNN Frontend with autotuning Production inference systems Consider cuDNN Frontend Exotic architectures Use cuDNN Frontend"},{"location":"faq/#what-gpu-architectures-are-supported","title":"What GPU architectures are supported?","text":"Architecture GPUs SDPA Support Volta (SM70) V100 No Turing (SM75) RTX 20xx, T4 No Ampere (SM80) A100, RTX 30xx Yes Ada (SM89) RTX 40xx Yes Hopper (SM90) H100 Yes (+ FP8) Blackwell (SM100) B100, B200, B300, RTX 50xx Yes <p>SDPA (Scaled Dot-Product Attention) requires Ampere or newer.</p>"},{"location":"faq/#version-compatibility","title":"Version Compatibility","text":"<p>Critical for avoiding issues:</p> cuDNN Frontend cuDNN Backend CUDA PyTorch 1.0.x 8.9+ 11.8+ 2.0+ 1.1.x 9.0+ 12.0+ 2.1+ 1.2.x 9.1+ 12.1+ 2.2+ <p>Check your versions:</p> <pre><code>import torch\nimport cudnn\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.version.cuda}\")\nprint(f\"cuDNN (PyTorch): {torch.backends.cudnn.version()}\")\nprint(f\"cuDNN Backend: {cudnn.backend_version()}\")  # e.g., 90300 = 9.3.0\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"Compute Capability: {torch.cuda.get_device_capability()}\")\n</code></pre> <p>Version Mismatches</p> <p>Most cuDNN errors come from version mismatches. Always verify compatibility!</p>"},{"location":"faq/#installation-issues","title":"Installation Issues","text":""},{"location":"faq/#cuda-not-available-error","title":"\"CUDA not available\" error","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.cuda.is_available()\nFalse\n</code></pre> <p>Solutions:</p> <ol> <li>Check if GPU is recognized: <code>nvidia-smi</code></li> <li>Install CUDA-enabled PyTorch:    <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu126\n</code></pre></li> <li>Verify CUDA toolkit: <code>nvcc --version</code></li> </ol>"},{"location":"faq/#cudnn-version-mismatch-error","title":"\"cuDNN version mismatch\" error","text":"<p>Solution: Install matching versions:</p> <pre><code>pip install nvidia-cudnn-cu12==9.0.0\npip install nvidia-cudnn-frontend\n</code></pre>"},{"location":"faq/#import-error-no-module-named-cudnn","title":"Import error: \"No module named 'cudnn'\"","text":"<p>Solution:</p> <pre><code>pip uninstall nvidia-cudnn-frontend\npip install nvidia-cudnn-frontend\n</code></pre>"},{"location":"faq/#graph-building-issues","title":"Graph Building Issues","text":""},{"location":"faq/#no-execution-plan-found-error","title":"\"No execution plan found\" error","text":"<p>This means cuDNN can't find an algorithm for your configuration.</p> <p>Common causes and solutions:</p> <ol> <li>GPU too old: Check compute capability (need SM70+, SM80+ for SDPA)</li> <li>Invalid tensor dimensions: Ensure shapes are valid</li> <li>Unsupported data type combination: Use supported types (HALF, BFLOAT16, FLOAT)</li> </ol> <pre><code># Check GPU capability\nimport torch\nprint(torch.cuda.get_device_capability())  # Should be (8, 0) or higher for SDPA\n</code></pre>"},{"location":"faq/#tensor-dimensions-mismatch-error","title":"\"Tensor dimensions mismatch\" error","text":"<p>Solution: Ensure tensors match what you used when building the graph:</p> <pre><code># Graph was built with shape [8, 64, 56, 56]\n# Execution must use the SAME shape\nx = torch.randn(8, 64, 56, 56, ...)  # Correct\nx = torch.randn(16, 64, 56, 56, ...)  # Wrong - different batch size\n</code></pre>"},{"location":"faq/#forgot-to-call-set_outputtrue","title":"Forgot to call <code>set_output(True)</code>","text":"<pre><code># WRONG\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w)\n    # Missing: y.set_output(True)\n\n# CORRECT\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w)\n    y.set_output(True)  # Required!\n</code></pre>"},{"location":"faq/#performance-issues","title":"Performance Issues","text":""},{"location":"faq/#slow-first-execution","title":"Slow first execution","text":"<p>This is normal! cuDNN compiles and optimizes the graph on first run.</p> <p>Solution: Warm up before timing:</p> <pre><code># Warmup\nfor _ in range(3):\n    _ = graph(x, w, handle=handle)\ntorch.cuda.synchronize()\n\n# Now time it\nstart = time.time()\nfor _ in range(100):\n    _ = graph(x, w, handle=handle)\ntorch.cuda.synchronize()\nprint(f\"Time: {(time.time()-start)/100*1000:.2f} ms\")\n</code></pre>"},{"location":"faq/#performance-worse-than-expected","title":"Performance worse than expected","text":"<p>Checklist:</p> <ol> <li>\u2705 Using channels-last layout? (<code>.to(memory_format=torch.channels_last)</code>)</li> <li>\u2705 Using FP16/BF16 for I/O?</li> <li>\u2705 Batch size large enough? (&gt;= 32 recommended)</li> <li>\u2705 Tensor dimensions aligned? (multiples of 8 for Tensor Cores)</li> <li>\u2705 Graph reused, not rebuilt each iteration?</li> </ol>"},{"location":"faq/#memory-usage-too-high","title":"Memory usage too high","text":"<p>Solutions:</p> <ol> <li>Use virtual tensors for intermediates (don't mark all tensors as output)</li> <li>Reduce batch size</li> <li>Use lower precision (FP16 instead of FP32)</li> <li>Limit workspace size:    <pre><code>graph.create_execution_plans(\n    [cudnn.heur_mode.A],\n    max_workspace_size=256 * 1024 * 1024  # 256MB limit\n)\n</code></pre></li> </ol>"},{"location":"faq/#sdpa-specific-issues","title":"SDPA-Specific Issues","text":""},{"location":"faq/#sdpa-requires-sm80-or-higher","title":"\"SDPA requires SM80 or higher\"","text":"<p>SDPA (scaled dot-product attention) requires Ampere GPU or newer.</p> <p>Check your GPU: <pre><code>print(torch.cuda.get_device_capability())\n# Needs to be (8, 0) or higher\n</code></pre></p>"},{"location":"faq/#wrong-output-shape-from-sdpa","title":"Wrong output shape from SDPA","text":"<p>Must set output dimensions:</p> <pre><code>output, _ = graph.sdpa(q=Q, k=K, v=V, ...)\noutput.set_output(True)\noutput.set_dim(Q.shape)      # Required!\noutput.set_stride(Q.stride()) # Required!\n</code></pre>"},{"location":"faq/#numerical-differences-with-pytorch","title":"Numerical differences with PyTorch","text":"<p>Small differences (1e-2 to 1e-3) are expected due to: - Different algorithm implementations - Different accumulation order - Mixed precision effects</p> <pre><code># Use appropriate tolerances\ntorch.testing.assert_close(result, reference, atol=5e-3, rtol=3e-3)\n</code></pre>"},{"location":"faq/#training-issues","title":"Training Issues","text":""},{"location":"faq/#need-gradients-for-backward-pass","title":"Need gradients for backward pass","text":"<p>For training, set <code>is_inference=False</code> and <code>generate_stats=True</code>:</p> <pre><code># Forward\noutput, stats = graph.sdpa(\n    q=Q, k=K, v=V,\n    is_inference=False,   # Training mode\n    generate_stats=True,  # Save for backward\n)\nstats.set_output(True)  # Save stats!\n</code></pre>"},{"location":"faq/#backward-pass-errors","title":"Backward pass errors","text":"<p>Make sure to: 1. Save stats from forward pass 2. Use same parameters in backward as forward 3. Set output on all gradient tensors</p>"},{"location":"faq/#debugging-tips","title":"Debugging Tips","text":""},{"location":"faq/#enable-logging","title":"Enable logging","text":"<pre><code>export CUDNN_FRONTEND_LOG_INFO=1\nexport CUDNN_FRONTEND_LOG_FILE=stdout\n</code></pre>"},{"location":"faq/#print-graph-info","title":"Print graph info","text":"<pre><code>print(graph)  # Shows graph structure after building\n</code></pre>"},{"location":"faq/#check-workspace-size","title":"Check workspace size","text":"<pre><code>workspace_size = graph.get_workspace_size()\nprint(f\"Workspace needed: {workspace_size / 1024**2:.1f} MB\")\n</code></pre>"},{"location":"faq/#verify-tensor-properties","title":"Verify tensor properties","text":"<pre><code>print(f\"Shape: {x.shape}\")\nprint(f\"Stride: {x.stride()}\")\nprint(f\"Dtype: {x.dtype}\")\nprint(f\"Device: {x.device}\")\nprint(f\"Contiguous: {x.is_contiguous()}\")\nprint(f\"Channels-last: {x.is_contiguous(memory_format=torch.channels_last)}\")\n</code></pre>"},{"location":"faq/#getting-more-help","title":"Getting More Help","text":"<ol> <li>GitHub Issues: github.com/NVIDIA/cudnn-frontend/issues</li> <li>NVIDIA Forums: forums.developer.nvidia.com</li> <li>cuDNN Documentation: docs.nvidia.com/deeplearning/cudnn</li> </ol> <p>When reporting issues, include:</p> <ul> <li>GPU model (<code>nvidia-smi</code>)</li> <li>cuDNN version (<code>cudnn.backend_version()</code>)</li> <li>PyTorch version</li> <li>Minimal reproducible code</li> <li>Full error message</li> </ul>"},{"location":"api-reference/cpp/","title":"C++ API Reference","text":"<p>This reference covers the cuDNN Graph API for C++ - the modern, header-only API for high-performance deep learning. The C++ API is functionally equivalent to Python and is ideal for production inference systems.</p> <p>Graph API Focus</p> <p>This documentation covers the Graph API (v1.x). The legacy descriptor-based API is not covered here.</p>"},{"location":"api-reference/cpp/#setup","title":"Setup","text":"<p>cuDNN Frontend for C++ is header-only:</p> <pre><code>#include &lt;cudnn_frontend.h&gt;\n\nnamespace fe = cudnn_frontend;\n</code></pre>"},{"location":"api-reference/cpp/#core-classes","title":"Core Classes","text":""},{"location":"api-reference/cpp/#cudnn_frontendgraphgraph","title":"cudnn_frontend::graph::Graph","text":"<p>The primary class for building computation graphs.</p> <pre><code>namespace cudnn_frontend::graph {\n\nclass Graph {\npublic:\n    // Constructor with optional name\n    Graph(std::string name = \"\");\n\n    // Set global data types\n    Graph&amp; set_io_data_type(DataType_t dtype);\n    Graph&amp; set_intermediate_data_type(DataType_t dtype);\n    Graph&amp; set_compute_data_type(DataType_t dtype);\n\n    // Create tensors\n    std::shared_ptr&lt;Tensor&gt; tensor(TensorBuilder const&amp;);\n\n    // Operations (see below)\n\n    // Build and execution\n    error_t validate();\n    error_t build_operation_graph(cudnnHandle_t handle);\n    error_t create_execution_plans(std::vector&lt;HeurMode_t&gt; const&amp; modes);\n    error_t check_support(cudnnHandle_t handle);\n    error_t build_plans(cudnnHandle_t handle);\n    error_t execute(cudnnHandle_t handle,\n                    VariantPack const&amp; variant_pack,\n                    void* workspace);\n\n    // Utilities\n    int64_t get_workspace_size() const;\n    error_t serialize(std::vector&lt;uint8_t&gt;&amp; data) const;\n    static Graph deserialize(std::vector&lt;uint8_t&gt; const&amp; data);\n};\n\n}  // namespace cudnn_frontend::graph\n</code></pre>"},{"location":"api-reference/cpp/#basic-usage","title":"Basic Usage","text":"<pre><code>#include &lt;cudnn_frontend.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\nnamespace fe = cudnn_frontend;\n\nint main() {\n    // Create cuDNN handle\n    cudnnHandle_t handle;\n    cudnnCreate(&amp;handle);\n\n    // Create graph\n    auto graph = fe::graph::Graph();\n    graph.set_io_data_type(fe::DataType_t::HALF)\n         .set_compute_data_type(fe::DataType_t::FLOAT);\n\n    // Create input tensors\n    auto X = graph.tensor(fe::graph::Tensor_attributes()\n        .set_name(\"X\")\n        .set_dim({8, 64, 56, 56})\n        .set_stride({64*56*56, 1, 64*56, 64})  // NHWC\n        .set_data_type(fe::DataType_t::HALF));\n\n    auto W = graph.tensor(fe::graph::Tensor_attributes()\n        .set_name(\"W\")\n        .set_dim({128, 64, 3, 3})\n        .set_stride({64*3*3, 1, 64*3, 64})\n        .set_data_type(fe::DataType_t::HALF));\n\n    // Add convolution operation\n    auto conv_options = fe::graph::Conv_fprop_attributes()\n        .set_padding({1, 1})\n        .set_stride({1, 1})\n        .set_dilation({1, 1});\n\n    auto Y = graph.conv_fprop(X, W, conv_options);\n    Y-&gt;set_output(true);\n\n    // Build the graph\n    graph.validate();\n    graph.build_operation_graph(handle);\n    graph.create_execution_plans({fe::HeurMode_t::A, fe::HeurMode_t::FALLBACK});\n    graph.check_support(handle);\n    graph.build_plans(handle);\n\n    // Allocate memory and execute\n    void *x_ptr, *w_ptr, *y_ptr, *workspace;\n    // ... allocate CUDA memory ...\n\n    auto workspace_size = graph.get_workspace_size();\n    cudaMalloc(&amp;workspace, workspace_size);\n\n    std::unordered_map&lt;std::shared_ptr&lt;fe::graph::Tensor&gt;, void*&gt; variant_pack = {\n        {X, x_ptr},\n        {W, w_ptr},\n        {Y, y_ptr}\n    };\n\n    graph.execute(handle, variant_pack, workspace);\n\n    // Cleanup\n    cudnnDestroy(handle);\n    return 0;\n}\n</code></pre>"},{"location":"api-reference/cpp/#data-types","title":"Data Types","text":"<pre><code>enum class DataType_t {\n    FLOAT,       // FP32\n    HALF,        // FP16\n    BFLOAT16,    // BF16\n    FP8_E4M3,    // 8-bit FP (Hopper+)\n    FP8_E5M2,    // 8-bit FP for gradients\n    INT8,        // Quantized\n    INT32,       // Indices\n    INT64,       // Large indices\n    BOOLEAN      // Masks\n};\n</code></pre>"},{"location":"api-reference/cpp/#tensor-attributes","title":"Tensor Attributes","text":"<pre><code>class Tensor_attributes {\npublic:\n    Tensor_attributes&amp; set_name(std::string const&amp; name);\n    Tensor_attributes&amp; set_dim(std::vector&lt;int64_t&gt; const&amp; dim);\n    Tensor_attributes&amp; set_stride(std::vector&lt;int64_t&gt; const&amp; stride);\n    Tensor_attributes&amp; set_data_type(DataType_t dtype);\n    Tensor_attributes&amp; set_is_virtual(bool is_virtual);\n    Tensor_attributes&amp; set_is_pass_by_value(bool is_pass_by_value);\n};\n</code></pre>"},{"location":"api-reference/cpp/#operations","title":"Operations","text":""},{"location":"api-reference/cpp/#scaled-dot-product-attention","title":"Scaled Dot-Product Attention","text":"<p>For LLM/VLM attention layers:</p> <pre><code>class SDPA_attributes {\npublic:\n    SDPA_attributes&amp; set_attn_scale(float scale);\n    SDPA_attributes&amp; set_is_inference(bool is_inference);\n    SDPA_attributes&amp; set_causal_mask(bool use_causal);\n    SDPA_attributes&amp; set_dropout(float probability);\n    SDPA_attributes&amp; set_name(std::string const&amp; name);\n};\n\n// Returns tuple of (output, stats)\nauto [O, Stats] = graph.sdpa(Q, K, V, SDPA_attributes()\n    .set_attn_scale(1.0f / sqrtf(head_dim))\n    .set_is_inference(true)\n    .set_causal_mask(true)\n    .set_name(\"self_attention\"));\n\nO-&gt;set_output(true);\n</code></pre> <p>LLM Inference Example:</p> <pre><code>// Self-attention for LLM inference\nauto create_attention_graph(\n    int batch_size, int num_heads, int seq_len, int head_dim,\n    cudnnHandle_t handle\n) {\n    auto graph = fe::graph::Graph();\n    graph.set_io_data_type(fe::DataType_t::HALF)\n         .set_intermediate_data_type(fe::DataType_t::FLOAT)\n         .set_compute_data_type(fe::DataType_t::FLOAT);\n\n    // Q, K, V: [batch, num_heads, seq_len, head_dim]\n    auto Q = graph.tensor(fe::graph::Tensor_attributes()\n        .set_name(\"Q\")\n        .set_dim({batch_size, num_heads, seq_len, head_dim})\n        .set_stride({num_heads*seq_len*head_dim, seq_len*head_dim, head_dim, 1})\n        .set_data_type(fe::DataType_t::HALF));\n\n    auto K = graph.tensor(fe::graph::Tensor_attributes()\n        .set_name(\"K\")\n        .set_dim({batch_size, num_heads, seq_len, head_dim})\n        .set_stride({num_heads*seq_len*head_dim, seq_len*head_dim, head_dim, 1})\n        .set_data_type(fe::DataType_t::HALF));\n\n    auto V = graph.tensor(fe::graph::Tensor_attributes()\n        .set_name(\"V\")\n        .set_dim({batch_size, num_heads, seq_len, head_dim})\n        .set_stride({num_heads*seq_len*head_dim, seq_len*head_dim, head_dim, 1})\n        .set_data_type(fe::DataType_t::HALF));\n\n    // SDPA with causal mask\n    auto [O, Stats] = graph.sdpa(Q, K, V,\n        fe::graph::SDPA_attributes()\n            .set_attn_scale(1.0f / sqrtf(static_cast&lt;float&gt;(head_dim)))\n            .set_is_inference(true)\n            .set_causal_mask(true));\n\n    O-&gt;set_output(true);\n    O-&gt;set_dim({batch_size, num_heads, seq_len, head_dim});\n    O-&gt;set_stride({num_heads*seq_len*head_dim, seq_len*head_dim, head_dim, 1});\n\n    // Build\n    graph.validate();\n    graph.build_operation_graph(handle);\n    graph.create_execution_plans({fe::HeurMode_t::A});\n    graph.check_support(handle);\n    graph.build_plans(handle);\n\n    return std::make_tuple(graph, Q, K, V, O);\n}\n</code></pre>"},{"location":"api-reference/cpp/#matrix-multiplication","title":"Matrix Multiplication","text":"<pre><code>class Matmul_attributes {\npublic:\n    Matmul_attributes&amp; set_compute_data_type(DataType_t dtype);\n    Matmul_attributes&amp; set_name(std::string const&amp; name);\n};\n\nauto C = graph.matmul(A, B, Matmul_attributes()\n    .set_compute_data_type(fe::DataType_t::FLOAT)\n    .set_name(\"projection\"));\n</code></pre>"},{"location":"api-reference/cpp/#normalization","title":"Normalization","text":""},{"location":"api-reference/cpp/#layer-normalization","title":"Layer Normalization","text":"<pre><code>class Layernorm_attributes {\npublic:\n    Layernorm_attributes&amp; set_epsilon(float eps);\n    Layernorm_attributes&amp; set_name(std::string const&amp; name);\n};\n\nauto [Y, Mean, InvVar] = graph.layernorm(X, Scale, Bias,\n    Layernorm_attributes()\n        .set_epsilon(1e-5f)\n        .set_name(\"pre_attn_norm\"));\n\nY-&gt;set_output(true);\n</code></pre>"},{"location":"api-reference/cpp/#rms-normalization","title":"RMS Normalization","text":"<pre><code>class Rmsnorm_attributes {\npublic:\n    Rmsnorm_attributes&amp; set_epsilon(float eps);\n    Rmsnorm_attributes&amp; set_name(std::string const&amp; name);\n};\n\nauto [Y, InvRms] = graph.rmsnorm(X, Scale,\n    Rmsnorm_attributes()\n        .set_epsilon(1e-5f)\n        .set_name(\"rms_norm\"));\n</code></pre>"},{"location":"api-reference/cpp/#convolution-for-vlm-vision-encoders","title":"Convolution (for VLM vision encoders)","text":"<pre><code>class Conv_fprop_attributes {\npublic:\n    Conv_fprop_attributes&amp; set_padding(std::vector&lt;int64_t&gt; const&amp; padding);\n    Conv_fprop_attributes&amp; set_stride(std::vector&lt;int64_t&gt; const&amp; stride);\n    Conv_fprop_attributes&amp; set_dilation(std::vector&lt;int64_t&gt; const&amp; dilation);\n    Conv_fprop_attributes&amp; set_name(std::string const&amp; name);\n};\n\nauto Y = graph.conv_fprop(X, W, Conv_fprop_attributes()\n    .set_padding({1, 1})\n    .set_stride({1, 1})\n    .set_dilation({1, 1})\n    .set_name(\"vision_conv\"));\n</code></pre>"},{"location":"api-reference/cpp/#pointwise-operations","title":"Pointwise Operations","text":"<pre><code>// ReLU\nauto Y = graph.relu(X);\n\n// GELU\nauto Y = graph.gelu(X);\n\n// SiLU/Swish\nauto Y = graph.silu(X);\n\n// Addition\nauto Y = graph.add(A, B);\n\n// Multiplication\nauto Y = graph.mul(A, B);\n</code></pre>"},{"location":"api-reference/cpp/#build-process","title":"Build Process","text":"<pre><code>// 1. Validate graph structure\nauto status = graph.validate();\nif (status.is_bad()) {\n    std::cerr &lt;&lt; \"Validation failed: \" &lt;&lt; status.get_message() &lt;&lt; std::endl;\n    return -1;\n}\n\n// 2. Build operation graph\ngraph.build_operation_graph(handle);\n\n// 3. Create execution plans with heuristics\ngraph.create_execution_plans({\n    fe::HeurMode_t::A,\n    fe::HeurMode_t::FALLBACK\n});\n\n// 4. Check hardware support\nstatus = graph.check_support(handle);\nif (status.is_bad()) {\n    std::cerr &lt;&lt; \"Not supported: \" &lt;&lt; status.get_message() &lt;&lt; std::endl;\n    return -1;\n}\n\n// 5. Build selected plans\ngraph.build_plans(handle);\n</code></pre>"},{"location":"api-reference/cpp/#execution","title":"Execution","text":"<pre><code>// Get workspace size\nint64_t workspace_size = graph.get_workspace_size();\n\n// Allocate workspace\nvoid* workspace;\ncudaMalloc(&amp;workspace, workspace_size);\n\n// Create variant pack (tensor -&gt; device pointer mapping)\nstd::unordered_map&lt;std::shared_ptr&lt;fe::graph::Tensor&gt;, void*&gt; variant_pack = {\n    {Q, q_device_ptr},\n    {K, k_device_ptr},\n    {V, v_device_ptr},\n    {O, o_device_ptr}\n};\n\n// Execute\nauto status = graph.execute(handle, variant_pack, workspace);\ncudaDeviceSynchronize();\n</code></pre>"},{"location":"api-reference/cpp/#serialization","title":"Serialization","text":"<pre><code>// Serialize graph to bytes\nstd::vector&lt;uint8_t&gt; serialized_data;\ngraph.serialize(serialized_data);\n\n// Save to file\nstd::ofstream file(\"attention_graph.bin\", std::ios::binary);\nfile.write(reinterpret_cast&lt;char*&gt;(serialized_data.data()),\n           serialized_data.size());\nfile.close();\n\n// Later: deserialize\nstd::ifstream file(\"attention_graph.bin\", std::ios::binary);\nstd::vector&lt;uint8_t&gt; loaded_data(\n    std::istreambuf_iterator&lt;char&gt;(file),\n    std::istreambuf_iterator&lt;char&gt;());\n\nauto loaded_graph = fe::graph::Graph::deserialize(loaded_data);\n// Ready to execute immediately!\n</code></pre>"},{"location":"api-reference/cpp/#complete-llm-attention-example","title":"Complete LLM Attention Example","text":"<pre><code>#include &lt;cudnn_frontend.h&gt;\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cmath&gt;\n\nnamespace fe = cudnn_frontend;\n\nclass LLMAttention {\npublic:\n    LLMAttention(int batch, int heads, int seq_len, int head_dim)\n        : batch_(batch), heads_(heads), seq_len_(seq_len), head_dim_(head_dim)\n    {\n        cudnnCreate(&amp;handle_);\n        build_graph();\n    }\n\n    ~LLMAttention() {\n        cudaFree(workspace_);\n        cudnnDestroy(handle_);\n    }\n\n    void forward(void* q_ptr, void* k_ptr, void* v_ptr, void* o_ptr) {\n        std::unordered_map&lt;std::shared_ptr&lt;fe::graph::Tensor&gt;, void*&gt; variant_pack = {\n            {Q_, q_ptr}, {K_, k_ptr}, {V_, v_ptr}, {O_, o_ptr}\n        };\n        graph_.execute(handle_, variant_pack, workspace_);\n    }\n\nprivate:\n    void build_graph() {\n        graph_.set_io_data_type(fe::DataType_t::HALF)\n              .set_intermediate_data_type(fe::DataType_t::FLOAT)\n              .set_compute_data_type(fe::DataType_t::FLOAT);\n\n        std::vector&lt;int64_t&gt; dims = {batch_, heads_, seq_len_, head_dim_};\n        std::vector&lt;int64_t&gt; strides = {\n            heads_ * seq_len_ * head_dim_,\n            seq_len_ * head_dim_,\n            head_dim_,\n            1\n        };\n\n        Q_ = graph_.tensor(fe::graph::Tensor_attributes()\n            .set_name(\"Q\").set_dim(dims).set_stride(strides)\n            .set_data_type(fe::DataType_t::HALF));\n\n        K_ = graph_.tensor(fe::graph::Tensor_attributes()\n            .set_name(\"K\").set_dim(dims).set_stride(strides)\n            .set_data_type(fe::DataType_t::HALF));\n\n        V_ = graph_.tensor(fe::graph::Tensor_attributes()\n            .set_name(\"V\").set_dim(dims).set_stride(strides)\n            .set_data_type(fe::DataType_t::HALF));\n\n        float scale = 1.0f / sqrtf(static_cast&lt;float&gt;(head_dim_));\n        auto [O, Stats] = graph_.sdpa(Q_, K_, V_,\n            fe::graph::SDPA_attributes()\n                .set_attn_scale(scale)\n                .set_is_inference(true)\n                .set_causal_mask(true));\n\n        O_ = O;\n        O_-&gt;set_output(true);\n        O_-&gt;set_dim(dims);\n        O_-&gt;set_stride(strides);\n\n        graph_.validate();\n        graph_.build_operation_graph(handle_);\n        graph_.create_execution_plans({fe::HeurMode_t::A});\n        graph_.check_support(handle_);\n        graph_.build_plans(handle_);\n\n        cudaMalloc(&amp;workspace_, graph_.get_workspace_size());\n    }\n\n    int batch_, heads_, seq_len_, head_dim_;\n    cudnnHandle_t handle_;\n    fe::graph::Graph graph_;\n    std::shared_ptr&lt;fe::graph::Tensor&gt; Q_, K_, V_, O_;\n    void* workspace_ = nullptr;\n};\n</code></pre>"},{"location":"api-reference/cpp/#cmake-integration","title":"CMake Integration","text":"<pre><code>cmake_minimum_required(VERSION 3.18)\nproject(cudnn_frontend_example CUDA CXX)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CUDA_STANDARD 17)\n\n# Find CUDA\nfind_package(CUDAToolkit REQUIRED)\n\n# Find cuDNN\nfind_path(CUDNN_INCLUDE_DIR cudnn.h\n    HINTS ${CUDNN_PATH}/include)\nfind_library(CUDNN_LIBRARY cudnn\n    HINTS ${CUDNN_PATH}/lib64)\n\n# Add executable\nadd_executable(attention_example main.cpp)\n\ntarget_include_directories(attention_example PRIVATE\n    ${CUDNN_INCLUDE_DIR}\n    ${CMAKE_SOURCE_DIR}/cudnn-frontend/include)\n\ntarget_link_libraries(attention_example\n    CUDA::cudart\n    ${CUDNN_LIBRARY})\n</code></pre>"},{"location":"api-reference/cpp/#next-steps","title":"Next Steps","text":"<p>See common patterns and best practices.</p> <p>Common Patterns </p>"},{"location":"api-reference/patterns/","title":"Common Patterns","text":"<p>This page provides production-ready patterns for common deep learning operations, focused on LLM and VLM workloads.</p> <p>What cuDNN Can Actually Fuse</p> <p>cuDNN graphs have limits on what can be fused together. A single graph typically handles:</p> <ul> <li>SDPA: Q, K, V \u2192 attention output (this IS heavily fused internally)</li> <li>Normalization: input \u2192 normalized output (with optional bias/scale)</li> <li>Conv + activation: convolution \u2192 activation (e.g., ReLU, SiLU)</li> <li>MatMul + bias + activation: limited fusion</li> </ul> <p>A full transformer layer requires multiple separate graphs plus PyTorch operations. The examples below show realistic patterns, not magical full-layer fusion.</p>"},{"location":"api-reference/patterns/#llm-patterns","title":"LLM Patterns","text":""},{"location":"api-reference/patterns/#complete-transformer-layer","title":"Complete Transformer Layer","text":"<p>The standard pre-norm transformer block - note this uses multiple small graphs, not one fused graph:</p> <pre><code>import cudnn\nimport torch\nimport math\n\nclass TransformerLayer:\n    \"\"\"\n    Pre-norm transformer layer as used in LLaMA, Mistral, etc.\n\n    Architecture:\n    x \u2192 RMSNorm \u2192 Self-Attention \u2192 (+x) \u2192 RMSNorm \u2192 FFN \u2192 (+x) \u2192 y\n    \"\"\"\n\n    def __init__(self, hidden_dim, num_heads, ff_dim, handle):\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        self.head_dim = hidden_dim // num_heads\n        self.ff_dim = ff_dim\n        self.handle = handle\n        self.attn_scale = 1.0 / math.sqrt(self.head_dim)\n\n        # Initialize parameters (in production, these would be loaded)\n        self._init_parameters()\n\n    def _init_parameters(self):\n        device = \"cuda\"\n        dtype = torch.bfloat16\n\n        # RMSNorm parameters\n        self.norm1_weight = torch.ones(self.hidden_dim, device=device, dtype=torch.float32)\n        self.norm2_weight = torch.ones(self.hidden_dim, device=device, dtype=torch.float32)\n\n        # Attention projections (fused QKV)\n        self.W_qkv = torch.randn(self.hidden_dim, 3 * self.hidden_dim,\n                                  device=device, dtype=dtype) * 0.02\n        self.W_out = torch.randn(self.hidden_dim, self.hidden_dim,\n                                  device=device, dtype=dtype) * 0.02\n\n        # FFN (SwiGLU)\n        self.W_gate = torch.randn(self.hidden_dim, self.ff_dim,\n                                   device=device, dtype=dtype) * 0.02\n        self.W_up = torch.randn(self.hidden_dim, self.ff_dim,\n                                 device=device, dtype=dtype) * 0.02\n        self.W_down = torch.randn(self.ff_dim, self.hidden_dim,\n                                   device=device, dtype=dtype) * 0.02\n\n    def forward(self, x, kv_cache=None):\n        batch, seq_len, _ = x.shape\n\n        # === Self-Attention ===\n        residual = x\n\n        # Pre-attention RMSNorm\n        with cudnn.Graph(io_data_type=cudnn.data_type.BFLOAT16,\n                         compute_data_type=cudnn.data_type.FLOAT) as norm_graph:\n            x_norm, _ = norm_graph.rmsnorm(x, self.norm1_weight, epsilon=1e-5)\n            x_norm.set_output(True)\n        x_norm = norm_graph(x, self.norm1_weight, handle=self.handle)\n\n        # QKV projection\n        qkv = x_norm @ self.W_qkv\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for attention\n        q = q.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # cuDNN SDPA\n        with cudnn.Graph(io_data_type=cudnn.data_type.BFLOAT16,\n                         intermediate_data_type=cudnn.data_type.FLOAT,\n                         compute_data_type=cudnn.data_type.FLOAT) as attn_graph:\n            attn_out, _ = attn_graph.sdpa(\n                q=q, k=k, v=v,\n                attn_scale=self.attn_scale,\n                is_inference=True,\n                use_causal_mask=True,\n            )\n            attn_out.set_output(True).set_dim(q.shape).set_stride(q.stride())\n\n        attn_out = attn_graph(q, k, v, handle=self.handle)\n\n        # Output projection\n        attn_out = attn_out.transpose(1, 2).contiguous().view(batch, seq_len, -1)\n        attn_out = attn_out @ self.W_out\n\n        # Residual\n        x = residual + attn_out\n\n        # === FFN (SwiGLU) ===\n        residual = x\n\n        # Pre-FFN RMSNorm\n        with cudnn.Graph(io_data_type=cudnn.data_type.BFLOAT16,\n                         compute_data_type=cudnn.data_type.FLOAT) as norm2_graph:\n            x_norm, _ = norm2_graph.rmsnorm(x, self.norm2_weight, epsilon=1e-5)\n            x_norm.set_output(True)\n        x_norm = norm2_graph(x, self.norm2_weight, handle=self.handle)\n\n        # SwiGLU: (x @ W_up) * silu(x @ W_gate) @ W_down\n        gate = x_norm @ self.W_gate\n        up = x_norm @ self.W_up\n\n        with cudnn.Graph(io_data_type=cudnn.data_type.BFLOAT16,\n                         compute_data_type=cudnn.data_type.FLOAT) as swiglu_graph:\n            gate_activated = swiglu_graph.silu(gate)\n            gated = swiglu_graph.mul(up, gate_activated)\n            gated.set_output(True)\n        gated = swiglu_graph(gate, up, handle=self.handle)\n\n        ff_out = gated @ self.W_down\n\n        # Residual\n        x = residual + ff_out\n\n        return x\n</code></pre>"},{"location":"api-reference/patterns/#grouped-query-attention-gqa","title":"Grouped Query Attention (GQA)","text":"<p>Used in LLaMA 2+, Mistral:</p> <pre><code>def gqa_attention(q, k, v, num_kv_groups, handle):\n    \"\"\"\n    Grouped Query Attention where K, V have fewer heads than Q.\n\n    Args:\n        q: [batch, num_q_heads, seq_len, head_dim]\n        k: [batch, num_kv_heads, seq_len, head_dim]\n        v: [batch, num_kv_heads, seq_len, head_dim]\n        num_kv_groups: num_q_heads // num_kv_heads\n    \"\"\"\n    batch, num_q_heads, seq_len, head_dim = q.shape\n    num_kv_heads = k.shape[1]\n\n    # cuDNN handles GQA natively when head counts differ\n    with cudnn.Graph(\n        io_data_type=cudnn.data_type.BFLOAT16,\n        intermediate_data_type=cudnn.data_type.FLOAT,\n        compute_data_type=cudnn.data_type.FLOAT,\n    ) as graph:\n        output, _ = graph.sdpa(\n            q=q, k=k, v=v,\n            attn_scale=1.0 / math.sqrt(head_dim),\n            is_inference=True,\n            use_causal_mask=True,\n        )\n        output.set_output(True).set_dim(q.shape).set_stride(q.stride())\n\n    return graph(q, k, v, handle=handle)\n</code></pre>"},{"location":"api-reference/patterns/#rotary-position-embedding-rope","title":"Rotary Position Embedding (RoPE)","text":"<p>Used in most modern LLMs:</p> <pre><code>def apply_rotary_embedding(q, k, cos, sin):\n    \"\"\"\n    Apply RoPE to query and key tensors.\n\n    This is typically done before SDPA.\n    \"\"\"\n    # Split into pairs\n    q1, q2 = q[..., ::2], q[..., 1::2]\n    k1, k2 = k[..., ::2], k[..., 1::2]\n\n    # Apply rotation\n    q_rotated = torch.cat([\n        q1 * cos - q2 * sin,\n        q1 * sin + q2 * cos\n    ], dim=-1)\n\n    k_rotated = torch.cat([\n        k1 * cos - k2 * sin,\n        k1 * sin + k2 * cos\n    ], dim=-1)\n\n    return q_rotated, k_rotated\n\n# Usage with cuDNN SDPA\nq_rope, k_rope = apply_rotary_embedding(q, k, cos, sin)\n# Then use q_rope, k_rope in SDPA\n</code></pre>"},{"location":"api-reference/patterns/#vlm-patterns","title":"VLM Patterns","text":""},{"location":"api-reference/patterns/#vision-encoder-vit-style","title":"Vision Encoder (ViT-style)","text":"<pre><code>class VisionEncoder:\n    \"\"\"Vision encoder for VLMs like LLaVA, Qwen-VL.\"\"\"\n\n    def __init__(self, image_size, patch_size, hidden_dim, num_heads, num_layers, handle):\n        self.patch_size = patch_size\n        self.num_patches = (image_size // patch_size) ** 2\n        self.hidden_dim = hidden_dim\n        self.handle = handle\n\n        # Patch embedding (convolution)\n        self.patch_embed = torch.randn(\n            hidden_dim, 3, patch_size, patch_size,\n            device=\"cuda\", dtype=torch.float16\n        ).to(memory_format=torch.channels_last)\n\n        # Position embedding\n        self.pos_embed = torch.randn(\n            1, self.num_patches + 1, hidden_dim,\n            device=\"cuda\", dtype=torch.float16\n        )\n\n        # CLS token\n        self.cls_token = torch.randn(\n            1, 1, hidden_dim,\n            device=\"cuda\", dtype=torch.float16\n        )\n\n    def patch_embedding(self, images):\n        \"\"\"Convert images to patch embeddings using convolution.\"\"\"\n        batch = images.shape[0]\n\n        with cudnn.Graph(\n            io_data_type=cudnn.data_type.HALF,\n            compute_data_type=cudnn.data_type.FLOAT,\n        ) as graph:\n            patches = graph.conv_fprop(\n                images, self.patch_embed,\n                padding=[0, 0],\n                stride=[self.patch_size, self.patch_size],\n            )\n            patches.set_output(True)\n\n        patches = graph(images, self.patch_embed, handle=self.handle)\n        # Reshape: [B, D, H/P, W/P] -&gt; [B, num_patches, D]\n        patches = patches.flatten(2).transpose(1, 2)\n        return patches\n\n    def forward(self, images):\n        # Patch embedding\n        x = self.patch_embedding(images)\n\n        # Add CLS token\n        cls_tokens = self.cls_token.expand(images.shape[0], -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n\n        # Add position embedding\n        x = x + self.pos_embed\n\n        # Apply transformer layers (similar to LLM layers above)\n        # ...\n\n        return x\n</code></pre>"},{"location":"api-reference/patterns/#cross-attention-for-vlm","title":"Cross-Attention for VLM","text":"<pre><code>def cross_attention(query, key, value, handle):\n    \"\"\"\n    Cross-attention between text and vision features.\n\n    Args:\n        query: Text features [batch, text_len, hidden_dim]\n        key: Vision features [batch, vision_len, hidden_dim]\n        value: Vision features [batch, vision_len, hidden_dim]\n    \"\"\"\n    batch, text_len, hidden_dim = query.shape\n    vision_len = key.shape[1]\n\n    # Reshape for multi-head attention\n    num_heads = 12\n    head_dim = hidden_dim // num_heads\n\n    q = query.view(batch, text_len, num_heads, head_dim).transpose(1, 2)\n    k = key.view(batch, vision_len, num_heads, head_dim).transpose(1, 2)\n    v = value.view(batch, vision_len, num_heads, head_dim).transpose(1, 2)\n\n    # Cross-attention (no causal mask!)\n    with cudnn.Graph(\n        io_data_type=cudnn.data_type.HALF,\n        intermediate_data_type=cudnn.data_type.FLOAT,\n        compute_data_type=cudnn.data_type.FLOAT,\n    ) as graph:\n        output, _ = graph.sdpa(\n            q=q, k=k, v=v,\n            attn_scale=1.0 / math.sqrt(head_dim),\n            is_inference=True,\n            use_causal_mask=False,  # No causal mask for cross-attention!\n        )\n        output.set_output(True).set_dim(q.shape).set_stride(q.stride())\n\n    output = graph(q, k, v, handle=handle)\n    return output.transpose(1, 2).contiguous().view(batch, text_len, hidden_dim)\n</code></pre>"},{"location":"api-reference/patterns/#production-patterns","title":"Production Patterns","text":""},{"location":"api-reference/patterns/#graph-caching-for-inference-server","title":"Graph Caching for Inference Server","text":"<pre><code>from functools import lru_cache\nfrom typing import Tuple\n\nclass GraphCache:\n    \"\"\"Cache cuDNN graphs for different configurations.\"\"\"\n\n    def __init__(self, handle):\n        self.handle = handle\n        self._cache = {}\n\n    def get_attention_graph(\n        self,\n        batch_size: int,\n        num_heads: int,\n        seq_len: int,\n        head_dim: int,\n    ) -&gt; cudnn.Graph:\n        \"\"\"Get or create attention graph for given dimensions.\"\"\"\n        key = (\"attention\", batch_size, num_heads, seq_len, head_dim)\n\n        if key not in self._cache:\n            self._cache[key] = self._build_attention_graph(\n                batch_size, num_heads, seq_len, head_dim\n            )\n\n        return self._cache[key]\n\n    def _build_attention_graph(self, batch, heads, seq, dim):\n        q = torch.randn(batch, heads, seq, dim, device=\"cuda\", dtype=torch.bfloat16)\n        k = torch.randn(batch, heads, seq, dim, device=\"cuda\", dtype=torch.bfloat16)\n        v = torch.randn(batch, heads, seq, dim, device=\"cuda\", dtype=torch.bfloat16)\n\n        with cudnn.Graph(\n            io_data_type=cudnn.data_type.BFLOAT16,\n            intermediate_data_type=cudnn.data_type.FLOAT,\n            compute_data_type=cudnn.data_type.FLOAT,\n        ) as graph:\n            o, _ = graph.sdpa(q, k, v, attn_scale=1/math.sqrt(dim),\n                              is_inference=True, use_causal_mask=True)\n            o.set_output(True).set_dim(q.shape).set_stride(q.stride())\n\n        return graph\n\n# Usage in inference server\ncache = GraphCache(handle)\n\ndef process_request(batch_size, seq_len):\n    graph = cache.get_attention_graph(batch_size, 32, seq_len, 128)\n    return graph(q, k, v, handle=handle)\n</code></pre>"},{"location":"api-reference/patterns/#batched-inference-with-dynamic-shapes","title":"Batched Inference with Dynamic Shapes","text":"<pre><code>class DynamicBatchProcessor:\n    \"\"\"Handle variable batch sizes efficiently.\"\"\"\n\n    def __init__(self, max_batch_size, num_heads, max_seq_len, head_dim, handle):\n        self.handle = handle\n        self.graphs = {}\n\n        # Pre-build graphs for common batch sizes\n        for batch_size in [1, 2, 4, 8, 16, 32]:\n            if batch_size &lt;= max_batch_size:\n                self.graphs[batch_size] = self._build_graph(\n                    batch_size, num_heads, max_seq_len, head_dim\n                )\n\n    def _build_graph(self, batch, heads, seq, dim):\n        # Build graph for specific batch size\n        ...\n\n    def process(self, q, k, v):\n        batch_size = q.shape[0]\n\n        # Find closest pre-built graph\n        valid_sizes = [s for s in self.graphs.keys() if s &gt;= batch_size]\n        if not valid_sizes:\n            raise ValueError(f\"Batch size {batch_size} too large\")\n\n        graph = self.graphs[min(valid_sizes)]\n\n        # Pad if necessary\n        if batch_size &lt; min(valid_sizes):\n            # Pad tensors to match graph dimensions\n            pad_size = min(valid_sizes) - batch_size\n            q = torch.cat([q, torch.zeros(pad_size, *q.shape[1:], device=q.device)], dim=0)\n            k = torch.cat([k, torch.zeros(pad_size, *k.shape[1:], device=k.device)], dim=0)\n            v = torch.cat([v, torch.zeros(pad_size, *v.shape[1:], device=v.device)], dim=0)\n\n        output = graph(q, k, v, handle=self.handle)\n\n        # Remove padding\n        return output[:batch_size]\n</code></pre>"},{"location":"api-reference/patterns/#fp8-inference-hopper","title":"FP8 Inference (Hopper+)","text":"<pre><code>def fp8_attention(q_fp8, k_fp8, v_fp8, q_scale, k_scale, v_scale, handle):\n    \"\"\"\n    FP8 attention for maximum throughput on Hopper GPUs.\n\n    Requires proper quantization of inputs.\n    \"\"\"\n    with cudnn.Graph(\n        io_data_type=cudnn.data_type.FP8_E4M3,\n        intermediate_data_type=cudnn.data_type.FLOAT,\n        compute_data_type=cudnn.data_type.FLOAT,\n    ) as graph:\n        output, _ = graph.sdpa(\n            q=q_fp8, k=k_fp8, v=v_fp8,\n            attn_scale=1.0 / math.sqrt(128),  # Includes scaling\n            is_inference=True,\n            use_causal_mask=True,\n            # FP8 specific parameters\n            q_scale=q_scale,\n            k_scale=k_scale,\n            v_scale=v_scale,\n        )\n        output.set_output(True)\n        output.set_data_type(cudnn.data_type.BFLOAT16)  # Output in BF16\n\n    return graph(q_fp8, k_fp8, v_fp8, handle=handle)\n</code></pre>"},{"location":"api-reference/patterns/#error-handling-pattern","title":"Error Handling Pattern","text":"<pre><code>def safe_execute(graph, *args, handle, fallback_fn=None):\n    \"\"\"Execute graph with fallback on error.\"\"\"\n    try:\n        return graph(*args, handle=handle)\n    except Exception as e:\n        print(f\"cuDNN execution failed: {e}\")\n        if fallback_fn:\n            print(\"Using fallback implementation\")\n            return fallback_fn(*args)\n        raise\n\n# Usage\nresult = safe_execute(\n    attention_graph,\n    q, k, v,\n    handle=handle,\n    fallback_fn=lambda q, k, v: torch.nn.functional.scaled_dot_product_attention(q, k, v)\n)\n</code></pre>"},{"location":"api-reference/patterns/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\n\nclass PerformanceMonitor:\n    \"\"\"Monitor cuDNN graph execution performance.\"\"\"\n\n    def __init__(self):\n        self.timings = {}\n\n    def time_execution(self, name, graph, *args, handle, warmup=3, iterations=10):\n        # Warmup\n        for _ in range(warmup):\n            graph(*args, handle=handle)\n        torch.cuda.synchronize()\n\n        # Timed runs\n        start = time.perf_counter()\n        for _ in range(iterations):\n            graph(*args, handle=handle)\n        torch.cuda.synchronize()\n        end = time.perf_counter()\n\n        avg_time = (end - start) / iterations * 1000  # ms\n        self.timings[name] = avg_time\n        return avg_time\n\n# Usage\nmonitor = PerformanceMonitor()\nattn_time = monitor.time_execution(\"attention\", attn_graph, q, k, v, handle=handle)\nprint(f\"Attention: {attn_time:.2f} ms\")\n</code></pre>"},{"location":"api-reference/patterns/#summary","title":"Summary","text":"Pattern Use Case Key Points Pre-norm Transformer LLM layers RMSNorm \u2192 Attention \u2192 Residual GQA Modern LLMs Fewer KV heads than Q heads RoPE Position encoding Apply before SDPA Vision Encoder VLMs Conv \u2192 Flatten \u2192 Transformer Cross-Attention VLMs No causal mask Graph Caching Inference servers Build once, execute many FP8 Maximum throughput Hopper GPUs only"},{"location":"api-reference/python/","title":"Python API Reference","text":"<p>This reference covers the cuDNN Graph API (Frontend v1) - the modern, recommended API for all new projects. The Graph API is designed for modern workloads like Large Language Models (LLMs) and Vision-Language Models (VLMs).</p> <p>Graph API vs Legacy API</p> <p>This documentation focuses exclusively on the Graph API introduced in cuDNN Frontend v1.0. The legacy <code>pygraph</code> API (v0.x) is deprecated and should not be used for new projects.</p>"},{"location":"api-reference/python/#core-classes","title":"Core Classes","text":""},{"location":"api-reference/python/#cudnngraph","title":"cudnn.Graph","text":"<p>The primary interface for building computation graphs.</p> <pre><code>class Graph:\n    \"\"\"\n    Context manager for building cuDNN computation graphs.\n\n    The Graph class provides a declarative way to define deep learning\n    operations that are optimized and executed on NVIDIA GPUs.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = None,\n        io_data_type: cudnn.data_type = None,\n        intermediate_data_type: cudnn.data_type = None,\n        compute_data_type: cudnn.data_type = None,\n        handle: cudnn.Handle = None,\n        heuristics: List[cudnn.heur_mode] = None,\n        inputs: List[str] = None,\n        outputs: List[str] = None,\n    ):\n        \"\"\"\n        Initialize a Graph.\n\n        Args:\n            name: Optional name for debugging\n            io_data_type: Default data type for inputs/outputs\n            intermediate_data_type: Data type for intermediate tensors\n            compute_data_type: Data type for internal computations\n            handle: cuDNN handle (can be provided at execution instead)\n            heuristics: Algorithm selection modes (default: [A, FALLBACK])\n            inputs: Input tensor names for positional execution\n            outputs: Output tensor names for positional execution\n        \"\"\"\n</code></pre> <p>Basic Usage:</p> <pre><code># Context manager pattern (recommended)\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    y = graph.conv_fprop(image=x, weight=w, padding=[1, 1])\n    y.set_output(True)\n\nresult = graph(x, w, handle=handle)\n</code></pre> <p>For LLM inference:</p> <pre><code># Optimized for transformer inference\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    intermediate_data_type=cudnn.data_type.FLOAT,  # Softmax precision\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    # Self-attention\n    output, _ = graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=1/math.sqrt(head_dim),\n        is_inference=True,\n        use_causal_mask=True,\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n</code></pre>"},{"location":"api-reference/python/#cudnndata_type","title":"cudnn.data_type","text":"<p>Enumeration of supported data types.</p> Value Description Use Case <code>FLOAT</code> 32-bit floating point Computation precision <code>HALF</code> 16-bit floating point (FP16) LLM inference, training <code>BFLOAT16</code> Brain floating point 16 Modern LLMs (LLaMA, etc.) <code>FP8_E4M3</code> 8-bit FP (4 exp, 3 mantissa) High-throughput inference <code>FP8_E5M2</code> 8-bit FP (5 exp, 2 mantissa) Gradient computation <code>INT8</code> 8-bit integer Quantized inference <p>Recommended configurations for LLMs:</p> <pre><code># Standard FP16 (most compatible)\nio_data_type=cudnn.data_type.HALF\ncompute_data_type=cudnn.data_type.FLOAT\n\n# BF16 for modern LLMs (LLaMA, Mistral, etc.)\nio_data_type=cudnn.data_type.BFLOAT16\ncompute_data_type=cudnn.data_type.FLOAT\n\n# FP8 for maximum throughput (Hopper GPUs)\nio_data_type=cudnn.data_type.FP8_E4M3\ncompute_data_type=cudnn.data_type.FLOAT\n</code></pre>"},{"location":"api-reference/python/#cudnnheur_mode","title":"cudnn.heur_mode","text":"<p>Heuristic modes for algorithm selection.</p> Value Description <code>A</code> Best heuristic (recommended primary) <code>B</code> Alternative heuristic <code>FALLBACK</code> Maximum compatibility <code>CUDNN_FIND</code> Exhaustive search (slow but optimal)"},{"location":"api-reference/python/#graph-operations","title":"Graph Operations","text":""},{"location":"api-reference/python/#attention-operations-llmvlm-core","title":"Attention Operations (LLM/VLM Core)","text":""},{"location":"api-reference/python/#graphsdpa","title":"graph.sdpa","text":"<p>Scaled Dot-Product Attention - the heart of transformers.</p> <pre><code>def sdpa(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attn_scale: float,\n    is_inference: bool = True,\n    use_causal_mask: bool = False,\n    generate_stats: bool = False,\n    attn_bias: Tensor = None,\n    dropout: float = 0.0,\n    dropout_seed: int = None,\n    dropout_offset: int = 0,\n    name: str = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"\n    Scaled Dot-Product Attention using FlashAttention algorithm.\n\n    Computes: softmax(Q @ K.T / sqrt(d_k)) @ V\n\n    Args:\n        q: Query tensor [B, H, N, D] or [B, N, H, D] physical\n        k: Key tensor [B, H, N, D] or [B, N, H, D] physical\n        v: Value tensor [B, H, N, D] or [B, N, H, D] physical\n        attn_scale: Scaling factor (usually 1/sqrt(head_dim))\n        is_inference: True for inference, False for training\n        use_causal_mask: Enable causal (autoregressive) masking\n        generate_stats: Save stats for backward pass\n        attn_bias: Optional attention bias [B, H, N, N]\n        dropout: Dropout probability (training only)\n        dropout_seed: Random seed for dropout\n        dropout_offset: Offset for dropout RNG\n        name: Operation name\n\n    Returns:\n        Tuple of (output, stats) tensors\n\n    Supports:\n        - Multi-Head Attention (MHA)\n        - Grouped Query Attention (GQA)\n        - Multi-Query Attention (MQA)\n    \"\"\"\n</code></pre> <p>LLM Inference Example:</p> <pre><code># Standard causal attention for LLM inference\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa(\n        q=queries, k=keys, v=values,\n        attn_scale=1.0 / math.sqrt(64),\n        is_inference=True,\n        use_causal_mask=True,  # Autoregressive decoding\n    )\n    output.set_output(True).set_dim(queries.shape).set_stride(queries.stride())\n</code></pre> <p>GQA for LLaMA-style models:</p> <pre><code># Grouped Query Attention (fewer KV heads than Q heads)\n# Q: [B, 32, N, D], K/V: [B, 8, N, D]\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa(\n        q=queries,  # 32 heads\n        k=keys,     # 8 heads (4x fewer)\n        v=values,   # 8 heads\n        attn_scale=scale,\n        is_inference=True,\n        use_causal_mask=True,\n    )\n</code></pre>"},{"location":"api-reference/python/#graphsdpa_backward","title":"graph.sdpa_backward","text":"<p>Backward pass for SDPA (training).</p> <pre><code>def sdpa_backward(\n    self,\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    o: Tensor,\n    dO: Tensor,\n    stats: Tensor,\n    attn_scale: float,\n    use_causal_mask: bool = False,\n) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"\n    Backward pass for SDPA.\n\n    Returns:\n        Tuple of (dQ, dK, dV) gradients\n    \"\"\"\n</code></pre>"},{"location":"api-reference/python/#matrix-operations","title":"Matrix Operations","text":""},{"location":"api-reference/python/#graphmatmul","title":"graph.matmul","text":"<p>General matrix multiplication for linear layers.</p> <pre><code>def matmul(\n    self,\n    A: Tensor,\n    B: Tensor,\n    compute_data_type: data_type = None,\n    name: str = None,\n) -&gt; Tensor:\n    \"\"\"\n    Matrix multiplication: C = A @ B\n\n    Supports:\n        - 2D: [M, K] @ [K, N] = [M, N]\n        - Batched: [B, M, K] @ [B, K, N] = [B, M, N]\n        - Mixed precision inputs\n\n    Common LLM uses:\n        - QKV projections\n        - Output projections\n        - FFN layers\n    \"\"\"\n</code></pre> <p>LLM Linear Layers:</p> <pre><code># FFN first layer (hidden expansion)\nwith cudnn.Graph() as graph:\n    # x: [B, N, D], W1: [D, 4D]\n    h = graph.matmul(x, W1)\n    h = graph.gelu(h)\n    # h: [B, N, 4D]\n\n    # Second layer (hidden contraction)\n    # W2: [4D, D]\n    out = graph.matmul(h, W2)\n    out.set_output(True)\n</code></pre>"},{"location":"api-reference/python/#normalization-operations","title":"Normalization Operations","text":""},{"location":"api-reference/python/#graphlayernorm","title":"graph.layernorm","text":"<p>Layer normalization (used in all modern transformers).</p> <pre><code>def layernorm(\n    self,\n    input: Tensor,\n    scale: Tensor,\n    bias: Tensor,\n    epsilon: float = 1e-5,\n    zero_centered_gamma: bool = False,\n    name: str = None,\n) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    \"\"\"\n    Layer Normalization: y = (x - mean) / sqrt(var + eps) * gamma + beta\n\n    Args:\n        input: Input tensor [B, N, D]\n        scale: Scale parameter (gamma) [D]\n        bias: Bias parameter (beta) [D]\n        epsilon: Numerical stability constant\n        zero_centered_gamma: Use (1 + gamma) instead of gamma\n\n    Returns:\n        Tuple of (output, mean, inv_variance)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/python/#graphrmsnorm","title":"graph.rmsnorm","text":"<p>RMS Normalization (LLaMA, Mistral, etc.).</p> <pre><code>def rmsnorm(\n    self,\n    input: Tensor,\n    scale: Tensor,\n    epsilon: float = 1e-5,\n    name: str = None,\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"\n    RMS Normalization: y = x / sqrt(mean(x^2) + eps) * gamma\n\n    Simpler than LayerNorm - no mean centering.\n    Used in LLaMA, Mistral, and other modern LLMs.\n\n    Returns:\n        Tuple of (output, inv_rms)\n    \"\"\"\n</code></pre> <p>Pre-norm Transformer Pattern:</p> <pre><code># Pre-norm (modern LLM style)\nwith cudnn.Graph() as graph:\n    # RMS norm before attention\n    x_norm, _ = graph.rmsnorm(x, gamma, epsilon=1e-5)\n    # ... attention ...\n    x = graph.add(x, attn_output)  # Residual\n\n    # RMS norm before FFN\n    x_norm, _ = graph.rmsnorm(x, gamma2, epsilon=1e-5)\n    # ... FFN ...\n    x = graph.add(x, ffn_output)  # Residual\n</code></pre>"},{"location":"api-reference/python/#activation-functions","title":"Activation Functions","text":""},{"location":"api-reference/python/#graphgelu","title":"graph.gelu","text":"<p>Gaussian Error Linear Unit (standard in transformers).</p> <pre><code>def gelu(self, input: Tensor, name: str = None) -&gt; Tensor:\n    \"\"\"GELU activation: x * \u03a6(x)\"\"\"\n</code></pre>"},{"location":"api-reference/python/#graphsilu-graphswish","title":"graph.silu / graph.swish","text":"<p>SiLU activation (used in SwiGLU).</p> <pre><code>def silu(self, input: Tensor, name: str = None) -&gt; Tensor:\n    \"\"\"SiLU/Swish activation: x * sigmoid(x)\"\"\"\n</code></pre>"},{"location":"api-reference/python/#graphrelu","title":"graph.relu","text":"<p>ReLU activation.</p> <pre><code>def relu(self, input: Tensor, name: str = None) -&gt; Tensor:\n    \"\"\"ReLU activation: max(0, x)\"\"\"\n</code></pre>"},{"location":"api-reference/python/#element-wise-operations","title":"Element-wise Operations","text":"<pre><code># Addition\ny = graph.add(a=x1, b=x2)\n\n# Multiplication\ny = graph.mul(a=x1, b=x2)\n\n# Bias addition\ny = graph.bias(input=x, bias=b)\n\n# Scaling\ny = graph.scale(input=x, scale=s)\n</code></pre>"},{"location":"api-reference/python/#convolution-operations","title":"Convolution Operations","text":"<p>For vision components in VLMs:</p> <pre><code># Forward convolution\ny = graph.conv_fprop(\n    image=x,           # [N, C, H, W]\n    weight=w,          # [K, C, R, S]\n    padding=[1, 1],\n    stride=[1, 1],\n    dilation=[1, 1],\n)\n\n# Data gradient (backward)\ndx = graph.conv_dgrad(weight=w, loss=dy, padding=[1, 1])\n\n# Weight gradient (backward)\ndw = graph.conv_wgrad(image=x, loss=dy, padding=[1, 1])\n</code></pre>"},{"location":"api-reference/python/#tensor-methods","title":"Tensor Methods","text":""},{"location":"api-reference/python/#set_output","title":"set_output","text":"<p>Mark tensor as graph output.</p> <pre><code>y.set_output(True)  # Required to retrieve this tensor\n</code></pre>"},{"location":"api-reference/python/#set_name","title":"set_name","text":"<p>Assign a name for referencing.</p> <pre><code>y.set_name(\"attention_output\")\n</code></pre>"},{"location":"api-reference/python/#set_data_type","title":"set_data_type","text":"<p>Override data type.</p> <pre><code>y.set_data_type(cudnn.data_type.FLOAT)\n</code></pre>"},{"location":"api-reference/python/#set_dim-set_stride","title":"set_dim / set_stride","text":"<p>Set explicit dimensions and strides.</p> <pre><code>y.set_dim([batch, num_heads, seq_len, head_dim])\ny.set_stride([seq_len * num_heads * head_dim, head_dim, num_heads * head_dim, 1])\n</code></pre>"},{"location":"api-reference/python/#handle-management","title":"Handle Management","text":"<pre><code># Create handle\nhandle = cudnn.create_handle()\n\n# Use in graph execution\nresult = graph(x, w, handle=handle)\n\n# Destroy when done\ncudnn.destroy_handle(handle)\n</code></pre>"},{"location":"api-reference/python/#graph-execution","title":"Graph Execution","text":""},{"location":"api-reference/python/#positional-arguments","title":"Positional Arguments","text":"<pre><code>with cudnn.Graph(\n    inputs=[\"attn::q\", \"attn::k\", \"attn::v\"],\n    outputs=[\"output\"],\n) as graph:\n    out, _ = graph.sdpa(q=Q, k=K, v=V, name=\"attn\")\n    out.set_output(True).set_name(\"output\")\n\n# Execute with positional args\nresult = graph(Q_data, K_data, V_data, handle=handle)\n</code></pre>"},{"location":"api-reference/python/#dictionary-interface","title":"Dictionary Interface","text":"<pre><code>with cudnn.Graph() as graph:\n    out, _ = graph.sdpa(q=Q, k=K, v=V, name=\"attn\")\n    out.set_output(True)\n\n# Execute with dict\nresult_dict = graph({\n    \"attn::q\": Q_data,\n    \"attn::k\": K_data,\n    \"attn::v\": V_data,\n}, handle=handle)\noutput = result_dict[\"attn::O\"]\n</code></pre>"},{"location":"api-reference/python/#utility-functions","title":"Utility Functions","text":"<pre><code># Get cuDNN backend version\nversion = cudnn.backend_version()  # e.g., 90300 for 9.3.0\n\n# Query workspace size\nworkspace_size = graph.get_workspace_size()\n\n# Serialize/deserialize graphs\nserialized = graph.serialize()\ngraph = cudnn.Graph.deserialize(serialized)\n</code></pre>"},{"location":"api-reference/python/#complete-llm-layer-example","title":"Complete LLM Layer Example","text":"<pre><code>import cudnn\nimport torch\nimport math\n\ndef create_llm_attention_layer(\n    batch_size, seq_len, num_heads, head_dim, handle\n):\n    \"\"\"Creates an optimized attention layer for LLM inference.\"\"\"\n\n    # Create template tensors\n    hidden_dim = num_heads * head_dim\n    x = torch.randn(batch_size, seq_len, hidden_dim,\n                    device=\"cuda\", dtype=torch.bfloat16)\n\n    # RMSNorm parameters\n    gamma = torch.ones(hidden_dim, device=\"cuda\", dtype=torch.float32)\n\n    # Projection weights (fused QKV)\n    W_qkv = torch.randn(hidden_dim, 3 * hidden_dim,\n                        device=\"cuda\", dtype=torch.bfloat16)\n    W_out = torch.randn(hidden_dim, hidden_dim,\n                        device=\"cuda\", dtype=torch.bfloat16)\n\n    # Build the graph\n    with cudnn.Graph(\n        io_data_type=cudnn.data_type.BFLOAT16,\n        intermediate_data_type=cudnn.data_type.FLOAT,\n        compute_data_type=cudnn.data_type.FLOAT,\n    ) as graph:\n        # Pre-attention RMSNorm\n        x_norm, _ = graph.rmsnorm(x, gamma, epsilon=1e-5)\n\n        # QKV projection would go here (using matmul)\n        # For this example, we assume Q, K, V are pre-computed\n\n    return graph\n</code></pre>"},{"location":"api-reference/python/#next-steps","title":"Next Steps","text":"<p>See the C++ API reference for equivalent C++ interfaces.</p> <p>C++ API Reference </p>"},{"location":"concepts/cuda-ecosystem/","title":"The CUDA Ecosystem","text":"<p>Before diving deeper into cuDNN Frontend, let's understand where it fits in the larger NVIDIA software stack. This context will help you make better decisions and debug issues more effectively.</p>"},{"location":"concepts/cuda-ecosystem/#the-big-picture","title":"The Big Picture","text":"<pre><code>graph TB\n    subgraph \"Your Application Layer\"\n        App[Your AI/ML Application]\n    end\n\n    subgraph \"Training Frameworks\"\n        PT[PyTorch]\n        TF[TensorFlow]\n        JAX[JAX]\n        Nemo[Nemo]\n    end\n\n    subgraph \"Inference Frameworks\"\n        TRTLLM[TensorRT-LLM]\n        vLLM[vLLM]\n        SGLang[SGLang]\n        TRT[TensorRT]\n    end\n\n    subgraph \"CUDA Libraries\"\n        FI[FlashInfer]\n        cuDNN[cuDNN Frontend/Backend]\n        cuBLAS[cuBLAS]\n        CUTLASS[CUTLASS]\n        NCCL[NCCL]\n    end\n\n    subgraph \"Runtime\"\n        CUDA[CUDA Runtime]\n    end\n\n    subgraph \"Hardware\"\n        GPU[NVIDIA GPU]\n    end\n\n    App --&gt; TRTLLM &amp; vLLM &amp; SGLang\n    App --&gt; PT &amp; TF &amp; JAX &amp; Nemo\n    App --&gt; TRT\n\n    TRTLLM &amp; vLLM &amp; SGLang --&gt; FI\n    FI --&gt; cuDNN\n    cuDNN --&gt; cuBLAS\n    cuDNN --&gt; CUTLASS\n\n    PT &amp; TF &amp; JAX &amp; Nemo --&gt; cuDNN &amp; cuBLAS &amp; NCCL\n\n    TRT --&gt; CUDA\n    cuDNN &amp; CUTLASS --&gt; CUDA\n    cuBLAS &amp; NCCL --&gt; CUDA\n    CUDA --&gt; GPU\n\n    style cuDNN fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n    style FI fill:#e1bee7\n    style GPU fill:#76ff03</code></pre>"},{"location":"concepts/cuda-ecosystem/#component-overview","title":"Component Overview","text":"Component What It Does Training Frameworks PyTorch, TensorFlow, JAX, Nemo - high-level APIs for model development Inference Frameworks TensorRT-LLM, vLLM, SGLang - optimized LLM serving; TensorRT - general inference FlashInfer Unified kernel library for LLM inference (MLSys 2025 Best Paper) cuDNN Frontend Modern graph-based API for deep learning ops - what you're learning! cuDNN Backend Core C library with optimized GPU kernels cuBLAS Basic linear algebra (GEMM, etc.) CUTLASS Template library for custom CUDA kernels NCCL Multi-GPU communication (AllReduce, etc.) CUDA Runtime Foundation for all GPU computing"},{"location":"concepts/cuda-ecosystem/#next-steps","title":"Next Steps","text":"<p>Now that you understand where cuDNN Frontend fits, learn why it exists and what problems it solves.</p> <p>Why cuDNN Frontend? </p>"},{"location":"concepts/execution-plans/","title":"Execution Plans","text":"<p>When you build a cuDNN graph, the system generates one or more execution plans - specific strategies for running your computation on the GPU. Understanding this process helps you optimize performance.</p>"},{"location":"concepts/execution-plans/#what-is-an-execution-plan","title":"What is an Execution Plan?","text":"<p>An execution plan is a concrete recipe for executing your graph:</p> <pre><code>graph LR\n    subgraph \"Your Graph\"\n        G[Abstract Operations]\n    end\n\n    subgraph \"Execution Plans\"\n        P1[Plan 1: Algorithm A]\n        P2[Plan 2: Algorithm B]\n        P3[Plan 3: Algorithm C]\n    end\n\n    subgraph \"GPU Execution\"\n        E[Optimized Kernel]\n    end\n\n    G --&gt; P1 &amp; P2 &amp; P3\n    P1 --&gt;|Selected| E\n\n    style G fill:#bbdefb\n    style P1 fill:#c8e6c9\n    style P2 fill:#fff9c4\n    style P3 fill:#fff9c4\n    style E fill:#76ff03</code></pre>"},{"location":"concepts/execution-plans/#the-planning-process","title":"The Planning Process","text":"<p>When you call <code>graph.build()</code> (or exit a context manager), several steps happen:</p>"},{"location":"concepts/execution-plans/#step-1-validation","title":"Step 1: Validation","text":"<p>The graph checks for errors:</p> <pre><code>graph.validate()  # Checks shapes, types, compatibility\n</code></pre>"},{"location":"concepts/execution-plans/#step-2-lowering","title":"Step 2: Lowering","text":"<p>Your high-level graph is converted to cuDNN's internal representation:</p> <pre><code>graph.build_operation_graph()  # Creates backend operation graph\n</code></pre>"},{"location":"concepts/execution-plans/#step-3-plan-generation","title":"Step 3: Plan Generation","text":"<p>cuDNN uses heuristics to generate candidate execution plans:</p> <pre><code>graph.create_execution_plans([\n    cudnn.heur_mode.A,        # Primary heuristic\n    cudnn.heur_mode.FALLBACK  # Backup heuristic\n])\n</code></pre>"},{"location":"concepts/execution-plans/#step-4-support-check","title":"Step 4: Support Check","text":"<p>Verify plans are supported on your hardware:</p> <pre><code>graph.check_support()  # Ensures at least one plan works\n</code></pre>"},{"location":"concepts/execution-plans/#step-5-plan-building","title":"Step 5: Plan Building","text":"<p>Compile the selected plan(s):</p> <pre><code>graph.build_plans()  # Prepares for execution\n</code></pre>"},{"location":"concepts/execution-plans/#heuristic-modes","title":"Heuristic Modes","text":"<p>cuDNN offers different strategies for finding execution plans:</p> Mode Description Speed Quality <code>heur_mode.A</code> Best quality heuristic Fast Best <code>heur_mode.B</code> Alternative heuristic Fast Good <code>heur_mode.FALLBACK</code> Maximum compatibility Fast Basic <code>heur_mode.CUDNN_FIND</code> Exhaustive search Slow Optimal <pre><code># Recommended: Primary + Fallback\ngraph.create_execution_plans([\n    cudnn.heur_mode.A,\n    cudnn.heur_mode.FALLBACK\n])\n\n# For maximum performance (slower initial build):\ngraph.create_execution_plans([\n    cudnn.heur_mode.CUDNN_FIND\n])\n</code></pre>"},{"location":"concepts/execution-plans/#autotuning","title":"Autotuning","text":"<p>Autotuning benchmarks multiple plans to find the fastest:</p> <pre><code># Build graph with autotuning\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w, padding=[1, 1])\n    y.set_output(True)\n\n# Get all candidate plans\ngraph.build_operation_graph()\ngraph.create_execution_plans([cudnn.heur_mode.A])\n\n# Autotune to find best\nworkspace = torch.empty(graph.get_workspace_size(), device=\"cuda\", dtype=torch.uint8)\nbest_plan = graph.autotune(\n    {x: x_gpu, w: w_gpu, y: y_gpu},\n    workspace,\n    handle=handle\n)\n</code></pre>"},{"location":"concepts/execution-plans/#autotuning-workflow","title":"Autotuning Workflow","text":"<pre><code>graph TD\n    A[Generate Candidate Plans] --&gt; B[Benchmark Plan 1]\n    B --&gt; C[Benchmark Plan 2]\n    C --&gt; D[Benchmark Plan N]\n    D --&gt; E[Select Fastest Plan]\n    E --&gt; F[Cache for Reuse]\n\n    style E fill:#c8e6c9\n    style F fill:#fff9c4</code></pre>"},{"location":"concepts/execution-plans/#when-to-autotune","title":"When to Autotune","text":"Scenario Recommendation Development/prototyping Skip autotuning Production deployment Autotune once, cache result Batch size changes frequently Autotune each size Static workload Autotune once"},{"location":"concepts/execution-plans/#understanding-cudnn-algorithms","title":"Understanding cuDNN Algorithms","text":"<p>Different algorithms trade off between memory and speed:</p>"},{"location":"concepts/execution-plans/#convolution-algorithms","title":"Convolution Algorithms","text":"Algorithm Memory Speed Best For IMPLICIT_GEMM Low Medium Memory-constrained IMPLICIT_PRECOMP_GEMM Medium Fast Most cases GEMM High Fast Large filters FFT Very High Very Fast Large images WINOGRAD Medium Very Fast 3x3 filters <p>cuDNN Frontend selects automatically, but you can influence the choice through workspace size.</p>"},{"location":"concepts/execution-plans/#attention-algorithms","title":"Attention Algorithms","text":"<p>For SDPA, cuDNN may use:</p> <ul> <li>Flash Attention: Memory-efficient, fast</li> <li>Memory Efficient Attention: Lower memory, slightly slower</li> <li>Standard Attention: For small sequences</li> </ul>"},{"location":"concepts/execution-plans/#workspace-memory","title":"Workspace Memory","text":"<p>Execution plans may need temporary memory (workspace):</p> <pre><code># Query workspace size\nworkspace_size = graph.get_workspace_size()\nprint(f\"Workspace needed: {workspace_size / 1024**2:.1f} MB\")\n\n# Allocate workspace\nworkspace = torch.empty(workspace_size, device=\"cuda\", dtype=torch.uint8)\n\n# Execute with workspace\ngraph.execute(variant_pack, workspace, handle=handle)\n</code></pre>"},{"location":"concepts/execution-plans/#workspace-strategies","title":"Workspace Strategies","text":"Strategy Approach Trade-off Minimal Small workspace limit Slower algorithms Generous Large workspace Faster algorithms Adaptive Query and allocate Balanced <pre><code># Limit workspace to 256MB\ngraph.create_execution_plans(\n    [cudnn.heur_mode.A],\n    max_workspace_size=256 * 1024 * 1024\n)\n</code></pre>"},{"location":"concepts/execution-plans/#plan-serialization","title":"Plan Serialization","text":"<p>Save compiled plans for faster startup:</p> <pre><code># Serialize to file\nserialized = graph.serialize()\nwith open(\"conv_plan.bin\", \"wb\") as f:\n    f.write(serialized)\n\n# Later: Deserialize\nwith open(\"conv_plan.bin\", \"rb\") as f:\n    serialized = f.read()\n\ngraph = cudnn.Graph.deserialize(serialized)\n# Graph is ready to execute immediately!\n</code></pre>"},{"location":"concepts/execution-plans/#serialization-benefits","title":"Serialization Benefits","text":"<ul> <li>Skip compilation on subsequent runs</li> <li>Share optimized plans across machines (same GPU)</li> <li>Cache autotuned results</li> </ul>"},{"location":"concepts/execution-plans/#execution-plan-details","title":"Execution Plan Details","text":"<p>Inspect what cuDNN chose:</p> <pre><code># Build graph\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w, padding=[1, 1])\n    y.set_output(True)\n\n# Print graph info\nprint(graph)  # Shows selected algorithms, workspace, etc.\n</code></pre>"},{"location":"concepts/execution-plans/#execution-modes","title":"Execution Modes","text":""},{"location":"concepts/execution-plans/#asynchronous-execution-default","title":"Asynchronous Execution (Default)","text":"<p>By default, cuDNN executes asynchronously - the call returns immediately while the GPU works:</p> <pre><code>result = graph(x, w, handle=handle)  # Returns immediately, GPU runs async\ntorch.cuda.synchronize()  # Wait for GPU to finish if needed\n</code></pre>"},{"location":"concepts/execution-plans/#with-cuda-graphs","title":"With CUDA Graphs","text":"<p>For repetitive execution, use CUDA graphs for minimal overhead:</p> <pre><code>import cudnn\n\n# Build cuDNN graph\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w, padding=[1, 1])\n    y.set_output(True)\n\n# Capture into CUDA graph (advanced)\n# This reduces CPU overhead for repeated execution\n</code></pre>"},{"location":"concepts/execution-plans/#debugging-execution-plans","title":"Debugging Execution Plans","text":""},{"location":"concepts/execution-plans/#enable-logging","title":"Enable Logging","text":"<pre><code>export CUDNN_FRONTEND_LOG_INFO=1\nexport CUDNN_FRONTEND_LOG_FILE=cudnn_log.txt\n</code></pre>"},{"location":"concepts/execution-plans/#check-support-issues","title":"Check Support Issues","text":"<pre><code>try:\n    graph.check_support()\nexcept Exception as e:\n    print(f\"No supported plan: {e}\")\n    # Try different parameters or check GPU compatibility\n</code></pre>"},{"location":"concepts/execution-plans/#common-issues","title":"Common Issues","text":"<p>No Execution Plan Found</p> <p>Cause: Operation not supported on your GPU</p> <p>Solutions:</p> <ul> <li>Check GPU compute capability (need SM 7.0+)</li> <li>Verify cuDNN version compatibility</li> <li>Try different tensor sizes/shapes</li> <li>Use FALLBACK heuristic</li> </ul> <p>Workspace Too Large</p> <p>Cause: Best algorithm needs more memory than available</p> <p>Solutions:</p> <ul> <li>Reduce batch size</li> <li>Use workspace limit parameter</li> <li>Accept slower algorithm</li> </ul>"},{"location":"concepts/execution-plans/#performance-tips","title":"Performance Tips","text":"<p>Execution Plan Optimization</p> <ol> <li>Autotune for production - The time investment pays off</li> <li>Cache plans - Serialize and reuse</li> <li>Right-size workspace - More workspace = faster algorithms</li> <li>Match shapes - Consistent shapes enable plan reuse</li> <li>Use channels-last - Best algorithm availability</li> </ol>"},{"location":"concepts/execution-plans/#execution-plan-lifecycle-summary","title":"Execution Plan Lifecycle Summary","text":"<pre><code>graph TD\n    A[Create Graph] --&gt; B[Add Operations]\n    B --&gt; C[Validate]\n    C --&gt; D[Generate Plans]\n    D --&gt; E{Autotune?}\n    E --&gt;|Yes| F[Benchmark All Plans]\n    E --&gt;|No| G[Use Heuristic Selection]\n    F --&gt; H[Select Best Plan]\n    G --&gt; H\n    H --&gt; I[Build Plan]\n    I --&gt; J[Execute]\n    J --&gt; K{Same Shapes?}\n    K --&gt;|Yes| J\n    K --&gt;|No| A</code></pre>"},{"location":"concepts/execution-plans/#next-steps","title":"Next Steps","text":"<p>Learn about memory management and performance optimization.</p> <p>Memory &amp; Performance </p>"},{"location":"concepts/graphs/","title":"Understanding Graphs","text":"<p>The Graph API is the heart of cuDNN Frontend. This page explains what graphs are, why they matter, and how to think about them.</p>"},{"location":"concepts/graphs/#what-is-a-computation-graph","title":"What is a Computation Graph?","text":"<p>A computation graph is a visual representation of mathematical operations. Instead of writing sequential code, you describe your computation as a network of connected operations.</p> <pre><code>graph LR\n    subgraph \"Inputs\"\n        X[Tensor X]\n        W[Weight W]\n        B[Bias B]\n    end\n\n    subgraph \"Operations\"\n        Conv[Convolution]\n        Add[Add]\n        ReLU[ReLU]\n    end\n\n    subgraph \"Output\"\n        Y[Output Y]\n    end\n\n    X --&gt; Conv\n    W --&gt; Conv\n    Conv --&gt; Add\n    B --&gt; Add\n    Add --&gt; ReLU\n    ReLU --&gt; Y\n\n    style X fill:#bbdefb\n    style W fill:#bbdefb\n    style B fill:#bbdefb\n    style Conv fill:#fff9c4\n    style Add fill:#fff9c4\n    style ReLU fill:#fff9c4\n    style Y fill:#c8e6c9</code></pre> <p>This graph describes: Y = ReLU(Conv(X, W) + B)</p>"},{"location":"concepts/graphs/#why-graphs","title":"Why Graphs?","text":""},{"location":"concepts/graphs/#traditional-approach-problems","title":"Traditional Approach (Problems)","text":"<pre><code># Sequential execution - each line runs independently\nconv_result = convolution(x, w)       # Launch kernel 1, write to memory\nbias_result = add(conv_result, bias)  # Read from memory, launch kernel 2, write\nfinal = relu(bias_result)             # Read from memory, launch kernel 3, write\n</code></pre> <p>Problems:</p> <ul> <li>Three separate kernel launches - each has overhead</li> <li>Three memory round-trips - reading/writing intermediate results is slow</li> <li>No cross-operation optimization - the system can't see the big picture</li> </ul>"},{"location":"concepts/graphs/#graph-approach-solution","title":"Graph Approach (Solution)","text":"<pre><code># Graph-based - describe everything, then execute once\nwith cudnn.Graph() as graph:\n    conv_result = graph.conv_fprop(x, w)\n    bias_result = graph.add(conv_result, bias)\n    final = graph.relu(bias_result)\n    final.set_output(True)\n\nresult = graph(x, w, bias)  # Single optimized execution\n</code></pre> <p>Benefits:</p> <ul> <li>One kernel launch - operations are fused together</li> <li>One memory round-trip - intermediate results stay in registers</li> <li>Global optimization - cuDNN optimizes the entire graph</li> </ul> <pre><code>graph TD\n    subgraph \"Traditional: 3 Kernels\"\n        T1[Conv Kernel] --&gt; TM1[Memory]\n        TM1 --&gt; T2[Add Kernel]\n        T2 --&gt; TM2[Memory]\n        TM2 --&gt; T3[ReLU Kernel]\n    end\n\n    subgraph \"Graph: 1 Fused Kernel\"\n        G1[Conv + Add + ReLU]\n    end\n\n    style T1 fill:#ffcdd2\n    style T2 fill:#ffcdd2\n    style T3 fill:#ffcdd2\n    style TM1 fill:#e0e0e0\n    style TM2 fill:#e0e0e0\n    style G1 fill:#c8e6c9</code></pre>"},{"location":"concepts/graphs/#graph-lifecycle","title":"Graph Lifecycle","text":"<p>A cuDNN graph goes through several phases:</p> <pre><code>graph LR\n    A[Creation] --&gt; B[Building]\n    B --&gt; C[Validation]\n    C --&gt; D[Planning]\n    D --&gt; E[Execution]\n\n    style A fill:#e3f2fd\n    style B fill:#fff9c4\n    style C fill:#ffe0b2\n    style D fill:#f3e5f5\n    style E fill:#c8e6c9</code></pre>"},{"location":"concepts/graphs/#phase-1-creation","title":"Phase 1: Creation","text":"<p>Create an empty graph with global settings:</p> <pre><code>graph = cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,       # Default I/O precision\n    compute_data_type=cudnn.data_type.FLOAT,  # Internal computation precision\n    intermediate_data_type=cudnn.data_type.FLOAT,  # Intermediate tensor precision\n    name=\"my_graph\",  # Optional name for debugging\n)\n</code></pre>"},{"location":"concepts/graphs/#phase-2-building","title":"Phase 2: Building","text":"<p>Add operations (nodes) to the graph:</p> <pre><code># Each operation returns its output tensor(s)\nconv_out = graph.conv_fprop(image=x, weight=w, padding=[1, 1])\nrelu_out = graph.relu(input=conv_out)\n\n# Mark which tensors are outputs\nrelu_out.set_output(True)\n</code></pre>"},{"location":"concepts/graphs/#phase-3-validation","title":"Phase 3: Validation","text":"<p>The graph checks that everything is correct:</p> <ul> <li>Tensor shapes are compatible</li> <li>Data types make sense</li> <li>Operations are supported</li> </ul> <pre><code># Validation happens automatically when using context manager\nwith cudnn.Graph() as graph:\n    # ... build graph ...\n# Validation runs here when exiting the context\n\n# Or manually:\ngraph.validate()\n</code></pre>"},{"location":"concepts/graphs/#phase-4-planning","title":"Phase 4: Planning","text":"<p>cuDNN finds the best way to execute your graph:</p> <pre><code># Happens automatically, or manually:\ngraph.build_operation_graph()\ngraph.create_execution_plans([cudnn.heur_mode.A, cudnn.heur_mode.FALLBACK])\ngraph.check_support()\ngraph.build_plans()\n</code></pre>"},{"location":"concepts/graphs/#phase-5-execution","title":"Phase 5: Execution","text":"<p>Run the graph with actual data:</p> <pre><code>result = graph(x, w, handle=handle)\n</code></pre>"},{"location":"concepts/graphs/#graph-properties","title":"Graph Properties","text":""},{"location":"concepts/graphs/#io_data_type","title":"io_data_type","text":"<p>Default data type for input/output tensors:</p> <pre><code>cudnn.data_type.HALF     # FP16 - fastest on modern GPUs\ncudnn.data_type.FLOAT    # FP32 - full precision\ncudnn.data_type.BFLOAT16 # BF16 - good range, less precision\ncudnn.data_type.FP8_E4M3 # FP8 - extreme performance (Hopper+)\n</code></pre>"},{"location":"concepts/graphs/#compute_data_type","title":"compute_data_type","text":"<p>Data type for internal calculations:</p> <pre><code># Common pattern: FP16 I/O with FP32 accumulation\ngraph = cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,  # Prevents overflow\n)\n</code></pre>"},{"location":"concepts/graphs/#intermediate_data_type","title":"intermediate_data_type","text":"<p>For tensors between operations:</p> <pre><code># Useful for attention where softmax needs higher precision\ngraph = cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    intermediate_data_type=cudnn.data_type.FLOAT,\n    compute_data_type=cudnn.data_type.FLOAT,\n)\n</code></pre>"},{"location":"concepts/graphs/#graph-reuse","title":"Graph Reuse","text":"<p>One of the biggest advantages of graphs is reuse. Build once, execute many times:</p> <pre><code># Build the graph (expensive, do once)\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(image=x, weight=w, padding=[1, 1])\n    y.set_output(True)\n\n# Execute many times (cheap, do often)\nfor batch in dataloader:\n    result = graph(batch.x, batch.w, handle=handle)\n</code></pre> <p>Reuse Rules</p> <p>A graph can be reused only if the new tensors have:</p> <ul> <li>Same shapes as the original</li> <li>Same data types as the original</li> <li>Same memory layout as the original</li> </ul>"},{"location":"concepts/graphs/#virtual-vs-real-tensors","title":"Virtual vs Real Tensors","text":"<p>When building a graph, intermediate tensors are virtual:</p> <pre><code>with cudnn.Graph() as graph:\n    conv_out = graph.conv_fprop(x, w)   # conv_out is virtual\n    relu_out = graph.relu(conv_out)     # relu_out is virtual\n    relu_out.set_output(True)           # Now relu_out is REAL\n\n# Virtual tensors don't need memory allocation\n# Only real (output) tensors have device memory\n</code></pre> <pre><code>graph LR\n    X[X&lt;br&gt;REAL] --&gt; Conv\n    W[W&lt;br&gt;REAL] --&gt; Conv\n    Conv --&gt; C[conv_out&lt;br&gt;VIRTUAL]\n    C --&gt; ReLU\n    ReLU --&gt; R[relu_out&lt;br&gt;REAL]\n\n    style X fill:#c8e6c9\n    style W fill:#c8e6c9\n    style C fill:#fff9c4,stroke-dasharray: 5 5\n    style R fill:#c8e6c9</code></pre>"},{"location":"concepts/graphs/#inputoutput-specification","title":"Input/Output Specification","text":"<p>There are multiple ways to specify graph inputs and outputs:</p>"},{"location":"concepts/graphs/#method-1-using-context-manager-arguments","title":"Method 1: Using Context Manager Arguments","text":"<pre><code>with cudnn.Graph(\n    inputs=[\"conv::image\", \"conv::weight\"],\n    outputs=[\"output\"],\n) as graph:\n    y = graph.conv_fprop(image=x, weight=w, name=\"conv\")\n    y.set_output(True).set_name(\"output\")\n\n# Execute with positional arguments\nresult = graph(x, w, handle=handle)\n</code></pre>"},{"location":"concepts/graphs/#method-2-using-set_io_tuples","title":"Method 2: Using set_io_tuples","text":"<pre><code>with cudnn.Graph() as graph:\n    y = graph.conv_fprop(image=x, weight=w, name=\"conv\")\n    y.set_output(True)\n\ngraph.set_io_tuples([\"conv::image\", \"conv::weight\"], [\"conv::Y\"])\nresult = graph(x, w, handle=handle)\n</code></pre>"},{"location":"concepts/graphs/#method-3-using-dictionary","title":"Method 3: Using Dictionary","text":"<pre><code>with cudnn.Graph() as graph:\n    y = graph.conv_fprop(image=x, weight=w, name=\"conv\")\n    y.set_output(True)\n\n# More flexible - no need to specify order\nresult = graph({\n    \"conv::image\": x,\n    \"conv::weight\": w,\n}, handle=handle)\noutput = result[\"conv::Y\"]\n</code></pre>"},{"location":"concepts/graphs/#debugging-graphs","title":"Debugging Graphs","text":""},{"location":"concepts/graphs/#print-graph-info","title":"Print Graph Info","text":"<pre><code>print(graph)  # Shows graph structure after building\n</code></pre>"},{"location":"concepts/graphs/#enable-logging","title":"Enable Logging","text":"<pre><code>import os\nos.environ[\"CUDNN_FRONTEND_LOG_INFO\"] = \"1\"\nos.environ[\"CUDNN_FRONTEND_LOG_FILE\"] = \"stdout\"  # or a filename\n</code></pre>"},{"location":"concepts/graphs/#check-support","title":"Check Support","text":"<pre><code>try:\n    graph.check_support()\n    print(\"Graph is supported!\")\nexcept Exception as e:\n    print(f\"Graph not supported: {e}\")\n</code></pre>"},{"location":"concepts/graphs/#best-practices","title":"Best Practices","text":"<p>Do</p> <ul> <li>Reuse graphs when possible</li> <li>Use FP16/BF16 for I/O with FP32 compute</li> <li>Mark only necessary tensors as output</li> <li>Use meaningful names for debugging</li> </ul> <p>Don't</p> <ul> <li>Rebuild graphs unnecessarily</li> <li>Mark virtual tensors as output when not needed</li> <li>Use mismatched tensor layouts</li> <li>Forget to destroy handles</li> </ul>"},{"location":"concepts/graphs/#next-steps","title":"Next Steps","text":"<p>Now that you understand graphs, learn how cuDNN selects and optimizes execution plans.</p> <p>Execution Plans </p>"},{"location":"concepts/memory-performance/","title":"Memory &amp; Performance","text":"<p>Optimizing memory usage and performance is crucial for production deep learning. This page covers best practices for getting the most out of cuDNN Frontend.</p>"},{"location":"concepts/memory-performance/#gpu-memory-hierarchy","title":"GPU Memory Hierarchy","text":"<p>Understanding GPU memory is key to optimization:</p> <pre><code>graph TD\n    subgraph \"On-Chip (Fast)\"\n        REG[Registers&lt;br&gt;~20KB/SM&lt;br&gt;~1 cycle]\n        SMEM[Shared Memory&lt;br&gt;~100KB/SM&lt;br&gt;~20 cycles]\n        L1[L1 Cache&lt;br&gt;~128KB/SM&lt;br&gt;~30 cycles]\n    end\n\n    subgraph \"On-Package (Medium)\"\n        L2[L2 Cache&lt;br&gt;40-80MB&lt;br&gt;~200 cycles]\n    end\n\n    subgraph \"Off-Chip (Slow)\"\n        HBM[HBM/GDDR&lt;br&gt;16-80GB&lt;br&gt;~400+ cycles]\n    end\n\n    REG &lt;--&gt; SMEM &lt;--&gt; L1 &lt;--&gt; L2 &lt;--&gt; HBM\n\n    style REG fill:#76ff03\n    style SMEM fill:#c8e6c9\n    style L1 fill:#c8e6c9\n    style L2 fill:#fff9c4\n    style HBM fill:#ffcdd2</code></pre> <p>Key insight: Most deep learning is memory-bound, not compute-bound. Reducing memory traffic is often more important than reducing FLOPs.</p>"},{"location":"concepts/memory-performance/#why-cudnn-frontend-is-fast","title":"Why cuDNN Frontend is Fast","text":""},{"location":"concepts/memory-performance/#1-operation-fusion","title":"1. Operation Fusion","text":"<p>Without fusion:</p> <pre><code>Conv \u2192 Write to HBM \u2192 Read from HBM \u2192 ReLU \u2192 Write to HBM\n</code></pre> <p>With cuDNN Frontend fusion:</p> <pre><code>Conv \u2192 ReLU (all in registers) \u2192 Write to HBM\n</code></pre> <p>This can be 2-3x faster for memory-bound operations!</p>"},{"location":"concepts/memory-performance/#2-optimized-memory-access-patterns","title":"2. Optimized Memory Access Patterns","text":"<p>cuDNN ensures:</p> <ul> <li>Coalesced memory access: Adjacent threads access adjacent memory</li> <li>Optimal tiling: Data fits in shared memory</li> <li>Bank conflict avoidance: Parallel shared memory access</li> </ul>"},{"location":"concepts/memory-performance/#3-tensor-core-utilization","title":"3. Tensor Core Utilization","text":"<p>Modern GPUs have Tensor Cores - specialized hardware for matrix operations:</p> Precision Tensor Core Speed Standard Speed FP16 312 TFLOPS (A100) 19.5 TFLOPS TF32 156 TFLOPS 19.5 TFLOPS FP8 624 TFLOPS (H100) N/A <p>cuDNN Frontend automatically uses Tensor Cores when possible.</p>"},{"location":"concepts/memory-performance/#memory-optimization-strategies","title":"Memory Optimization Strategies","text":""},{"location":"concepts/memory-performance/#use-appropriate-data-types","title":"Use Appropriate Data Types","text":"<pre><code># FP32: Most memory, best precision\nx_fp32 = torch.randn(1024, 1024, dtype=torch.float32)  # 4 MB\n\n# FP16: Half memory, good precision\nx_fp16 = torch.randn(1024, 1024, dtype=torch.float16)  # 2 MB\n\n# FP8: Quarter memory (inference)\n# Supported on Hopper and newer\n</code></pre> <p>Memory savings by precision:</p> Precision Relative Size Use Case FP32 1.0x Reference, debugging FP16/BF16 0.5x Training, inference FP8 0.25x Inference (Hopper+) INT8 0.25x Quantized inference"},{"location":"concepts/memory-performance/#virtual-tensors","title":"Virtual Tensors","text":"<p>Intermediate tensors marked as virtual don't allocate memory:</p> <pre><code>with cudnn.Graph() as graph:\n    # Only x, w, and final output need memory\n    conv = graph.conv_fprop(x, w)      # Virtual - no allocation\n    bn = graph.batchnorm(conv, ...)    # Virtual - no allocation\n    relu = graph.relu(bn)              # Virtual - no allocation\n    pool = graph.pooling(relu, ...)\n    pool.set_output(True)              # Real - needs memory\n</code></pre>"},{"location":"concepts/memory-performance/#workspace-management","title":"Workspace Management","text":"<p>cuDNN needs temporary workspace memory:</p> <pre><code># Query workspace requirement\nworkspace_size = graph.get_workspace_size()\n\n# Option 1: Allocate exactly what's needed\nworkspace = torch.empty(workspace_size, device=\"cuda\", dtype=torch.uint8)\n\n# Option 2: Pre-allocate generous workspace for all graphs\nMAX_WORKSPACE = 1024 * 1024 * 1024  # 1 GB\nglobal_workspace = torch.empty(MAX_WORKSPACE, device=\"cuda\", dtype=torch.uint8)\n</code></pre>"},{"location":"concepts/memory-performance/#memory-efficient-attention","title":"Memory-Efficient Attention","text":"<p>For transformers, use SDPA which implements flash attention:</p> <pre><code># Standard attention: O(N\u00b2) memory\n# scores = Q @ K.T  # [B, H, N, N] - huge for large N!\n\n# cuDNN SDPA: O(N) memory\no, _ = graph.sdpa(q, k, v, attn_scale=scale)\n# Never materializes full attention matrix!\n</code></pre>"},{"location":"concepts/memory-performance/#performance-optimization","title":"Performance Optimization","text":""},{"location":"concepts/memory-performance/#1-batch-size-tuning","title":"1. Batch Size Tuning","text":"<p>Larger batches = better GPU utilization:</p> Batch Size GPU Utilization Memory Training Speed 1 10-20% Low Slow 32 60-80% Medium Good 128 90%+ High Best 512+ ~100% Very High Optimal <pre><code># Find optimal batch size\nfor batch_size in [32, 64, 128, 256]:\n    try:\n        x = torch.randn(batch_size, 64, 224, 224, device=\"cuda\")\n        # ... test training loop ...\n    except RuntimeError as e:  # Out of memory\n        print(f\"Max batch size: {batch_size // 2}\")\n        break\n</code></pre>"},{"location":"concepts/memory-performance/#2-graph-reuse","title":"2. Graph Reuse","text":"<p>Building graphs is expensive. Always reuse:</p> <pre><code># BAD: Rebuilding every iteration\nfor batch in dataloader:\n    with cudnn.Graph() as graph:  # Slow!\n        y = graph.conv_fprop(batch.x, w)\n        y.set_output(True)\n    result = graph(batch.x, w, handle=handle)\n\n# GOOD: Build once, reuse\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(template_x, w)\n    y.set_output(True)\n\nfor batch in dataloader:\n    result = graph(batch.x, w, handle=handle)  # Fast!\n</code></pre>"},{"location":"concepts/memory-performance/#3-autotuning","title":"3. Autotuning","text":"<p>Invest time upfront for faster execution:</p> <pre><code># Autotune once at startup\nbest_plan = graph.autotune(variant_pack, workspace, handle=handle)\n\n# Save for future runs\nwith open(\"tuned_plan.bin\", \"wb\") as f:\n    f.write(graph.serialize())\n</code></pre>"},{"location":"concepts/memory-performance/#4-mixed-precision","title":"4. Mixed Precision","text":"<p>Use lower precision where possible:</p> <pre><code># Optimal mixed precision setup\ngraph = cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,       # FP16 I/O\n    intermediate_data_type=cudnn.data_type.FLOAT,  # FP32 softmax\n    compute_data_type=cudnn.data_type.FLOAT,  # FP32 accumulation\n)\n</code></pre>"},{"location":"concepts/memory-performance/#5-memory-layout","title":"5. Memory Layout","text":"<p>Channels-last (NHWC) is faster on modern GPUs:</p> <pre><code># BAD: Channels-first (NCHW)\nx = torch.randn(8, 64, 224, 224, device=\"cuda\")\n\n# GOOD: Channels-last (NHWC)\nx = torch.randn(8, 64, 224, 224, device=\"cuda\").to(\n    memory_format=torch.channels_last\n)\n</code></pre>"},{"location":"concepts/memory-performance/#profiling","title":"Profiling","text":""},{"location":"concepts/memory-performance/#cudnn-logging","title":"cuDNN Logging","text":"<pre><code># Enable detailed logging\nexport CUDNN_FRONTEND_LOG_INFO=1\nexport CUDNN_FRONTEND_LOG_FILE=cudnn_perf.log\n</code></pre>"},{"location":"concepts/memory-performance/#nvidia-nsight-systems","title":"NVIDIA Nsight Systems","text":"<p>Profile the full stack:</p> <pre><code>nsys profile -o report python train.py\n</code></pre>"},{"location":"concepts/memory-performance/#pytorch-profiler","title":"PyTorch Profiler","text":"<pre><code>with torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n    with_stack=True,\n) as prof:\n    result = graph(x, w, handle=handle)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n</code></pre>"},{"location":"concepts/memory-performance/#memory-debugging","title":"Memory Debugging","text":""},{"location":"concepts/memory-performance/#check-memory-usage","title":"Check Memory Usage","text":"<pre><code># Current allocation\nprint(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n\n# Peak allocation\nprint(f\"Peak: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n\n# Reserved by allocator\nprint(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n</code></pre>"},{"location":"concepts/memory-performance/#find-memory-leaks","title":"Find Memory Leaks","text":"<pre><code>torch.cuda.reset_peak_memory_stats()\n\n# Your code here\nresult = graph(x, w, handle=handle)\n\nprint(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n</code></pre>"},{"location":"concepts/memory-performance/#memory-snapshot","title":"Memory Snapshot","text":"<pre><code># For detailed memory analysis\ntorch.cuda.memory._record_memory_history()\n\n# ... your operations ...\n\ntorch.cuda.memory._dump_snapshot(\"memory_snapshot.pickle\")\n</code></pre>"},{"location":"concepts/memory-performance/#performance-checklist","title":"Performance Checklist","text":"<p>Before Production Deployment</p> <ul> <li>[ ] Use FP16/BF16 where accuracy permits</li> <li>[ ] Enable channels-last memory format</li> <li>[ ] Autotune execution plans</li> <li>[ ] Cache/serialize optimized plans</li> <li>[ ] Reuse graphs (don't rebuild)</li> <li>[ ] Profile with Nsight Systems</li> <li>[ ] Test with production batch sizes</li> <li>[ ] Verify memory usage fits target GPU</li> </ul>"},{"location":"concepts/memory-performance/#common-performance-issues","title":"Common Performance Issues","text":"<p>Slow First Iteration</p> <p>Cause: JIT compilation, graph building</p> <p>Solution: Warm up with a dummy batch before timing</p> <p>High Memory Usage</p> <p>Cause: Large intermediate tensors, inefficient algorithm</p> <p>Solutions:</p> <ul> <li>Use virtual tensors where possible</li> <li>Limit workspace size</li> <li>Reduce batch size</li> <li>Use gradient checkpointing</li> </ul> <p>Low GPU Utilization</p> <p>Cause: Small batch size, CPU bottleneck, memory transfers</p> <p>Solutions:</p> <ul> <li>Increase batch size</li> <li>Use data prefetching</li> <li>Profile to find bottleneck</li> <li>Ensure data is already on GPU</li> </ul>"},{"location":"concepts/memory-performance/#architecture-specific-tips","title":"Architecture-Specific Tips","text":""},{"location":"concepts/memory-performance/#ampere-a100-rtx-30xx","title":"Ampere (A100, RTX 30xx)","text":"<ul> <li>Use TF32 for faster FP32-like training</li> <li>BF16 well-supported</li> <li>Large L2 cache (40MB)</li> </ul>"},{"location":"concepts/memory-performance/#hopper-h100","title":"Hopper (H100)","text":"<ul> <li>Use FP8 for maximum throughput</li> <li>Transformer Engine integration</li> <li>TMA (Tensor Memory Accelerator)</li> </ul>"},{"location":"concepts/memory-performance/#ada-rtx-40xx","title":"Ada (RTX 40xx)","text":"<ul> <li>Good FP8 support</li> <li>Efficient for inference</li> <li>Strong for mixed workloads</li> </ul>"},{"location":"concepts/memory-performance/#blackwell-b100-b200-b300-rtx-50xx","title":"Blackwell (B100, B200, B300, RTX 50xx)","text":"<ul> <li>Enhanced FP8 and FP4 support</li> <li>Improved Transformer Engine</li> <li>Higher memory bandwidth</li> <li>Better multi-GPU scaling</li> </ul>"},{"location":"concepts/memory-performance/#summary","title":"Summary","text":"Optimization Impact Effort Graph reuse High Low Channels-last High Low Mixed precision High Medium Autotuning Medium Low Virtual tensors Medium Low Workspace tuning Low Low"},{"location":"concepts/memory-performance/#next-steps","title":"Next Steps","text":"<p>Put these concepts into practice with hands-on tutorials!</p> <p>Convolution Tutorial </p>"},{"location":"concepts/operations/","title":"Operations","text":"<p>Operations are the building blocks of your computation graphs. This page covers the main operation types in cuDNN Frontend.</p>"},{"location":"concepts/operations/#operation-categories","title":"Operation Categories","text":"<p>cuDNN Frontend supports these main categories:</p> <pre><code>graph TD\n    subgraph \"Convolution\"\n        Conv[conv_fprop]\n        Dgrad[conv_dgrad]\n        Wgrad[conv_wgrad]\n    end\n\n    subgraph \"Matrix Ops\"\n        Matmul[matmul]\n    end\n\n    subgraph \"Attention\"\n        SDPA[sdpa]\n        SDPABack[sdpa_backward]\n    end\n\n    subgraph \"Normalization\"\n        LayerNorm[layernorm]\n        BatchNorm[batchnorm]\n        RMSNorm[rmsnorm]\n    end\n\n    subgraph \"Pointwise\"\n        ReLU[relu]\n        Add[add]\n        Mul[mul]\n    end</code></pre>"},{"location":"concepts/operations/#convolution-operations","title":"Convolution Operations","text":""},{"location":"concepts/operations/#forward-convolution-conv_fprop","title":"Forward Convolution (conv_fprop)","text":"<p>The workhorse of computer vision - slides filters over images:</p> <pre><code>y = graph.conv_fprop(\n    image=x,           # Input tensor [N, C, H, W]\n    weight=w,          # Filter tensor [K, C, R, S]\n    padding=[1, 1],    # Padding [pad_h, pad_w]\n    stride=[1, 1],     # Stride [stride_h, stride_w]\n    dilation=[1, 1],   # Dilation [dilation_h, dilation_w]\n    compute_data_type=cudnn.data_type.FLOAT,\n    name=\"conv\"\n)\n</code></pre> <p>Parameters explained:</p> Parameter Description Typical Values padding Zero-padding added to input [1,1] for 3x3 kernel stride Step size of the filter [1,1] or [2,2] dilation Spacing between filter elements [1,1] (no dilation) <pre><code>graph LR\n    subgraph \"Convolution\"\n        I[\"Input&lt;br&gt;[8,64,56,56]\"]\n        W[\"Weight&lt;br&gt;[128,64,3,3]\"]\n        C[conv_fprop]\n        O[\"Output&lt;br&gt;[8,128,56,56]\"]\n    end\n\n    I --&gt; C\n    W --&gt; C\n    C --&gt; O</code></pre>"},{"location":"concepts/operations/#data-gradient-conv_dgrad","title":"Data Gradient (conv_dgrad)","text":"<p>Backpropagation through convolution - computes gradient w.r.t input:</p> <pre><code>dx = graph.conv_dgrad(\n    weight=w,          # Same weights used in forward\n    loss=dy,           # Gradient from next layer\n    padding=[1, 1],\n    stride=[1, 1],\n    dilation=[1, 1],\n)\n</code></pre>"},{"location":"concepts/operations/#weight-gradient-conv_wgrad","title":"Weight Gradient (conv_wgrad)","text":"<p>Computes gradient w.r.t weights for training:</p> <pre><code>dw = graph.conv_wgrad(\n    image=x,           # Original input\n    loss=dy,           # Gradient from next layer\n    padding=[1, 1],\n    stride=[1, 1],\n    dilation=[1, 1],\n)\n</code></pre>"},{"location":"concepts/operations/#matrix-multiplication","title":"Matrix Multiplication","text":""},{"location":"concepts/operations/#basic-matmul","title":"Basic Matmul","text":"<p>General matrix multiplication: <code>C = A @ B</code></p> <pre><code>C = graph.matmul(\n    A=A,               # [M, K] or [B, M, K]\n    B=B,               # [K, N] or [B, K, N]\n    compute_data_type=cudnn.data_type.FLOAT,\n    name=\"gemm\"\n)\n</code></pre> <p>Batch matmul is supported:</p> <pre><code># Batched: [32, 512, 256] @ [32, 256, 1024] = [32, 512, 1024]\nA = torch.randn(32, 512, 256, device=\"cuda\", dtype=torch.float16)\nB = torch.randn(32, 256, 1024, device=\"cuda\", dtype=torch.float16)\n\nC = graph.matmul(A=A, B=B)\n</code></pre>"},{"location":"concepts/operations/#attention-operations","title":"Attention Operations","text":""},{"location":"concepts/operations/#scaled-dot-product-attention-sdpa","title":"Scaled Dot-Product Attention (SDPA)","text":"<p>The core of transformer models:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\\] <pre><code>import math\n\n# Q, K, V: [batch, num_heads, seq_len, head_dim]\no, stats = graph.sdpa(\n    q=Q,\n    k=K,\n    v=V,\n    attn_scale=1.0 / math.sqrt(head_dim),\n    is_inference=True,         # False for training\n    use_causal_mask=True,      # Autoregressive mask\n    name=\"attention\"\n)\n</code></pre> <p>Key parameters:</p> Parameter Description attn_scale Scaling factor (usually 1/sqrt(d_k)) is_inference True for inference, False for training use_causal_mask Triangular mask for autoregressive generate_stats Return stats for backward pass <p>For training (backward pass):</p> <pre><code># Forward with stats\no, stats = graph.sdpa(\n    q=Q, k=K, v=V,\n    attn_scale=attn_scale,\n    is_inference=False,\n    generate_stats=True,\n)\n\n# Backward\ndQ, dK, dV = graph.sdpa_backward(\n    q=Q, k=K, v=V, o=o,\n    dO=grad_output,\n    stats=stats,\n    attn_scale=attn_scale,\n)\n</code></pre>"},{"location":"concepts/operations/#normalization-operations","title":"Normalization Operations","text":""},{"location":"concepts/operations/#layer-normalization","title":"Layer Normalization","text":"<p>Normalizes across feature dimension:</p> <pre><code># Input: [batch, seq_len, hidden_dim]\ny, mean, inv_var = graph.layernorm(\n    input=x,\n    scale=gamma,       # [hidden_dim]\n    bias=beta,         # [hidden_dim]\n    epsilon=1e-5,\n    name=\"layernorm\"\n)\n</code></pre>"},{"location":"concepts/operations/#rms-normalization","title":"RMS Normalization","text":"<p>Root Mean Square normalization (used in LLaMA, etc.):</p> <pre><code>y, inv_rms = graph.rmsnorm(\n    input=x,\n    scale=gamma,\n    epsilon=1e-5,\n)\n</code></pre>"},{"location":"concepts/operations/#batch-normalization","title":"Batch Normalization","text":"<p>Normalizes across batch dimension:</p> <pre><code># Training\ny, running_mean, running_var, saved_mean, saved_inv_var = graph.batchnorm(\n    input=x,\n    scale=gamma,\n    bias=beta,\n    running_mean=running_mean,\n    running_var=running_var,\n    epsilon=1e-5,\n    momentum=0.1,\n)\n\n# Inference\ny = graph.batchnorm_inference(\n    input=x,\n    scale=gamma,\n    bias=beta,\n    mean=running_mean,\n    inv_variance=1.0 / torch.sqrt(running_var + 1e-5),\n)\n</code></pre>"},{"location":"concepts/operations/#pointwise-operations","title":"Pointwise Operations","text":"<p>Pointwise operations apply element-by-element transformations:</p>"},{"location":"concepts/operations/#activation-functions","title":"Activation Functions","text":"<pre><code># ReLU: max(0, x)\ny = graph.relu(input=x)\n\n# GELU: x * \u03a6(x)\ny = graph.gelu(input=x)\n\n# SiLU/Swish: x * sigmoid(x)\ny = graph.silu(input=x)\n\n# Tanh\ny = graph.tanh(input=x)\n\n# Sigmoid\ny = graph.sigmoid(input=x)\n</code></pre>"},{"location":"concepts/operations/#arithmetic-operations","title":"Arithmetic Operations","text":"<pre><code># Addition\ny = graph.add(a=x, b=z)\n\n# Multiplication\ny = graph.mul(a=x, b=z)\n\n# Bias addition\ny = graph.bias(input=x, bias=b)\n\n# Scale\ny = graph.scale(input=x, scale=s)\n</code></pre>"},{"location":"concepts/operations/#custom-pointwise","title":"Custom Pointwise","text":"<p>For operations not directly exposed:</p> <pre><code># Using pointwise descriptor\npw_desc = cudnn.create_pointwise_descriptor(\n    mode=cudnn.pointwise_mode.EXP\n)\ny = graph.pointwise(\n    input=x,\n    descriptor=pw_desc,\n)\n</code></pre>"},{"location":"concepts/operations/#operation-fusion","title":"Operation Fusion","text":"<p>One of cuDNN Frontend's superpowers is operation fusion - combining multiple operations into one kernel:</p> <pre><code>with cudnn.Graph() as graph:\n    # These will be fused into a single kernel!\n    conv_out = graph.conv_fprop(x, w, padding=[1,1])\n    bias_out = graph.bias(conv_out, b)\n    bn_out = graph.batchnorm(bias_out, ...)\n    relu_out = graph.relu(bn_out)\n    relu_out.set_output(True)\n</code></pre>"},{"location":"concepts/operations/#fusion-patterns","title":"Fusion Patterns","text":"<p>Common patterns that fuse well:</p> Pattern Example Conv + Bias + Activation <code>conv -&gt; bias -&gt; relu</code> Matmul + Bias + Activation <code>matmul -&gt; bias -&gt; gelu</code> Norm + Pointwise <code>layernorm -&gt; dropout</code> SDPA (all internal ops) Q@K, softmax, dropout, @V"},{"location":"concepts/operations/#virtual-tensors-enable-fusion","title":"Virtual Tensors Enable Fusion","text":"<pre><code># Only relu_out needs memory - others stay in registers\nconv_out = graph.conv_fprop(x, w)   # Virtual\nbias_out = graph.bias(conv_out, b)  # Virtual\nrelu_out = graph.relu(bias_out)     # Real (set_output)\nrelu_out.set_output(True)\n</code></pre>"},{"location":"concepts/operations/#output-tensor-configuration","title":"Output Tensor Configuration","text":"<p>After an operation, configure the output:</p> <pre><code>y = graph.conv_fprop(x, w)\n\n# Mark as output (required to retrieve)\ny.set_output(True)\n\n# Optional: give it a name\ny.set_name(\"conv_output\")\n\n# Optional: force data type\ny.set_data_type(cudnn.data_type.FLOAT)\n\n# Optional: set explicit dimensions\ny.set_dim([8, 128, 56, 56])\ny.set_stride([128*56*56, 1, 128*56, 128])\n</code></pre>"},{"location":"concepts/operations/#operation-naming","title":"Operation Naming","text":"<p>Naming helps with debugging and I/O specification:</p> <pre><code>y = graph.conv_fprop(x, w, name=\"encoder_conv1\")\n\n# Reference inputs/outputs by name:\n# - \"encoder_conv1::image\"  (input)\n# - \"encoder_conv1::weight\" (input)\n# - \"encoder_conv1::Y\"      (output)\n</code></pre>"},{"location":"concepts/operations/#next-steps","title":"Next Steps","text":"<p>Learn how to combine operations into computation graphs.</p> <p>Understanding Graphs </p>"},{"location":"concepts/tensors/","title":"Tensors Explained","text":"<p>Tensors are the fundamental data structures in deep learning. This page explains how cuDNN Frontend handles tensors and the critical details you need to know.</p>"},{"location":"concepts/tensors/#what-is-a-tensor","title":"What is a Tensor?","text":"<p>A tensor is a multi-dimensional array of numbers. Unlike general-purpose frameworks, cuDNN operations expect specific tensor dimensions:</p> Operation Required Dims Shape Notes MatMul 3D <code>[B, M, K] \u00d7 [B, K, N]</code> Use <code>B=1</code> for non-batched Attention (SDPA) 4D <code>[B, H, S, D]</code> Batch, Heads, SeqLen, HeadDim Conv 2D 4D <code>[N, C, H, W]</code> Batch, Channels, Height, Width Conv 3D 5D <code>[N, C, D, H, W]</code> Adds Depth dimension Normalization 4D+ <code>[N, C, ...]</code> Varies by norm type <p>Dimension Requirements</p> <p>cuDNN does not support arbitrary tensor dimensions. Each operation interprets tensors in a specific way. A 2D matrix <code>[M, N]</code> must be reshaped to <code>[1, M, N]</code> for matmul.</p>"},{"location":"concepts/tensors/#tensor-properties-in-cudnn","title":"Tensor Properties in cuDNN","text":"<p>Every cuDNN tensor has these key properties:</p>"},{"location":"concepts/tensors/#dimensions-shape","title":"Dimensions (Shape)","text":"<p>The size of each dimension:</p> <pre><code># Image tensor: batch=8, channels=64, height=56, width=56\ndims = [8, 64, 56, 56]\n</code></pre>"},{"location":"concepts/tensors/#strides","title":"Strides","text":"<p>How many elements to skip to move one position in each dimension:</p> <pre><code># For contiguous NCHW tensor [8, 64, 56, 56]:\n# To move +1 in batch (N): skip 64*56*56 = 200704 elements\n# To move +1 in channel (C): skip 56*56 = 3136 elements\n# To move +1 in height (H): skip 56 elements\n# To move +1 in width (W): skip 1 element\nstrides = [200704, 3136, 56, 1]  # NCHW layout\n</code></pre>"},{"location":"concepts/tensors/#data-type","title":"Data Type","text":"<p>The numeric precision:</p> <pre><code>cudnn.data_type.HALF      # 16-bit floating point\ncudnn.data_type.FLOAT     # 32-bit floating point\ncudnn.data_type.BFLOAT16  # Brain floating point 16\ncudnn.data_type.FP8_E4M3  # 8-bit (Hopper GPUs)\ncudnn.data_type.INT8      # 8-bit integer\n</code></pre>"},{"location":"concepts/tensors/#cudnn-tensors-are-descriptors-not-memory","title":"cuDNN Tensors Are Descriptors, Not Memory","text":"<p>Key Difference from PyTorch</p> <p>A cuDNN tensor is just a descriptor - it describes the shape, strides, and data type of a tensor, but does not allocate any memory.</p> <p>A PyTorch tensor allocates and owns the underlying memory buffer.</p> <pre><code># PyTorch: Allocates memory\nx_torch = torch.randn(8, 64, 56, 56, device=\"cuda\")  # Memory allocated!\n\n# cuDNN: Just a descriptor - NO memory allocated\nx_cudnn = graph.tensor(\n    dim=[8, 64, 56, 56],\n    stride=[200704, 1, 3584, 64],\n    data_type=cudnn.data_type.HALF,\n)\n# x_cudnn only describes layout - you must provide memory later\n</code></pre> <p>Why this matters:</p> <ol> <li>You allocate memory yourself (typically via PyTorch)</li> <li>You pass memory pointers to cuDNN at execution time</li> <li>cuDNN reads from / writes to your memory</li> </ol> <pre><code># Workflow:\n# 1. Create descriptor\nx = graph.tensor(dim=[8, 64], stride=[64, 1], data_type=cudnn.data_type.HALF)\n\n# 2. Build graph\ny = graph.matmul(x, w)\ny.set_output(True)\ngraph.build()\n\n# 3. Allocate REAL memory (PyTorch)\nx_data = torch.randn(8, 64, device=\"cuda\", dtype=torch.float16)\ny_data = torch.empty(8, 32, device=\"cuda\", dtype=torch.float16)\n\n# 4. Execute with memory pointers\ngraph.execute({x: x_data, y: y_data}, workspace, handle=handle)\n</code></pre> <p>This separation allows cuDNN to:</p> <ul> <li>Plan execution before knowing actual data</li> <li>Reuse the same graph with different memory buffers</li> <li>Optimize memory layout without copying data</li> </ul>"},{"location":"concepts/tensors/#memory-layouts-the-critical-concept","title":"Memory Layouts: The Critical Concept","text":"<p>This is Important!</p> <p>Memory layout is the #1 source of errors for cuDNN beginners. Understanding it is essential.</p>"},{"location":"concepts/tensors/#logical-vs-physical-layout","title":"Logical vs Physical Layout","text":"<p>A tensor has two layouts that can be different:</p> <ul> <li>Logical layout: How you index the data (e.g., <code>tensor[batch, channel, height, width]</code>)</li> <li>Physical layout: How bytes are actually arranged in GPU memory</li> </ul> <p>Strides determine the physical layout. Two tensors with the same logical shape can have completely different memory layouts:</p> <pre><code># Same logical shape [2, 3, 4, 4], different physical layouts\n\n# NCHW layout - channels are contiguous\nnchw_strides = [48, 16, 4, 1]  # stride[C]=16 means channels grouped together\n\n# NHWC layout - pixels are contiguous\nnhwc_strides = [48, 1, 12, 3]  # stride[C]=1 means channels interleaved\n</code></pre> <pre><code>graph TD\n    subgraph \"Logical View (always NCHW indexing)\"\n        L1[\"tensor[0, 0, 0, 0]\"]\n        L2[\"tensor[0, 1, 0, 0]\"]\n        L3[\"tensor[0, 2, 0, 0]\"]\n    end\n\n    subgraph \"Physical Memory (NHWC strides)\"\n        P1[C0] --&gt; P2[C1] --&gt; P3[C2]\n        P3 --&gt; P4[next pixel...]\n    end</code></pre> <p>Key Insight</p> <p>The logical dimension order (NCHW) stays the same for indexing. The strides control how that maps to physical memory. This is why you can have a \"channels-last\" tensor that you still index as <code>[N, C, H, W]</code>.</p>"},{"location":"concepts/tensors/#nchw-vs-nhwc","title":"NCHW vs NHWC","text":"<p>For 4D image tensors:</p> Layout Order Memory Pattern GPU Performance NCHW Batch, Channel, Height, Width All red pixels, then all green, then blue Slower NHWC Batch, Height, Width, Channel RGB together for each pixel Faster <pre><code>NCHW (channels first):\n[R00, R01, R10, R11, G00, G01, G10, G11, B00, B01, B10, B11]\n\nNHWC (channels last):\n[R00, G00, B00, R01, G01, B01, R10, G10, B10, R11, G11, B11]\n</code></pre> <p>Use Channels-Last</p> <p>Modern NVIDIA GPUs are optimized for NHWC (channels-last) layout. Always use it for convolutions!</p>"},{"location":"concepts/tensors/#creating-channels-last-tensors-in-pytorch","title":"Creating Channels-Last Tensors in PyTorch","text":"<pre><code># Method 1: Create and convert\nx = torch.randn(8, 64, 56, 56, device=\"cuda\", dtype=torch.float16)\nx = x.to(memory_format=torch.channels_last)\n\n# Method 2: Create with permute\nx = torch.randn(8, 56, 56, 64, device=\"cuda\", dtype=torch.float16)  # NHWC shape\nx = x.permute(0, 3, 1, 2)  # Logical NCHW, physical NHWC\n\n# Check layout\nprint(x.is_contiguous())  # False (not NCHW contiguous)\nprint(x.is_contiguous(memory_format=torch.channels_last))  # True\n</code></pre>"},{"location":"concepts/tensors/#creating-tensors-in-cudnn","title":"Creating Tensors in cuDNN","text":""},{"location":"concepts/tensors/#from-pytorch-tensors","title":"From PyTorch Tensors","text":"<p>The simplest way - cuDNN infers properties from PyTorch:</p> <pre><code>x_torch = torch.randn(8, 64, 56, 56, device=\"cuda\", dtype=torch.float16)\n\nwith cudnn.Graph() as graph:\n    # cuDNN extracts shape, stride, dtype from x_torch\n    y = graph.conv_fprop(image=x_torch, weight=w_torch)\n</code></pre>"},{"location":"concepts/tensors/#explicit-tensor-creation","title":"Explicit Tensor Creation","text":"<p>For more control:</p> <pre><code>graph = cudnn.pygraph()\n\n# Create tensor with explicit properties\nx = graph.tensor(\n    name=\"X\",\n    dim=[8, 64, 56, 56],\n    stride=[56*56*64, 1, 56*64, 64],  # NHWC strides\n    data_type=cudnn.data_type.HALF,\n)\n</code></pre>"},{"location":"concepts/tensors/#using-tensor_like","title":"Using tensor_like","text":"<p>Create a cuDNN tensor matching a PyTorch tensor:</p> <pre><code>x_torch = torch.randn(8, 64, 56, 56, device=\"cuda\")\n\ngraph = cudnn.pygraph()\nx = graph.tensor_like(x_torch)  # Copies all properties\n</code></pre>"},{"location":"concepts/tensors/#special-tensor-properties","title":"Special Tensor Properties","text":""},{"location":"concepts/tensors/#virtual-tensors","title":"Virtual Tensors","text":"<p>Intermediate results that don't need memory:</p> <pre><code>with cudnn.Graph() as graph:\n    # conv_out is virtual - no memory allocated\n    conv_out = graph.conv_fprop(x, w)\n\n    # relu_out is real - memory will be allocated\n    relu_out = graph.relu(conv_out)\n    relu_out.set_output(True)  # Makes it real\n</code></pre>"},{"location":"concepts/tensors/#pass-by-value-tensors","title":"Pass-by-Value Tensors","text":"<p>For scalar values (like scaling factors):</p> <pre><code># Scalar passed directly\nscale = graph.tensor(\n    dim=[1],\n    stride=[1],\n    is_pass_by_value=True,  # Value passed directly, not via pointer\n)\n</code></pre>"},{"location":"concepts/tensors/#ragged-tensors","title":"Ragged Tensors","text":"<p>For variable-length sequences:</p> <pre><code># Tensor with ragged dimension\n# Useful for batched sequences of different lengths\nragged_dim = graph.tensor(\n    dim=[batch_size, max_seq_len],\n    stride=[max_seq_len, 1],\n    ragged_offset=offsets_tensor,  # Where each sequence ends\n)\n</code></pre>"},{"location":"concepts/tensors/#tensor-operations","title":"Tensor Operations","text":""},{"location":"concepts/tensors/#set_output","title":"set_output","text":"<p>Mark tensor as graph output:</p> <pre><code>y = graph.conv_fprop(x, w)\ny.set_output(True)  # Required to retrieve this tensor\n</code></pre>"},{"location":"concepts/tensors/#set_name","title":"set_name","text":"<p>Give tensor a name for reference:</p> <pre><code>y.set_name(\"conv_output\")\n\n# Later reference as \"conv_output\" or via node::name\n</code></pre>"},{"location":"concepts/tensors/#set_data_type","title":"set_data_type","text":"<p>Override data type:</p> <pre><code>y.set_data_type(cudnn.data_type.FLOAT)  # Force FP32 output\n</code></pre>"},{"location":"concepts/tensors/#set_dim-set_stride","title":"set_dim / set_stride","text":"<p>Manually specify shape and layout:</p> <pre><code>y.set_dim([8, 32, 56, 56])\ny.set_stride([32*56*56, 1, 32*56, 32])  # NHWC\n</code></pre>"},{"location":"concepts/tensors/#computing-strides","title":"Computing Strides","text":"<p>Here's how to compute strides for common layouts:</p>"},{"location":"concepts/tensors/#nchw-channels-first","title":"NCHW (Channels First)","text":"<pre><code>def nchw_strides(N, C, H, W):\n    return [C * H * W, H * W, W, 1]\n\n# Example: [8, 64, 56, 56]\n# Strides: [200704, 3136, 56, 1]\n</code></pre>"},{"location":"concepts/tensors/#nhwc-channels-last","title":"NHWC (Channels Last)","text":"<pre><code>def nhwc_strides(N, C, H, W):\n    return [H * W * C, 1, W * C, C]\n\n# Example: [8, 64, 56, 56]\n# Strides: [200704, 1, 3584, 64]\n</code></pre>"},{"location":"concepts/tensors/#pytorch-helper","title":"PyTorch Helper","text":"<pre><code>import torch\n\n# Create tensor and check strides\nx = torch.randn(8, 64, 56, 56).to(memory_format=torch.channels_last)\nprint(f\"Strides: {x.stride()}\")  # (200704, 1, 3584, 64)\n</code></pre>"},{"location":"concepts/tensors/#common-mistakes","title":"Common Mistakes","text":"<p>Stride Mismatch</p> <p>The strides in your cuDNN descriptor must match your actual tensor's memory layout:</p> <pre><code># PyTorch tensor is channels-last (NHWC in memory)\nx_torch = torch.randn(8, 64, 56, 56, device=\"cuda\").to(\n    memory_format=torch.channels_last\n)\nprint(x_torch.stride())  # (200704, 1, 3584, 64) - NHWC!\n\n# WRONG: cuDNN descriptor says NCHW but data is NHWC\nx = graph.tensor(\n    dim=[8, 64, 56, 56],\n    stride=[200704, 3136, 56, 1],  # NCHW strides - MISMATCH!\n)\n\n# CORRECT: Match the actual memory layout\nx = graph.tensor(\n    dim=[8, 64, 56, 56],\n    stride=[200704, 1, 3584, 64],  # Matches x_torch.stride()\n)\n</code></pre> <p>Note: NCHW layout is valid and works correctly - just slower on modern GPUs. The mistake is when strides don't match your actual data.</p> <p>Forgetting set_output</p> <pre><code># WRONG: Can't retrieve y\ny = graph.conv_fprop(x, w)\n# y is virtual, no memory!\n\n# CORRECT\ny = graph.conv_fprop(x, w)\ny.set_output(True)  # Now y is real\n</code></pre>"},{"location":"concepts/tensors/#data-type-considerations","title":"Data Type Considerations","text":""},{"location":"concepts/tensors/#precision-vs-performance","title":"Precision vs Performance","text":"Type Bits Precision Speed Use Case FP32 32 High Baseline Training accuracy-critical FP16 16 Medium 2x faster Most training/inference BF16 16 Medium 2x faster Transformers FP8 8 Low 4x faster Inference (Hopper+)"},{"location":"concepts/tensors/#mixed-precision","title":"Mixed Precision","text":"<p>Use lower precision I/O with higher precision compute:</p> <pre><code>with cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,       # Fast FP16 I/O\n    compute_data_type=cudnn.data_type.FLOAT,  # Accurate FP32 math\n) as graph:\n    y = graph.conv_fprop(x, w)\n</code></pre>"},{"location":"concepts/tensors/#next-steps","title":"Next Steps","text":"<p>Now let's explore the operations you can perform on tensors.</p> <p>Operations Guide </p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing cuDNN Frontend, make sure you have:</p> Requirement Minimum Version Recommended NVIDIA GPU Volta (V100) Ampere (A100) or newer NVIDIA Driver 450.0+ Latest stable CUDA Toolkit 11.0+ 12.x Python 3.8+ 3.10+ <p>Check Your GPU</p> <p>Run <code>nvidia-smi</code> to verify your GPU and driver: <pre><code>nvidia-smi\n</code></pre> Look for the GPU name and driver version in the output.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#option-1-pip-install-recommended-for-python","title":"Option 1: pip Install (Recommended for Python)","text":"<p>The easiest way to get started:</p> <pre><code>pip install nvidia-cudnn-frontend\n</code></pre> <p>This single command installs:</p> <ul> <li>cuDNN Frontend Python bindings</li> <li>Required cuDNN backend library</li> <li>All Python dependencies</li> </ul> <p>That's it!</p> <p>You can now start using cuDNN Frontend in Python.</p> <p>Verify the installation:</p> <pre><code>import cudnn\nprint(f\"cuDNN Backend Version: {cudnn.backend_version()}\")\n</code></pre>"},{"location":"getting-started/installation/#option-2-c-header-only-library","title":"Option 2: C++ Header-Only Library","text":"<p>cuDNN Frontend for C++ is header-only - no compilation needed!</p> <p>Step 1: Clone the repository</p> <pre><code>git clone https://github.com/NVIDIA/cudnn-frontend.git\ncd cudnn-frontend\n</code></pre> <p>Step 2: Include in your project</p> <p>Add the include directory to your compiler flags:</p> <pre><code>-I/path/to/cudnn-frontend/include\n</code></pre> <p>Step 3: Include the header</p> <pre><code>#include &lt;cudnn_frontend.h&gt;\n</code></pre>"},{"location":"getting-started/installation/#option-3-build-from-source","title":"Option 3: Build from Source","text":"<p>For development or customization:</p> <p>Step 1: Clone and enter the repository</p> <pre><code>git clone https://github.com/NVIDIA/cudnn-frontend.git\ncd cudnn-frontend\n</code></pre> <p>Step 2: Install Python build dependencies</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Step 3: Build and install</p> <pre><code>pip install -v .\n</code></pre> <p>Environment Variables</p> <p>You can customize paths with environment variables:</p> <ul> <li><code>CUDAToolkit_ROOT</code>: Path to CUDA installation</li> <li><code>CUDNN_PATH</code>: Path to cuDNN installation</li> </ul>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#setting-up-cuda-environment","title":"Setting Up CUDA Environment","text":"<p>Make sure CUDA is in your path:</p> Linux/macOSWindows <pre><code>export PATH=/usr/local/cuda/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n</code></pre> <pre><code>$env:PATH = \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.0\\bin;$env:PATH\"\n</code></pre>"},{"location":"getting-started/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<p>We recommend using a virtual environment. UV is the fastest option:</p> uv (Recommended)venvconda <pre><code># Install uv if you haven't\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create and activate environment\nuv venv cudnn-env\nsource cudnn-env/bin/activate  # Linux/macOS\n\n# Install packages (10-100x faster than pip)\nuv pip install nvidia-cudnn-frontend torch\n</code></pre> <pre><code>python -m venv cudnn-env\nsource cudnn-env/bin/activate  # Linux/macOS\ncudnn-env\\Scripts\\activate     # Windows\n\npip install nvidia-cudnn-frontend\n</code></pre> <pre><code>conda create -n cudnn-env python=3.10\nconda activate cudnn-env\n\npip install nvidia-cudnn-frontend\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Let's make sure everything is working. Create a file called <code>test_cudnn.py</code>:</p> <pre><code>import cudnn\nimport torch\n\n# Check versions\nprint(f\"cuDNN Backend Version: {cudnn.backend_version()}\")\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n    # Create a simple test\n    handle = cudnn.create_handle()\n\n    # Test tensor\n    x = torch.randn(4, 64, 32, 32, device=\"cuda\", dtype=torch.float16)\n    w = torch.randn(128, 64, 3, 3, device=\"cuda\", dtype=torch.float16)\n\n    # Build a simple convolution graph\n    with cudnn.Graph(\n        io_data_type=cudnn.data_type.HALF,\n        compute_data_type=cudnn.data_type.FLOAT,\n    ) as graph:\n        y = graph.conv_fprop(\n            image=x, weight=w,\n            padding=[1, 1], stride=[1, 1], dilation=[1, 1]\n        )\n        y.set_output(True)\n\n    # Execute\n    result = graph(x, w, handle=handle)\n\n    print(f\"Output shape: {result.shape}\")\n    print(\"SUCCESS: cuDNN Frontend is working!\")\n\n    cudnn.destroy_handle(handle)\n</code></pre> <p>Run it:</p> <pre><code>python test_cudnn.py\n</code></pre> <p>Expected output:</p> <pre><code>cuDNN Backend Version: 90300\nPyTorch Version: 2.x.x\nCUDA Available: True\nGPU: NVIDIA GeForce RTX ...\nOutput shape: torch.Size([4, 128, 32, 32])\nSUCCESS: cuDNN Frontend is working!\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"CUDA not found <p>Error: <code>CUDA not available</code> or <code>No CUDA GPUs are available</code></p> <p>Solution:</p> <ol> <li>Verify GPU with <code>nvidia-smi</code></li> <li>Check CUDA installation: <code>nvcc --version</code></li> <li>Ensure PyTorch CUDA version matches your system:    <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu126\n</code></pre></li> </ol> cuDNN version mismatch <p>Error: <code>cuDNN version X.Y.Z expected, found A.B.C</code></p> <p>Solution:</p> <ol> <li>Install matching cuDNN version:    <pre><code>pip install nvidia-cudnn-cu12==9.0.0\n</code></pre></li> <li>Or update cudnn-frontend:    <pre><code>pip install --upgrade nvidia-cudnn-frontend\n</code></pre></li> </ol> Import errors <p>Error: <code>ModuleNotFoundError: No module named 'cudnn'</code></p> <p>Solution:</p> <ol> <li>Verify installation:    <pre><code>pip list | grep cudnn\n</code></pre></li> <li>Reinstall if needed:    <pre><code>pip uninstall nvidia-cudnn-frontend\npip install nvidia-cudnn-frontend\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you're still having issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Search the NVIDIA Developer Forums</li> <li>Open a new issue with:<ul> <li>Your GPU model</li> <li>CUDA version</li> <li>Python version</li> <li>Full error message</li> </ul> </li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Your environment is ready! Let's build your first attention graph.</p> <p>Quick Start (SDPA) </p>"},{"location":"getting-started/introduction/","title":"Introduction to cuDNN Frontend","text":""},{"location":"getting-started/introduction/#the-story-of-deep-learning-acceleration","title":"The Story of Deep Learning Acceleration","text":"<p>Let's start with a question: Why do we need cuDNN Frontend?</p> <p>When you train a neural network, you're essentially doing millions of mathematical operations - matrix multiplications, convolutions, normalizations, and more. Your CPU could do these, but it would be painfully slow.</p> <pre><code>graph TD\n    subgraph \"CPU Processing\"\n        A1[Operation 1] --&gt; A2[Operation 2] --&gt; A3[Operation 3] --&gt; A4[Operation 4]\n    end\n\n    subgraph \"GPU Processing\"\n        B1[Op 1]\n        B2[Op 2]\n        B3[Op 3]\n        B4[Op 4]\n    end\n\n    style A1 fill:#ffcdd2\n    style A2 fill:#ffcdd2\n    style A3 fill:#ffcdd2\n    style A4 fill:#ffcdd2\n    style B1 fill:#c8e6c9\n    style B2 fill:#c8e6c9\n    style B3 fill:#c8e6c9\n    style B4 fill:#c8e6c9</code></pre> <p>CPUs process operations one at a time. GPUs process thousands in parallel. This is why modern AI runs on GPUs.</p>"},{"location":"getting-started/introduction/#what-is-cudnn","title":"What is cuDNN?","text":"<p>cuDNN (CUDA Deep Neural Network library) is NVIDIA's secret weapon for deep learning. It's a GPU-accelerated library of primitives for:</p> <ul> <li>Convolutions</li> <li>Matrix multiplications</li> <li>Activation functions</li> <li>Normalization</li> <li>Attention mechanisms</li> <li>And much more...</li> </ul> <p>cuDNN Powers Everything</p> <p>When you run PyTorch, TensorFlow, or any major deep learning framework on NVIDIA GPUs, cuDNN is doing the heavy lifting under the hood.</p>"},{"location":"getting-started/introduction/#the-problem-cudnn-is-complex","title":"The Problem: cuDNN is Complex","text":"<p>Directly using cuDNN requires you to:</p> <ol> <li>Create and configure tensor descriptors</li> <li>Create operation descriptors</li> <li>Query the runtime for supported configurations</li> <li>Allocate workspace memory</li> <li>Execute operations</li> <li>Handle errors at every step</li> </ol> <p>Here's what that might look like (simplified):</p> <pre><code>// Don't worry if this looks intimidating - that's the point!\ncudnnTensorDescriptor_t xDesc, yDesc;\ncudnnFilterDescriptor_t wDesc;\ncudnnConvolutionDescriptor_t convDesc;\n\ncudnnCreateTensorDescriptor(&amp;xDesc);\ncudnnSetTensor4dDescriptor(xDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, n, c, h, w);\n// ... many more lines of setup code ...\ncudnnConvolutionForward(handle, &amp;alpha, xDesc, x, wDesc, w, convDesc,\n                        algo, workspace, workspaceSize, &amp;beta, yDesc, y);\n</code></pre> <p>That's a lot of code just for one convolution! And we haven't even handled errors yet.</p>"},{"location":"getting-started/introduction/#the-solution-cudnn-frontend","title":"The Solution: cuDNN Frontend","text":"<p>cuDNN Frontend wraps all that complexity in an elegant, graph-based API:</p> <pre><code># The same convolution, using cuDNN Frontend\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(image=x, weight=w, padding=[1, 1])\n    y.set_output(True)\n\nresult = graph(x, w)  # Done!\n</code></pre> <p>Three lines. That's it.</p>"},{"location":"getting-started/introduction/#the-graph-concept","title":"The Graph Concept","text":"<p>The key innovation in cuDNN Frontend is the Graph API. Instead of thinking about individual operations, you describe your computation as a graph:</p> <pre><code>graph LR\n    subgraph \"Input Tensors\"\n        X[Image X]\n        W[Weight W]\n    end\n\n    subgraph \"Operations\"\n        Conv[Convolution]\n    end\n\n    subgraph \"Output\"\n        Y[Result Y]\n    end\n\n    X --&gt; Conv\n    W --&gt; Conv\n    Conv --&gt; Y\n\n    style X fill:#bbdefb\n    style W fill:#bbdefb\n    style Conv fill:#fff9c4\n    style Y fill:#c8e6c9</code></pre> <p>This approach has several advantages:</p> Advantage Description Optimization cuDNN can analyze your entire graph and find the best execution strategy Fusion Multiple operations can be combined into one efficient kernel Reusability Build a graph once, execute it millions of times Clarity Your code clearly describes what you want, not how to do it"},{"location":"getting-started/introduction/#real-world-impact","title":"Real-World Impact","text":"<p>Let's look at a more realistic example - the attention mechanism used in transformers:</p> Traditional ApproachcuDNN Frontend <pre><code># Implementing attention manually is complex\nscores = torch.matmul(query, key.transpose(-2, -1))\nscores = scores / math.sqrt(d_k)\nscores = torch.softmax(scores, dim=-1)\nscores = torch.dropout(scores, p=0.1)\noutput = torch.matmul(scores, value)\n# Multiple operations, multiple memory transfers\n</code></pre> <pre><code># cuDNN Frontend fuses everything into one optimized operation\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa(q=query, k=key, v=value,\n                           attn_scale=1/math.sqrt(d_k))\n    output.set_output(True)\n\nresult = graph(query, key, value)\n# One operation, maximum performance\n</code></pre> <p>The cuDNN Frontend version is not just cleaner - it's significantly faster because:</p> <ol> <li>All operations are fused into a single GPU kernel</li> <li>Intermediate results stay in fast GPU registers</li> <li>Memory bandwidth is minimized</li> </ol>"},{"location":"getting-started/introduction/#what-youll-learn","title":"What You'll Learn","text":"<p>In this guide, you'll learn:</p> <ol> <li>Installation - How to set up cuDNN Frontend</li> <li>Core Concepts - Understanding graphs, tensors, and operations</li> <li>Practical Usage - Building real operations for your models</li> <li>Best Practices - Tips for maximum performance</li> </ol>"},{"location":"getting-started/introduction/#next-steps","title":"Next Steps","text":"<p>Ready to see cuDNN Frontend in action? Let's build your first attention graph!</p> <p>Quick Start (SDPA) </p>"},{"location":"getting-started/llm-quickstart/","title":"LLM Quick Start","text":"<p>Most developers today are working with Large Language Models (LLMs). This quick start shows you how to use cuDNN Frontend for the key operations in LLMs - specifically Scaled Dot-Product Attention (SDPA).</p> <p>Why Start Here?</p> <p>If you're building or optimizing LLMs like LLaMA, Mistral, GPT, etc., this is the most relevant starting point. cuDNN's SDPA implementation can give you 2-3x speedup over naive implementations.</p>"},{"location":"getting-started/llm-quickstart/#what-youll-build","title":"What You'll Build","text":"<p>We'll implement the self-attention mechanism - the core of every transformer:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\\]"},{"location":"getting-started/llm-quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, make sure you have:</p> <ul> <li>NVIDIA GPU (Ampere A100 or newer recommended)</li> <li>Python 3.8+</li> <li>PyTorch with CUDA support</li> <li>cuDNN Frontend installed (<code>pip install nvidia-cudnn-frontend</code>)</li> </ul>"},{"location":"getting-started/llm-quickstart/#step-by-step-implementation","title":"Step-by-Step Implementation","text":""},{"location":"getting-started/llm-quickstart/#step-1-setup","title":"Step 1: Setup","text":"<pre><code>import cudnn\nimport torch\nimport math\n\n# Verify GPU and cuDNN\nassert torch.cuda.is_available(), \"Need CUDA GPU!\"\nassert torch.cuda.get_device_capability()[0] &gt;= 8, \"Need Ampere (SM80) or newer\"\n\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"cuDNN version: {cudnn.backend_version()}\")\n\n# Create handle\nhandle = cudnn.create_handle()\n</code></pre>"},{"location":"getting-started/llm-quickstart/#step-2-define-llm-dimensions","title":"Step 2: Define LLM Dimensions","text":"<pre><code># Typical LLM dimensions (similar to LLaMA 7B)\nbatch_size = 4\nnum_heads = 32\nseq_len = 2048\nhead_dim = 128\n\n# Scaling factor\nattn_scale = 1.0 / math.sqrt(head_dim)\n\nprint(f\"Configuration:\")\nprint(f\"  Batch size: {batch_size}\")\nprint(f\"  Sequence length: {seq_len}\")\nprint(f\"  Number of heads: {num_heads}\")\nprint(f\"  Head dimension: {head_dim}\")\n</code></pre>"},{"location":"getting-started/llm-quickstart/#step-3-create-q-k-v-tensors","title":"Step 3: Create Q, K, V Tensors","text":"<pre><code># In real LLMs, these come from linear projections of the input\n# Shape: [batch, num_heads, seq_len, head_dim]\n\n# Use bfloat16 - the standard for modern LLMs\ndtype = torch.bfloat16\n\nQ = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=\"cuda\", dtype=dtype)\nK = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=\"cuda\", dtype=dtype)\nV = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=\"cuda\", dtype=dtype)\n\nprint(f\"Q shape: {Q.shape}\")  # [4, 32, 2048, 128]\n</code></pre>"},{"location":"getting-started/llm-quickstart/#step-4-build-the-attention-graph","title":"Step 4: Build the Attention Graph","text":"<pre><code>with cudnn.Graph(\n    io_data_type=cudnn.data_type.BFLOAT16,       # Input/output in BF16\n    intermediate_data_type=cudnn.data_type.FLOAT, # Softmax in FP32 for stability\n    compute_data_type=cudnn.data_type.FLOAT,      # Accumulation in FP32\n) as graph:\n\n    # Scaled Dot-Product Attention\n    output, stats = graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=attn_scale,\n        is_inference=True,       # Set to False for training\n        use_causal_mask=True,    # Autoregressive (decoder) attention\n    )\n\n    # Mark output - IMPORTANT: must set dimensions and strides\n    output.set_output(True)\n    output.set_dim(Q.shape)\n    output.set_stride(Q.stride())\n\nprint(\"Graph built successfully!\")\n</code></pre>"},{"location":"getting-started/llm-quickstart/#step-5-execute","title":"Step 5: Execute","text":"<pre><code># Run the attention\nresult = graph(Q, K, V, handle=handle)\n\nprint(f\"Output shape: {result.shape}\")  # [4, 32, 2048, 128]\nprint(f\"Output dtype: {result.dtype}\")  # torch.bfloat16\n</code></pre>"},{"location":"getting-started/llm-quickstart/#step-6-verify-correctness","title":"Step 6: Verify Correctness","text":"<pre><code># Compare with PyTorch's native SDPA\nreference = torch.nn.functional.scaled_dot_product_attention(\n    Q, K, V,\n    scale=attn_scale,\n    is_causal=True,\n)\n\n# Check results match\ntorch.testing.assert_close(result, reference, atol=1e-2, rtol=1e-2)\nprint(\"Results match PyTorch!\")\n</code></pre>"},{"location":"getting-started/llm-quickstart/#step-7-cleanup","title":"Step 7: Cleanup","text":"<pre><code>cudnn.destroy_handle(handle)\n</code></pre>"},{"location":"getting-started/llm-quickstart/#complete-code","title":"Complete Code","text":"<pre><code>import cudnn\nimport torch\nimport math\n\n# Setup\nassert torch.cuda.is_available()\nhandle = cudnn.create_handle()\n\n# LLM dimensions\nbatch_size, num_heads, seq_len, head_dim = 4, 32, 2048, 128\nattn_scale = 1.0 / math.sqrt(head_dim)\n\n# Create tensors\nQ = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=\"cuda\", dtype=torch.bfloat16)\nK = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=\"cuda\", dtype=torch.bfloat16)\nV = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=\"cuda\", dtype=torch.bfloat16)\n\n# Build attention graph\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.BFLOAT16,\n    intermediate_data_type=cudnn.data_type.FLOAT,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    output, _ = graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=attn_scale,\n        is_inference=True,\n        use_causal_mask=True,\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n\n# Execute\nresult = graph(Q, K, V, handle=handle)\n\n# Verify\nreference = torch.nn.functional.scaled_dot_product_attention(\n    Q, K, V, scale=attn_scale, is_causal=True\n)\ntorch.testing.assert_close(result, reference, atol=1e-2, rtol=1e-2)\nprint(f\"Success! Attention output shape: {result.shape}\")\n\n# Cleanup\ncudnn.destroy_handle(handle)\n</code></pre>"},{"location":"getting-started/llm-quickstart/#why-cudnn-sdpa-is-special","title":"Why cuDNN SDPA is Special","text":"<p>cuDNN implements FlashAttention algorithm internally:</p> Feature Standard Attention cuDNN SDPA Memory O(N\u00b2) O(N) Materializes attention matrix Yes No Max sequence length ~4K 100K+ Performance Baseline 2-3x faster"},{"location":"getting-started/llm-quickstart/#grouped-query-attention-gqa","title":"Grouped Query Attention (GQA)","text":"<p>Modern LLMs like LLaMA 2+ use GQA where K and V have fewer heads:</p> <pre><code># GQA: 32 Q heads, 8 KV heads\nnum_q_heads = 32\nnum_kv_heads = 8\n\nQ = torch.randn(batch, num_q_heads, seq_len, head_dim, ...)\nK = torch.randn(batch, num_kv_heads, seq_len, head_dim, ...)\nV = torch.randn(batch, num_kv_heads, seq_len, head_dim, ...)\n\n# cuDNN handles GQA automatically!\nwith cudnn.Graph(...) as graph:\n    output, _ = graph.sdpa(q=Q, k=K, v=V, ...)\n</code></pre>"},{"location":"getting-started/llm-quickstart/#common-issues","title":"Common Issues","text":"<p>Wrong tensor dimensions</p> <p>Make sure Q, K, V are <code>[batch, heads, seq_len, head_dim]</code> not <code>[batch, seq_len, heads, head_dim]</code>. Use <code>.transpose(1, 2)</code> if needed.</p> <p>Missing set_dim/set_stride on output</p> <p>Always call both <code>set_dim()</code> and <code>set_stride()</code> on SDPA output tensors.</p> <p>SM70 or older GPU</p> <p>SDPA requires Ampere (SM80) or newer. RTX 30xx, A100, H100, RTX 40xx all work.</p>"},{"location":"getting-started/llm-quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you understand attention, explore:</p> <ul> <li>Full Transformer Layer - Build complete LLM layers</li> <li>Training with Backward Pass - Add gradient computation</li> <li>Memory Optimization - Maximize throughput</li> </ul> <p>Explore Tutorials </p>"},{"location":"tutorials/attention/","title":"Attention (SDPA) Tutorial","text":"<p>Scaled Dot-Product Attention (SDPA) is the heart of transformer models. cuDNN Frontend provides highly optimized implementations based on FlashAttention.</p>"},{"location":"tutorials/attention/#what-is-sdpa","title":"What is SDPA?","text":"<p>SDPA computes attention in transformers:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\\] <pre><code>graph LR\n    subgraph \"Inputs\"\n        Q[Query&lt;br&gt;B\u00d7H\u00d7N\u00d7D]\n        K[Key&lt;br&gt;B\u00d7H\u00d7N\u00d7D]\n        V[Value&lt;br&gt;B\u00d7H\u00d7N\u00d7D]\n    end\n\n    subgraph \"Attention\"\n        MM1[Q @ K.T]\n        Scale[\u00f7 \u221ad]\n        Soft[Softmax]\n        MM2[@ V]\n    end\n\n    subgraph \"Output\"\n        O[Output&lt;br&gt;B\u00d7H\u00d7N\u00d7D]\n    end\n\n    Q --&gt; MM1\n    K --&gt; MM1\n    MM1 --&gt; Scale --&gt; Soft --&gt; MM2\n    V --&gt; MM2\n    MM2 --&gt; O</code></pre>"},{"location":"tutorials/attention/#why-cudnn-sdpa","title":"Why cuDNN SDPA?","text":"<p>Standard attention has O(N\u00b2) memory complexity. cuDNN's implementation uses FlashAttention algorithm:</p> Aspect Standard Attention cuDNN SDPA (FlashAttention) Memory O(N\u00b2) O(N) Speed Limited by memory Compute-bound Sequence length ~4K max 128K+ possible"},{"location":"tutorials/attention/#basic-sdpa","title":"Basic SDPA","text":"<pre><code>import cudnn\nimport torch\nimport math\n\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\")\nhandle = cudnn.create_handle()\n\n# Typical transformer dimensions\nbatch_size = 8\nnum_heads = 12\nseq_len = 1024\nhead_dim = 64\n\n# Create Q, K, V tensors\n# Shape: [batch, num_heads, seq_len, head_dim]\nQ = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=device, dtype=torch.float16)\nK = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=device, dtype=torch.float16)\nV = torch.randn(batch_size, num_heads, seq_len, head_dim,\n                device=device, dtype=torch.float16)\n\n# Scaling factor\nattn_scale = 1.0 / math.sqrt(head_dim)\n\n# Build SDPA graph\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    intermediate_data_type=cudnn.data_type.FLOAT,  # Softmax in FP32\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    output, stats = graph.sdpa(\n        q=Q,\n        k=K,\n        v=V,\n        attn_scale=attn_scale,\n        is_inference=True,\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n\n# Execute\nresult = graph(Q, K, V, handle=handle)\nprint(f\"Output shape: {result.shape}\")  # [8, 12, 1024, 64]\n\n# Verify with PyTorch\nreference = torch.nn.functional.scaled_dot_product_attention(\n    Q, K, V, scale=attn_scale\n)\ntorch.testing.assert_close(result, reference, atol=5e-3, rtol=3e-3)\n</code></pre>"},{"location":"tutorials/attention/#causal-attention-autoregressive","title":"Causal Attention (Autoregressive)","text":"<p>For language models, use causal masking:</p> <pre><code>with cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    intermediate_data_type=cudnn.data_type.FLOAT,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    output, stats = graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=attn_scale,\n        is_inference=True,\n        use_causal_mask=True,  # Enable causal masking!\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n\nresult = graph(Q, K, V, handle=handle)\n\n# Verify with PyTorch causal attention\nreference = torch.nn.functional.scaled_dot_product_attention(\n    Q, K, V, scale=attn_scale, is_causal=True\n)\ntorch.testing.assert_close(result, reference, atol=5e-3, rtol=3e-3)\n</code></pre>"},{"location":"tutorials/attention/#training-with-backward-pass","title":"Training with Backward Pass","text":"<p>For training, you need stats from forward and backward gradients:</p> <pre><code># Forward pass (training mode)\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    intermediate_data_type=cudnn.data_type.FLOAT,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as fwd_graph:\n    output, stats = fwd_graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=attn_scale,\n        is_inference=False,      # Training mode\n        generate_stats=True,     # Need stats for backward\n        use_causal_mask=True,\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n    stats.set_output(True)  # Save stats for backward\n\n# Execute forward\nO, S = fwd_graph(Q, K, V, handle=handle)\n\n# Gradient from downstream\ndO = torch.randn_like(O)\n\n# Backward pass\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    intermediate_data_type=cudnn.data_type.FLOAT,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as bwd_graph:\n    dQ, dK, dV = bwd_graph.sdpa_backward(\n        q=Q, k=K, v=V, o=O,\n        dO=dO,\n        stats=S,\n        attn_scale=attn_scale,\n        use_causal_mask=True,\n    )\n    dQ.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n    dK.set_output(True).set_dim(K.shape).set_stride(K.stride())\n    dV.set_output(True).set_dim(V.shape).set_stride(V.stride())\n\n# Execute backward\ndQ_out, dK_out, dV_out = bwd_graph(Q, K, V, O, dO, S, handle=handle)\n</code></pre>"},{"location":"tutorials/attention/#different-tensor-layouts","title":"Different Tensor Layouts","text":"<p>cuDNN supports different memory layouts:</p>"},{"location":"tutorials/attention/#bhsd-batch-head-seq-dim-default","title":"BHSD (Batch, Head, Seq, Dim) - Default","text":"<pre><code># Standard layout\nQ = torch.randn(batch, num_heads, seq_len, head_dim, device=device)\n# Strides: (num_heads*seq_len*head_dim, seq_len*head_dim, head_dim, 1)\n</code></pre>"},{"location":"tutorials/attention/#bshd-batch-seq-head-dim-interleaved","title":"BSHD (Batch, Seq, Head, Dim) - Interleaved","text":"<p>More efficient for some operations:</p> <pre><code># Create in BSHD then view as BHSD\nQ = torch.randn(batch, seq_len, num_heads, head_dim, device=device)\nQ = Q.transpose(1, 2)  # Logical BHSD, physical BSHD\n# This is the recommended layout!\n</code></pre>"},{"location":"tutorials/attention/#grouped-query-attention-gqa","title":"Grouped Query Attention (GQA)","text":"<p>Used in LLaMA 2+, Mistral, etc.:</p> <pre><code># GQA: fewer K, V heads than Q heads\nnum_q_heads = 32\nnum_kv_heads = 8  # K, V have fewer heads\nhead_dim = 128\n\nQ = torch.randn(batch, num_q_heads, seq_len, head_dim, device=device, dtype=torch.float16)\nK = torch.randn(batch, num_kv_heads, seq_len, head_dim, device=device, dtype=torch.float16)\nV = torch.randn(batch, num_kv_heads, seq_len, head_dim, device=device, dtype=torch.float16)\n\n# cuDNN handles GQA automatically when head counts differ\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=1.0/math.sqrt(head_dim),\n        is_inference=True,\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n</code></pre>"},{"location":"tutorials/attention/#multi-query-attention-mqa","title":"Multi-Query Attention (MQA)","text":"<p>Extreme case: single K, V head:</p> <pre><code># MQA: 1 K, V head shared across all Q heads\nQ = torch.randn(batch, 32, seq_len, head_dim, device=device, dtype=torch.float16)\nK = torch.randn(batch, 1, seq_len, head_dim, device=device, dtype=torch.float16)\nV = torch.randn(batch, 1, seq_len, head_dim, device=device, dtype=torch.float16)\n\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa(q=Q, k=K, v=V, attn_scale=attn_scale)\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n</code></pre>"},{"location":"tutorials/attention/#paged-kv-cache-inference","title":"Paged KV Cache (Inference)","text":"<p>For efficient LLM serving with PagedAttention:</p> <pre><code># For inference with non-contiguous KV cache\n# See: samples/python/52_sdpa_with_paged_caches.ipynb\n\n# Page table maps virtual blocks to physical blocks\npage_size = 256\nnum_pages = 64\n\n# K cache stored in pages\nK_cache = torch.randn(num_pages, page_size, num_heads, head_dim,\n                       device=device, dtype=torch.float16)\n\n# Page table: [batch, num_blocks_per_seq]\npage_table = torch.randint(0, num_pages, (batch, seq_len // page_size),\n                            device=device, dtype=torch.int32)\n\n# cuDNN handles paged attention\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa_with_paged_cache(\n        q=Q, k=K_cache, v=V_cache,\n        page_table=page_table,\n        # ... other params\n    )\n</code></pre>"},{"location":"tutorials/attention/#dropout-in-attention","title":"Dropout in Attention","text":"<p>For training regularization:</p> <pre><code># With dropout\ndropout_prob = 0.1\nseed = 12345\n\nwith cudnn.Graph() as graph:\n    output, stats = graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=attn_scale,\n        is_inference=False,\n        dropout=dropout_prob,\n        dropout_seed=seed,\n        dropout_offset=0,\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n</code></pre>"},{"location":"tutorials/attention/#attention-with-bias","title":"Attention with Bias","text":"<p>For relative position encoding:</p> <pre><code># Attention bias: [batch, num_heads, seq_len, seq_len]\n# Or: [1, 1, seq_len, seq_len] for shared bias\nattn_bias = torch.randn(1, 1, seq_len, seq_len, device=device, dtype=torch.float16)\n\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa(\n        q=Q, k=K, v=V,\n        attn_scale=attn_scale,\n        attn_bias=attn_bias,  # Added to Q@K before softmax\n        is_inference=True,\n    )\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\n</code></pre>"},{"location":"tutorials/attention/#alibi-attention-with-linear-biases","title":"ALiBi (Attention with Linear Biases)","text":"<p>Popular in some models:</p> <pre><code># Create ALiBi slopes\ndef get_alibi_slopes(num_heads):\n    closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))\n    base = 2 ** (-(2 ** -(math.log2(closest_power_of_2) - 3)))\n    powers = torch.arange(1, closest_power_of_2 + 1)\n    slopes = base ** powers\n    if closest_power_of_2 != num_heads:\n        extra_powers = torch.arange(1, 2 * (num_heads - closest_power_of_2) + 1, 2)\n        extra_slopes = (base ** 0.5) ** extra_powers\n        slopes = torch.cat([slopes, extra_slopes])\n    return slopes\n\nslopes = get_alibi_slopes(num_heads).to(device)\n\n# Create position-based bias\npositions = torch.arange(seq_len, device=device)\nalibi = slopes[:, None, None] * (positions[None, :, None] - positions[None, None, :])\n# alibi shape: [num_heads, seq_len, seq_len]\n</code></pre>"},{"location":"tutorials/attention/#performance-tips","title":"Performance Tips","text":"<p>SDPA Optimization</p> <ol> <li>Use BSHD layout: Most efficient for tensor cores</li> <li>Align sequence lengths: Multiples of 64 or 128</li> <li>Prefer FP16/BF16: Significant speedup</li> <li>Use causal mask: Built-in optimization vs explicit mask</li> <li>Batch queries: Larger batches = better efficiency</li> </ol>"},{"location":"tutorials/attention/#comparison-with-pytorch","title":"Comparison with PyTorch","text":"<pre><code># cuDNN SDPA\nwith cudnn.Graph() as graph:\n    output, _ = graph.sdpa(q=Q, k=K, v=V, attn_scale=attn_scale, is_inference=True)\n    output.set_output(True).set_dim(Q.shape).set_stride(Q.stride())\ncudnn_out = graph(Q, K, V, handle=handle)\n\n# PyTorch SDPA (also uses FlashAttention when available)\ntorch_out = torch.nn.functional.scaled_dot_product_attention(\n    Q, K, V, scale=attn_scale\n)\n\n# Verify\ntorch.testing.assert_close(cudnn_out, torch_out, atol=5e-3, rtol=3e-3)\n</code></pre>"},{"location":"tutorials/attention/#complete-attention-layer-example","title":"Complete Attention Layer Example","text":"<pre><code>class CudnnAttention:\n    def __init__(self, hidden_dim, num_heads, handle):\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        self.head_dim = hidden_dim // num_heads\n        self.handle = handle\n        self.attn_scale = 1.0 / math.sqrt(self.head_dim)\n\n        # Projection weights (would normally be nn.Parameter)\n        self.W_qkv = torch.randn(hidden_dim, 3 * hidden_dim,\n                                  device=\"cuda\", dtype=torch.float16)\n        self.W_out = torch.randn(hidden_dim, hidden_dim,\n                                  device=\"cuda\", dtype=torch.float16)\n\n    def forward(self, x):\n        batch, seq_len, _ = x.shape\n\n        # QKV projection (could also use cuDNN matmul)\n        qkv = x @ self.W_qkv\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for attention\n        q = q.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # cuDNN SDPA\n        with cudnn.Graph() as graph:\n            out, _ = graph.sdpa(\n                q=q, k=k, v=v,\n                attn_scale=self.attn_scale,\n                is_inference=True,\n                use_causal_mask=True,\n            )\n            out.set_output(True).set_dim(q.shape).set_stride(q.stride())\n\n        attn_out = graph(q, k, v, handle=self.handle)\n\n        # Reshape and output projection\n        attn_out = attn_out.transpose(1, 2).contiguous().view(batch, seq_len, -1)\n        return attn_out @ self.W_out\n</code></pre>"},{"location":"tutorials/attention/#cleanup","title":"Cleanup","text":"<pre><code>cudnn.destroy_handle(handle)\n</code></pre>"},{"location":"tutorials/attention/#next-steps","title":"Next Steps","text":"<p>Learn about normalization operations.</p> <p>Normalization Tutorial </p>"},{"location":"tutorials/convolution/","title":"Convolution Operations Tutorial","text":"<p>Convolutions are the backbone of computer vision deep learning. This tutorial covers everything you need to know about using convolutions with cuDNN Frontend.</p>"},{"location":"tutorials/convolution/#what-is-a-convolution","title":"What is a Convolution?","text":"<p>A convolution slides a small filter (kernel) over an input image to produce feature maps:</p> <pre><code>graph LR\n    subgraph \"Input Image\"\n        I[\"[N, C_in, H, W]&lt;br&gt;Feature Map\"]\n    end\n\n    subgraph \"Convolution\"\n        K[\"[C_out, C_in, K_h, K_w]&lt;br&gt;Kernel/Filter\"]\n        OP((\u229b))\n    end\n\n    subgraph \"Output\"\n        O[\"[N, C_out, H', W']&lt;br&gt;Feature Map\"]\n    end\n\n    I --&gt; OP\n    K --&gt; OP\n    OP --&gt; O</code></pre>"},{"location":"tutorials/convolution/#forward-convolution-training-inference","title":"Forward Convolution (Training &amp; Inference)","text":""},{"location":"tutorials/convolution/#basic-forward-pass","title":"Basic Forward Pass","text":"<pre><code>import cudnn\nimport torch\n\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\")\nhandle = cudnn.create_handle()\n\n# Input: [batch=16, channels=64, height=56, width=56]\nx = torch.randn(16, 64, 56, 56, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\n# Kernel: [out_channels=128, in_channels=64, kernel_h=3, kernel_w=3]\nw = torch.randn(128, 64, 3, 3, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\n# Build convolution graph\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    y = graph.conv_fprop(\n        image=x,\n        weight=w,\n        padding=[1, 1],    # Same padding for 3x3 kernel\n        stride=[1, 1],\n        dilation=[1, 1],\n    )\n    y.set_output(True)\n\n# Execute\noutput = graph(x, w, handle=handle)\nprint(f\"Output shape: {output.shape}\")  # [16, 128, 56, 56]\n</code></pre>"},{"location":"tutorials/convolution/#understanding-convolution-parameters","title":"Understanding Convolution Parameters","text":""},{"location":"tutorials/convolution/#padding","title":"Padding","text":"<p>Controls output size and handles border pixels:</p> <pre><code># No padding: output shrinks\ny = graph.conv_fprop(x, w, padding=[0, 0])\n# Input [N, C, 56, 56] \u2192 Output [N, C', 54, 54]\n\n# Same padding: output same size (for stride=1)\ny = graph.conv_fprop(x, w, padding=[1, 1])  # For 3x3 kernel\n# Input [N, C, 56, 56] \u2192 Output [N, C', 56, 56]\n\n# Full padding: maximum output\ny = graph.conv_fprop(x, w, padding=[2, 2])  # For 3x3 kernel\n# Input [N, C, 56, 56] \u2192 Output [N, C', 58, 58]\n</code></pre> <p>Padding formula for \"same\" output: <code>padding = (kernel_size - 1) // 2</code></p>"},{"location":"tutorials/convolution/#stride","title":"Stride","text":"<p>Controls step size of the convolution:</p> <pre><code># Stride 1: dense computation\ny = graph.conv_fprop(x, w, padding=[1, 1], stride=[1, 1])\n# Input [N, C, 56, 56] \u2192 Output [N, C', 56, 56]\n\n# Stride 2: downsamples by 2x\ny = graph.conv_fprop(x, w, padding=[1, 1], stride=[2, 2])\n# Input [N, C, 56, 56] \u2192 Output [N, C', 28, 28]\n</code></pre> <p>Output size formula: <code>output_size = (input_size + 2*padding - kernel_size) // stride + 1</code></p>"},{"location":"tutorials/convolution/#dilation","title":"Dilation","text":"<p>Controls spacing between kernel elements:</p> <pre><code># Standard convolution\ny = graph.conv_fprop(x, w, padding=[1, 1], dilation=[1, 1])\n# 3x3 kernel covers 3x3 area\n\n# Dilated convolution (atrous)\ny = graph.conv_fprop(x, w, padding=[2, 2], dilation=[2, 2])\n# 3x3 kernel covers 5x5 area with gaps\n</code></pre>"},{"location":"tutorials/convolution/#convolution-with-bias-and-activation","title":"Convolution with Bias and Activation","text":"<p>The most common pattern is <code>Conv \u2192 Bias \u2192 Activation</code>:</p> <pre><code># Additional tensors\nbias = torch.randn(1, 128, 1, 1, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    # Convolution\n    conv_out = graph.conv_fprop(x, w, padding=[1, 1])\n\n    # Add bias\n    bias_out = graph.bias(input=conv_out, bias=bias)\n\n    # ReLU activation\n    relu_out = graph.relu(input=bias_out)\n    relu_out.set_output(True)\n\n# All operations are fused into a single kernel!\noutput = graph(x, w, bias, handle=handle)\n</code></pre>"},{"location":"tutorials/convolution/#backward-pass-training","title":"Backward Pass (Training)","text":"<p>For training, you need gradients for both inputs and weights.</p>"},{"location":"tutorials/convolution/#data-gradient-dgrad","title":"Data Gradient (dgrad)","text":"<p>Computes gradient with respect to input:</p> <pre><code># Forward pass first\nx = torch.randn(16, 64, 56, 56, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\nw = torch.randn(128, 64, 3, 3, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\n# Gradient flowing back from next layer\ndy = torch.randn(16, 128, 56, 56, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\n# Compute input gradient\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    dx = graph.conv_dgrad(\n        weight=w,\n        loss=dy,\n        padding=[1, 1],\n        stride=[1, 1],\n        dilation=[1, 1],\n    )\n    dx.set_output(True)\n\n# Execute\ninput_grad = graph(w, dy, handle=handle)\nprint(f\"Input gradient shape: {input_grad.shape}\")  # [16, 64, 56, 56]\n</code></pre>"},{"location":"tutorials/convolution/#weight-gradient-wgrad","title":"Weight Gradient (wgrad)","text":"<p>Computes gradient with respect to weights:</p> <pre><code>with cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    dw = graph.conv_wgrad(\n        image=x,\n        loss=dy,\n        padding=[1, 1],\n        stride=[1, 1],\n        dilation=[1, 1],\n    )\n    dw.set_output(True)\n\n# Execute\nweight_grad = graph(x, dy, handle=handle)\nprint(f\"Weight gradient shape: {weight_grad.shape}\")  # [128, 64, 3, 3]\n</code></pre>"},{"location":"tutorials/convolution/#combined-forward-and-backward","title":"Combined Forward and Backward","text":"<p>For training loops, you typically need all three:</p> <pre><code># Forward graph\nwith cudnn.Graph(name=\"forward\") as forward_graph:\n    y = graph.conv_fprop(x, w, padding=[1, 1])\n    y.set_output(True)\n\n# Backward input gradient graph\nwith cudnn.Graph(name=\"dgrad\") as dgrad_graph:\n    dx = graph.conv_dgrad(w, dy, padding=[1, 1])\n    dx.set_output(True)\n\n# Backward weight gradient graph\nwith cudnn.Graph(name=\"wgrad\") as wgrad_graph:\n    dw = graph.conv_wgrad(x, dy, padding=[1, 1])\n    dw.set_output(True)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward\n        y = forward_graph(batch.x, w, handle=handle)\n\n        # Compute loss and get dy\n        loss = criterion(y, batch.target)\n        dy = loss.backward()  # Or manual gradient\n\n        # Backward\n        dx = dgrad_graph(w, dy, handle=handle)\n        dw = wgrad_graph(batch.x, dy, handle=handle)\n\n        # Update weights\n        w -= learning_rate * dw\n</code></pre>"},{"location":"tutorials/convolution/#different-kernel-sizes","title":"Different Kernel Sizes","text":""},{"location":"tutorials/convolution/#1x1-convolution-pointwise","title":"1x1 Convolution (Pointwise)","text":"<p>Used for channel projection without spatial mixing:</p> <pre><code># 1x1 kernel for channel reduction\nw_1x1 = torch.randn(64, 256, 1, 1, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w_1x1, padding=[0, 0])  # No padding needed\n    y.set_output(True)\n# Reduces 256 channels to 64\n</code></pre>"},{"location":"tutorials/convolution/#3x3-convolution","title":"3x3 Convolution","text":"<p>The workhorse of CNNs:</p> <pre><code>w_3x3 = torch.randn(128, 64, 3, 3, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w_3x3, padding=[1, 1])  # Same padding\n    y.set_output(True)\n</code></pre>"},{"location":"tutorials/convolution/#5x5-7x7-convolutions","title":"5x5, 7x7 Convolutions","text":"<p>Larger receptive fields:</p> <pre><code># 7x7 convolution (common in first layer)\nw_7x7 = torch.randn(64, 3, 7, 7, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w_7x7, padding=[3, 3], stride=[2, 2])\n    y.set_output(True)\n</code></pre>"},{"location":"tutorials/convolution/#depthwise-convolution","title":"Depthwise Convolution","text":"<p>Used in MobileNets and EfficientNets:</p> <pre><code># Each channel processed independently\n# groups = in_channels = out_channels\nin_channels = 64\n\n# Depthwise: [out, in_per_group, kH, kW] = [64, 1, 3, 3]\nw_dw = torch.randn(64, 1, 3, 3, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(\n        x, w_dw,\n        padding=[1, 1],\n        groups=in_channels,  # Key parameter for depthwise\n    )\n    y.set_output(True)\n</code></pre>"},{"location":"tutorials/convolution/#transposed-convolution","title":"Transposed Convolution","text":"<p>For upsampling (used in autoencoders, GANs):</p> <pre><code># Transposed convolution increases spatial dimensions\n# Input: [N, 128, 14, 14] \u2192 Output: [N, 64, 28, 28]\n\nx_small = torch.randn(16, 128, 14, 14, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\nw_up = torch.randn(128, 64, 4, 4, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\n# Note: Use dgrad for transposed convolution\nwith cudnn.Graph() as graph:\n    y = graph.conv_dgrad(\n        weight=w_up,\n        loss=x_small,  # The \"input\" goes where loss would be\n        padding=[1, 1],\n        stride=[2, 2],\n    )\n    y.set_output(True)\n\noutput = graph(w_up, x_small, handle=handle)\nprint(f\"Upsampled shape: {output.shape}\")  # [16, 64, 28, 28]\n</code></pre>"},{"location":"tutorials/convolution/#resnet-block-example","title":"ResNet Block Example","text":"<p>Complete example of a residual block:</p> <pre><code>def resnet_basic_block(graph, x, w1, w2, bn1_params, bn2_params, downsample_w=None):\n    \"\"\"\n    Basic ResNet block:\n    x \u2192 Conv \u2192 BN \u2192 ReLU \u2192 Conv \u2192 BN \u2192 (+x) \u2192 ReLU\n    \"\"\"\n    identity = x\n\n    # First conv + bn + relu\n    out = graph.conv_fprop(x, w1, padding=[1, 1])\n    out = graph.batchnorm_inference(\n        out, bn1_params.scale, bn1_params.bias,\n        bn1_params.mean, bn1_params.inv_var\n    )\n    out = graph.relu(out)\n\n    # Second conv + bn\n    out = graph.conv_fprop(out, w2, padding=[1, 1])\n    out = graph.batchnorm_inference(\n        out, bn2_params.scale, bn2_params.bias,\n        bn2_params.mean, bn2_params.inv_var\n    )\n\n    # Downsample if needed\n    if downsample_w is not None:\n        identity = graph.conv_fprop(identity, downsample_w, stride=[2, 2])\n\n    # Residual connection\n    out = graph.add(out, identity)\n    out = graph.relu(out)\n\n    return out\n</code></pre>"},{"location":"tutorials/convolution/#performance-tips","title":"Performance Tips","text":"<p>Convolution Optimization</p> <ol> <li>Use channels-last: Always use NHWC layout</li> <li>Fuse operations: Combine Conv + Bias + Activation</li> <li>Power-of-2 channels: 64, 128, 256 are fastest</li> <li>Avoid small batches: Batch size &gt; 32 recommended</li> <li>Consider Winograd: cuDNN auto-selects for 3x3</li> </ol>"},{"location":"tutorials/convolution/#verification-with-pytorch","title":"Verification with PyTorch","text":"<p>Always verify your implementation:</p> <pre><code># cuDNN Frontend\nwith cudnn.Graph() as graph:\n    y = graph.conv_fprop(x, w, padding=[1, 1])\n    y.set_output(True)\ncudnn_result = graph(x, w, handle=handle)\n\n# PyTorch reference\npytorch_result = torch.nn.functional.conv2d(x, w, padding=1)\n\n# Compare\ntorch.testing.assert_close(cudnn_result, pytorch_result, atol=5e-3, rtol=3e-3)\nprint(\"Results match!\")\n</code></pre>"},{"location":"tutorials/convolution/#cleanup","title":"Cleanup","text":"<pre><code>cudnn.destroy_handle(handle)\n</code></pre>"},{"location":"tutorials/convolution/#next-steps","title":"Next Steps","text":"<p>Learn about matrix multiplication operations.</p> <p>Matrix Multiplication Tutorial </p>"},{"location":"tutorials/custom-graphs/","title":"Building Custom Graphs Tutorial","text":"<p>Now that you understand individual operations, let's combine them into custom computation graphs. This is where cuDNN Frontend's power really shines.</p>"},{"location":"tutorials/custom-graphs/#graph-building-philosophy","title":"Graph Building Philosophy","text":"<p>The key insight: Describe what you want, not how to do it.</p> <pre><code>graph TD\n    subgraph \"Your Intent\"\n        I1[Conv \u2192 BN \u2192 ReLU \u2192 Pool]\n    end\n\n    subgraph \"cuDNN Frontend\"\n        F1[Validate]\n        F2[Optimize]\n        F3[Fuse Operations]\n        F4[Select Algorithms]\n    end\n\n    subgraph \"GPU Execution\"\n        G1[Optimized Kernel]\n    end\n\n    I1 --&gt; F1 --&gt; F2 --&gt; F3 --&gt; F4 --&gt; G1</code></pre>"},{"location":"tutorials/custom-graphs/#building-blocks","title":"Building Blocks","text":""},{"location":"tutorials/custom-graphs/#sequential-operations","title":"Sequential Operations","text":"<pre><code>import cudnn\nimport torch\nimport math\n\nhandle = cudnn.create_handle()\ndevice = torch.device(\"cuda\")\n\n# Build a sequential CNN block\nx = torch.randn(16, 64, 56, 56, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\nw1 = torch.randn(128, 64, 3, 3, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\nw2 = torch.randn(128, 128, 3, 3, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    # First conv + relu\n    h1 = graph.conv_fprop(x, w1, padding=[1, 1])\n    h1 = graph.relu(h1)\n\n    # Second conv + relu\n    h2 = graph.conv_fprop(h1, w2, padding=[1, 1])\n    h2 = graph.relu(h2)\n\n    h2.set_output(True)\n\nresult = graph(x, w1, w2, handle=handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#residual-connections","title":"Residual Connections","text":"<pre><code>shortcut_w = torch.randn(128, 64, 1, 1, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    # Main path\n    h = graph.conv_fprop(x, w1, padding=[1, 1])\n    h = graph.relu(h)\n    h = graph.conv_fprop(h, w2, padding=[1, 1])\n\n    # Shortcut path (1x1 conv for channel matching)\n    shortcut = graph.conv_fprop(x, shortcut_w)\n\n    # Residual addition\n    out = graph.add(h, shortcut)\n    out = graph.relu(out)\n    out.set_output(True)\n\nresult = graph(x, w1, w2, shortcut_w, handle=handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#parallel-paths-inception-style","title":"Parallel Paths (Inception-style)","text":"<pre><code>w_1x1 = torch.randn(32, 64, 1, 1, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\nw_3x3 = torch.randn(64, 64, 3, 3, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\nw_5x5 = torch.randn(32, 64, 5, 5, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\nwith cudnn.Graph() as graph:\n    # Path 1: 1x1 conv\n    path1 = graph.conv_fprop(x, w_1x1)\n    path1 = graph.relu(path1)\n\n    # Path 2: 3x3 conv\n    path2 = graph.conv_fprop(x, w_3x3, padding=[1, 1])\n    path2 = graph.relu(path2)\n\n    # Path 3: 5x5 conv\n    path3 = graph.conv_fprop(x, w_5x5, padding=[2, 2])\n    path3 = graph.relu(path3)\n\n    # Concatenate along channel dimension\n    # Note: concat might need manual implementation\n    # For now, we output all three\n    path1.set_output(True).set_name(\"path1\")\n    path2.set_output(True).set_name(\"path2\")\n    path3.set_output(True).set_name(\"path3\")\n</code></pre>"},{"location":"tutorials/custom-graphs/#complete-network-blocks","title":"Complete Network Blocks","text":""},{"location":"tutorials/custom-graphs/#mlp-block","title":"MLP Block","text":"<pre><code>hidden_dim = 768\nff_dim = 3072\n\nx_mlp = torch.randn(8, 1024, hidden_dim, device=device, dtype=torch.float16)\nW1 = torch.randn(hidden_dim, ff_dim, device=device, dtype=torch.float16)\nW2 = torch.randn(ff_dim, hidden_dim, device=device, dtype=torch.float16)\nb1 = torch.randn(ff_dim, device=device, dtype=torch.float16)\nb2 = torch.randn(hidden_dim, device=device, dtype=torch.float16)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    # First linear + GELU\n    h = graph.matmul(x_mlp, W1)\n    h = graph.add(h, b1.view(1, 1, -1))\n    h = graph.gelu(h)\n\n    # Second linear\n    out = graph.matmul(h, W2)\n    out = graph.add(out, b2.view(1, 1, -1))\n    out.set_output(True)\n\nresult = graph(x_mlp, W1, W2, b1.view(1, 1, -1), b2.view(1, 1, -1), handle=handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#complete-transformer-block","title":"Complete Transformer Block","text":"<pre><code>class CudnnTransformerBlock:\n    def __init__(self, hidden_dim, num_heads, ff_dim, handle):\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        self.head_dim = hidden_dim // num_heads\n        self.ff_dim = ff_dim\n        self.handle = handle\n        self.attn_scale = 1.0 / math.sqrt(self.head_dim)\n\n        # Initialize weights (normally nn.Parameter)\n        self.ln1_gamma = torch.ones(hidden_dim, device=\"cuda\", dtype=torch.float32)\n        self.ln1_beta = torch.zeros(hidden_dim, device=\"cuda\", dtype=torch.float32)\n        self.ln2_gamma = torch.ones(hidden_dim, device=\"cuda\", dtype=torch.float32)\n        self.ln2_beta = torch.zeros(hidden_dim, device=\"cuda\", dtype=torch.float32)\n\n        self.W_qkv = torch.randn(hidden_dim, 3 * hidden_dim, device=\"cuda\", dtype=torch.float16)\n        self.W_out = torch.randn(hidden_dim, hidden_dim, device=\"cuda\", dtype=torch.float16)\n        self.W_ff1 = torch.randn(hidden_dim, ff_dim, device=\"cuda\", dtype=torch.float16)\n        self.W_ff2 = torch.randn(ff_dim, hidden_dim, device=\"cuda\", dtype=torch.float16)\n\n    def forward(self, x):\n        batch, seq_len, _ = x.shape\n\n        # === Self-Attention Sub-layer ===\n        residual = x\n\n        # Pre-LayerNorm\n        with cudnn.Graph(io_data_type=cudnn.data_type.HALF,\n                         compute_data_type=cudnn.data_type.FLOAT) as ln1_graph:\n            x_norm, _, _ = ln1_graph.layernorm(x, self.ln1_gamma, self.ln1_beta, 1e-5)\n            x_norm.set_output(True)\n        x_norm = ln1_graph(x, self.ln1_gamma, self.ln1_beta, handle=self.handle)\n\n        # QKV projection\n        qkv = x_norm @ self.W_qkv\n        q, k, v = qkv.chunk(3, dim=-1)\n        q = q.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # SDPA\n        with cudnn.Graph(io_data_type=cudnn.data_type.HALF,\n                         intermediate_data_type=cudnn.data_type.FLOAT,\n                         compute_data_type=cudnn.data_type.FLOAT) as attn_graph:\n            attn_out, _ = attn_graph.sdpa(q, k, v, attn_scale=self.attn_scale,\n                                           is_inference=True, use_causal_mask=True)\n            attn_out.set_output(True).set_dim(q.shape).set_stride(q.stride())\n\n        attn_out = attn_graph(q, k, v, handle=self.handle)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(batch, seq_len, -1)\n\n        # Output projection and residual\n        attn_out = attn_out @ self.W_out\n        x = residual + attn_out\n\n        # === FFN Sub-layer ===\n        residual = x\n\n        # Pre-LayerNorm\n        with cudnn.Graph(io_data_type=cudnn.data_type.HALF,\n                         compute_data_type=cudnn.data_type.FLOAT) as ln2_graph:\n            x_norm, _, _ = ln2_graph.layernorm(x, self.ln2_gamma, self.ln2_beta, 1e-5)\n            x_norm.set_output(True)\n        x_norm = ln2_graph(x, self.ln2_gamma, self.ln2_beta, handle=self.handle)\n\n        # FFN\n        with cudnn.Graph(io_data_type=cudnn.data_type.HALF,\n                         compute_data_type=cudnn.data_type.FLOAT) as ffn_graph:\n            h = ffn_graph.matmul(x_norm, self.W_ff1)\n            h = ffn_graph.gelu(h)\n            ff_out = ffn_graph.matmul(h, self.W_ff2)\n            ff_out.set_output(True)\n        ff_out = ffn_graph(x_norm, self.W_ff1, self.W_ff2, handle=self.handle)\n\n        # Residual\n        x = residual + ff_out\n\n        return x\n</code></pre>"},{"location":"tutorials/custom-graphs/#graph-caching-pattern","title":"Graph Caching Pattern","text":"<p>Build graphs once, execute many times:</p> <pre><code>class GraphCache:\n    def __init__(self):\n        self.cache = {}\n\n    def get_or_build(self, key, builder_fn):\n        if key not in self.cache:\n            self.cache[key] = builder_fn()\n        return self.cache[key]\n\n# Usage\ncache = GraphCache()\n\ndef build_conv_graph(in_ch, out_ch, kernel_size):\n    def builder():\n        x = torch.randn(1, in_ch, 56, 56, device=\"cuda\", dtype=torch.float16).to(\n            memory_format=torch.channels_last)\n        w = torch.randn(out_ch, in_ch, kernel_size, kernel_size,\n                        device=\"cuda\", dtype=torch.float16).to(memory_format=torch.channels_last)\n\n        with cudnn.Graph() as graph:\n            y = graph.conv_fprop(x, w, padding=[kernel_size//2, kernel_size//2])\n            y.set_output(True)\n        return graph\n    return builder\n\n# Get or build\nkey = (64, 128, 3)  # (in_ch, out_ch, kernel)\ngraph = cache.get_or_build(key, build_conv_graph(*key))\n\n# Execute (fast - graph already built)\nresult = graph(x_actual, w_actual, handle=handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#dynamic-shapes-with-graph-templates","title":"Dynamic Shapes with Graph Templates","text":"<p>For varying batch sizes:</p> <pre><code>def create_flexible_graph(max_batch_size, channels, height, width):\n    \"\"\"Create a graph that works for any batch size up to max_batch_size.\"\"\"\n    template_x = torch.randn(max_batch_size, channels, height, width,\n                              device=\"cuda\", dtype=torch.float16).to(\n                                  memory_format=torch.channels_last)\n    template_w = torch.randn(channels*2, channels, 3, 3,\n                              device=\"cuda\", dtype=torch.float16).to(\n                                  memory_format=torch.channels_last)\n\n    with cudnn.Graph() as graph:\n        y = graph.conv_fprop(template_x, template_w, padding=[1, 1])\n        y.set_output(True)\n\n    return graph\n\n# Create once\nmax_graph = create_flexible_graph(64, 64, 56, 56)\n\n# Execute with different batch sizes\nfor batch_size in [8, 16, 32, 64]:\n    x = torch.randn(batch_size, 64, 56, 56, device=\"cuda\", dtype=torch.float16).to(\n        memory_format=torch.channels_last)\n    w = torch.randn(128, 64, 3, 3, device=\"cuda\", dtype=torch.float16).to(\n        memory_format=torch.channels_last)\n    # This may need graph rebuild for different sizes\n    result = max_graph(x, w, handle=handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#error-handling","title":"Error Handling","text":"<pre><code>def safe_build_graph(x, w):\n    try:\n        with cudnn.Graph() as graph:\n            y = graph.conv_fprop(x, w, padding=[1, 1])\n            y.set_output(True)\n        return graph, None\n    except Exception as e:\n        return None, str(e)\n\n# Usage\ngraph, error = safe_build_graph(x, w)\nif error:\n    print(f\"Graph build failed: {error}\")\n    # Fall back to PyTorch\n    result = torch.nn.functional.conv2d(x, w, padding=1)\nelse:\n    result = graph(x, w, handle=handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#debugging-custom-graphs","title":"Debugging Custom Graphs","text":"<pre><code># Enable verbose logging\nimport os\nos.environ[\"CUDNN_FRONTEND_LOG_INFO\"] = \"1\"\nos.environ[\"CUDNN_FRONTEND_LOG_FILE\"] = \"stdout\"\n\n# Build and inspect\nwith cudnn.Graph() as graph:\n    h = graph.conv_fprop(x, w, padding=[1, 1], name=\"conv1\")\n    h = graph.relu(h, name=\"relu1\")\n    h = graph.conv_fprop(h, w2, padding=[1, 1], name=\"conv2\")\n    h.set_output(True).set_name(\"output\")\n\n# Print graph structure\nprint(graph)\n</code></pre>"},{"location":"tutorials/custom-graphs/#best-practices-summary","title":"Best Practices Summary","text":"<p>Custom Graph Best Practices</p> <ol> <li>Plan your graph topology before coding</li> <li>Name operations for debugging</li> <li>Cache graphs - don't rebuild unnecessarily</li> <li>Test incrementally - verify each subgraph</li> <li>Use virtual tensors - minimize memory</li> <li>Fuse aggressively - combine operations</li> <li>Profile first - identify bottlenecks before optimizing</li> </ol>"},{"location":"tutorials/custom-graphs/#complete-example-mini-resnet-block","title":"Complete Example: Mini ResNet Block","text":"<pre><code>def create_resnet_block_graph(in_channels, out_channels, stride, handle):\n    \"\"\"\n    Creates a complete ResNet basic block graph.\n\n    Structure:\n    x -&gt; Conv3x3 -&gt; BN -&gt; ReLU -&gt; Conv3x3 -&gt; BN -&gt; (+shortcut) -&gt; ReLU -&gt; y\n    \"\"\"\n    # Template tensors\n    x = torch.randn(16, in_channels, 56, 56, device=\"cuda\", dtype=torch.float16).to(\n        memory_format=torch.channels_last)\n    w1 = torch.randn(out_channels, in_channels, 3, 3, device=\"cuda\", dtype=torch.float16).to(\n        memory_format=torch.channels_last)\n    w2 = torch.randn(out_channels, out_channels, 3, 3, device=\"cuda\", dtype=torch.float16).to(\n        memory_format=torch.channels_last)\n\n    # BN parameters\n    gamma1 = torch.ones(out_channels, device=\"cuda\", dtype=torch.float32)\n    beta1 = torch.zeros(out_channels, device=\"cuda\", dtype=torch.float32)\n    gamma2 = torch.ones(out_channels, device=\"cuda\", dtype=torch.float32)\n    beta2 = torch.zeros(out_channels, device=\"cuda\", dtype=torch.float32)\n\n    downsample = in_channels != out_channels or stride != 1\n    if downsample:\n        w_ds = torch.randn(out_channels, in_channels, 1, 1, device=\"cuda\", dtype=torch.float16).to(\n            memory_format=torch.channels_last)\n\n    with cudnn.Graph(\n        io_data_type=cudnn.data_type.HALF,\n        compute_data_type=cudnn.data_type.FLOAT,\n    ) as graph:\n        # First conv block\n        h = graph.conv_fprop(x, w1, padding=[1, 1], stride=[stride, stride], name=\"conv1\")\n        h, _, _ = graph.layernorm(h, gamma1, beta1, 1e-5)  # Using layernorm for simplicity\n        h = graph.relu(h, name=\"relu1\")\n\n        # Second conv block\n        h = graph.conv_fprop(h, w2, padding=[1, 1], name=\"conv2\")\n        h, _, _ = graph.layernorm(h, gamma2, beta2, 1e-5)\n\n        # Shortcut\n        if downsample:\n            shortcut = graph.conv_fprop(x, w_ds, stride=[stride, stride], name=\"downsample\")\n        else:\n            shortcut = x\n\n        # Residual add + final ReLU\n        out = graph.add(h, shortcut, name=\"residual_add\")\n        out = graph.relu(out, name=\"relu2\")\n        out.set_output(True).set_name(\"output\")\n\n    return graph\n\n# Create and use\nblock_graph = create_resnet_block_graph(64, 128, 2, handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#cleanup","title":"Cleanup","text":"<pre><code>cudnn.destroy_handle(handle)\n</code></pre>"},{"location":"tutorials/custom-graphs/#next-steps","title":"Next Steps","text":"<p>Check out the API reference for complete documentation.</p> <p>Python API Reference </p>"},{"location":"tutorials/matmul/","title":"Matrix Multiplication Tutorial","text":"<p>Matrix multiplication (GEMM) is fundamental to neural networks. This tutorial covers everything from basic matmul to advanced fusions.</p>"},{"location":"tutorials/matmul/#the-basics","title":"The Basics","text":"<p>Matrix multiplication computes <code>C = A @ B</code>:</p> <pre><code>graph LR\n    subgraph \"Inputs\"\n        A[\"A&lt;br&gt;[M, K]\"]\n        B[\"B&lt;br&gt;[K, N]\"]\n    end\n\n    subgraph \"Operation\"\n        MM((MatMul))\n    end\n\n    subgraph \"Output\"\n        C[\"C&lt;br&gt;[M, N]\"]\n    end\n\n    A --&gt; MM\n    B --&gt; MM\n    MM --&gt; C</code></pre>"},{"location":"tutorials/matmul/#basic-matrix-multiplication","title":"Basic Matrix Multiplication","text":"<pre><code>import cudnn\nimport torch\n\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\")\nhandle = cudnn.create_handle()\n\n# Matrix A: [M=512, K=256]\nA = torch.randn(512, 256, device=device, dtype=torch.float16)\n\n# Matrix B: [K=256, N=1024]\nB = torch.randn(256, 1024, device=device, dtype=torch.float16)\n\n# Build matmul graph\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,  # FP32 accumulation\n) as graph:\n    C = graph.matmul(A=A, B=B)\n    C.set_output(True)\n\n# Execute\nresult = graph(A, B, handle=handle)\nprint(f\"Result shape: {result.shape}\")  # [512, 1024]\n\n# Verify\ntorch.testing.assert_close(result, A @ B, atol=1e-2, rtol=1e-2)\n</code></pre>"},{"location":"tutorials/matmul/#batched-matrix-multiplication","title":"Batched Matrix Multiplication","text":"<p>Process multiple matrices simultaneously:</p> <pre><code># Batched matmul: [Batch, M, K] @ [Batch, K, N] = [Batch, M, N]\nbatch_size = 32\nA_batched = torch.randn(batch_size, 512, 256, device=device, dtype=torch.float16)\nB_batched = torch.randn(batch_size, 256, 1024, device=device, dtype=torch.float16)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    C = graph.matmul(A=A_batched, B=B_batched)\n    C.set_output(True)\n\nresult = graph(A_batched, B_batched, handle=handle)\nprint(f\"Batched result shape: {result.shape}\")  # [32, 512, 1024]\n</code></pre>"},{"location":"tutorials/matmul/#matmul-with-bias-gemm-bias","title":"Matmul with Bias (GEMM + Bias)","text":"<p>Common pattern in linear layers:</p> <pre><code># Linear layer: y = x @ W + b\nx = torch.randn(32, 512, 256, device=device, dtype=torch.float16)  # [B, N, in_features]\nW = torch.randn(256, 1024, device=device, dtype=torch.float16)     # [in_features, out_features]\nbias = torch.randn(1024, device=device, dtype=torch.float16)       # [out_features]\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    # Matmul\n    mm_out = graph.matmul(A=x, B=W)\n\n    # Add bias (broadcast)\n    bias_reshaped = bias.unsqueeze(0).unsqueeze(0)  # [1, 1, out_features]\n    out = graph.add(a=mm_out, b=bias_reshaped)\n    out.set_output(True)\n\nresult = graph(x, W, bias_reshaped, handle=handle)\n</code></pre>"},{"location":"tutorials/matmul/#matmul-with-activation","title":"Matmul with Activation","text":"<p>Fuse activation for better performance:</p> <pre><code>with cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    mm_out = graph.matmul(A=x, B=W)\n    bias_out = graph.add(mm_out, bias_reshaped)\n\n    # Add activation - all fused!\n    gelu_out = graph.gelu(bias_out)  # Or: relu, silu, tanh\n    gelu_out.set_output(True)\n\nresult = graph(x, W, bias_reshaped, handle=handle)\n</code></pre>"},{"location":"tutorials/matmul/#transposed-matrix-multiplication","title":"Transposed Matrix Multiplication","text":"<p>For different contraction patterns:</p> <pre><code># C = A @ B.T\n# When B is stored as [N, K] but you need [K, N]\n\nA = torch.randn(512, 256, device=device, dtype=torch.float16)\nB_T = torch.randn(1024, 256, device=device, dtype=torch.float16)  # Stored transposed\n\nwith cudnn.Graph() as graph:\n    # Use B's transpose\n    C = graph.matmul(A=A, B=B_T.T)  # cuDNN handles the transpose\n    C.set_output(True)\n</code></pre>"},{"location":"tutorials/matmul/#fp8-matrix-multiplication","title":"FP8 Matrix Multiplication","text":"<p>For maximum performance on Hopper GPUs:</p> <pre><code># FP8 requires scaling factors\nA_fp8 = A.to(torch.float8_e4m3fn)\nB_fp8 = B.to(torch.float8_e4m3fn)\nscale_A = torch.tensor([1.0], device=device)\nscale_B = torch.tensor([1.0], device=device)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.FP8_E4M3,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    C = graph.matmul(\n        A=A_fp8, B=B_fp8,\n        scale_A=scale_A,\n        scale_B=scale_B,\n    )\n    C.set_output(True).set_data_type(cudnn.data_type.HALF)  # Output in FP16\n</code></pre>"},{"location":"tutorials/matmul/#multi-head-attention-projections","title":"Multi-Head Attention Projections","text":"<p>Efficient Q, K, V projections:</p> <pre><code># Input: [batch, seq_len, hidden_dim]\n# Projections: [hidden_dim, num_heads * head_dim]\n\nbatch, seq_len, hidden_dim = 8, 1024, 768\nnum_heads, head_dim = 12, 64\n\nx = torch.randn(batch, seq_len, hidden_dim, device=device, dtype=torch.float16)\nW_q = torch.randn(hidden_dim, num_heads * head_dim, device=device, dtype=torch.float16)\nW_k = torch.randn(hidden_dim, num_heads * head_dim, device=device, dtype=torch.float16)\nW_v = torch.randn(hidden_dim, num_heads * head_dim, device=device, dtype=torch.float16)\n\n# Option 1: Separate projections\nwith cudnn.Graph() as q_graph:\n    q = q_graph.matmul(x, W_q)\n    q.set_output(True)\n\nwith cudnn.Graph() as k_graph:\n    k = k_graph.matmul(x, W_k)\n    k.set_output(True)\n\nwith cudnn.Graph() as v_graph:\n    v = v_graph.matmul(x, W_v)\n    v.set_output(True)\n\n# Execute\nQ = q_graph(x, W_q, handle=handle)\nK = k_graph(x, W_k, handle=handle)\nV = v_graph(x, W_v, handle=handle)\n</code></pre>"},{"location":"tutorials/matmul/#fused-qkv-projection","title":"Fused QKV Projection","text":"<p>More efficient - single matmul for all projections:</p> <pre><code># Fused weights: [hidden_dim, 3 * num_heads * head_dim]\nW_qkv = torch.randn(hidden_dim, 3 * num_heads * head_dim,\n                     device=device, dtype=torch.float16)\n\nwith cudnn.Graph() as graph:\n    qkv = graph.matmul(x, W_qkv)\n    qkv.set_output(True)\n\n# Single matmul, then split\nqkv_out = graph(x, W_qkv, handle=handle)\nQ, K, V = qkv_out.chunk(3, dim=-1)\n\n# Reshape for attention: [batch, seq_len, num_heads, head_dim] \u2192 [batch, num_heads, seq_len, head_dim]\nQ = Q.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\nK = K.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\nV = V.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n</code></pre>"},{"location":"tutorials/matmul/#feed-forward-network-ffn","title":"Feed-Forward Network (FFN)","text":"<p>The two-layer MLP in transformers:</p> <pre><code># FFN: x \u2192 Linear1 \u2192 Activation \u2192 Linear2\n# Shapes: [B, N, D] \u2192 [B, N, 4D] \u2192 [B, N, D]\n\nhidden_dim = 768\nff_dim = 768 * 4  # Typical 4x expansion\n\nx = torch.randn(8, 1024, hidden_dim, device=device, dtype=torch.float16)\nW1 = torch.randn(hidden_dim, ff_dim, device=device, dtype=torch.float16)\nW2 = torch.randn(ff_dim, hidden_dim, device=device, dtype=torch.float16)\n\nwith cudnn.Graph() as graph:\n    # First linear + GELU\n    h = graph.matmul(x, W1)\n    h = graph.gelu(h)\n\n    # Second linear\n    out = graph.matmul(h, W2)\n    out.set_output(True)\n\nresult = graph(x, W1, W2, handle=handle)\n</code></pre>"},{"location":"tutorials/matmul/#swiglu-activation","title":"SwiGLU Activation","text":"<p>Popular in LLaMA and other modern architectures:</p> <pre><code># SwiGLU: (x @ W_gate * silu(x @ W_up)) @ W_down\n\nW_gate = torch.randn(hidden_dim, ff_dim, device=device, dtype=torch.float16)\nW_up = torch.randn(hidden_dim, ff_dim, device=device, dtype=torch.float16)\nW_down = torch.randn(ff_dim, hidden_dim, device=device, dtype=torch.float16)\n\nwith cudnn.Graph() as graph:\n    # Parallel projections\n    gate = graph.matmul(x, W_gate)\n    up = graph.matmul(x, W_up)\n\n    # SwiGLU\n    gate_activated = graph.silu(gate)\n    gated = graph.mul(up, gate_activated)\n\n    # Down projection\n    out = graph.matmul(gated, W_down)\n    out.set_output(True)\n\nresult = graph(x, W_gate, W_up, W_down, handle=handle)\n</code></pre>"},{"location":"tutorials/matmul/#mixed-precision-strategies","title":"Mixed Precision Strategies","text":"<pre><code># Strategy 1: FP16 everything with FP32 accumulation\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    C = graph.matmul(A, B)\n\n# Strategy 2: Mixed input types\nA_fp16 = A.half()\nB_bf16 = B.bfloat16()\n\nwith cudnn.Graph(\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    C = graph.matmul(A=A_fp16, B=B_bf16)  # Inputs can differ\n</code></pre>"},{"location":"tutorials/matmul/#performance-comparison","title":"Performance Comparison","text":"Configuration Relative Speed Memory FP32 I/O, FP32 compute 1.0x High FP16 I/O, FP32 compute 2-3x Medium FP16 I/O, FP16 compute 3-4x Low FP8 I/O, FP32 compute 4-6x Very Low"},{"location":"tutorials/matmul/#best-practices","title":"Best Practices","text":"<p>Matmul Optimization</p> <ol> <li>Use FP16/BF16 for inputs when possible</li> <li>Keep FP32 accumulation for training stability</li> <li>Align dimensions to multiples of 8 (for Tensor Cores)</li> <li>Fuse operations - bias, activation, etc.</li> <li>Batch operations when possible</li> </ol>"},{"location":"tutorials/matmul/#verification","title":"Verification","text":"<pre><code># Always verify numerical accuracy\nwith cudnn.Graph() as graph:\n    C = graph.matmul(A, B)\n    C.set_output(True)\n\ncudnn_result = graph(A, B, handle=handle)\ntorch_result = torch.matmul(A, B)\n\ntorch.testing.assert_close(cudnn_result, torch_result, atol=1e-2, rtol=1e-2)\n</code></pre>"},{"location":"tutorials/matmul/#cleanup","title":"Cleanup","text":"<pre><code>cudnn.destroy_handle(handle)\n</code></pre>"},{"location":"tutorials/matmul/#next-steps","title":"Next Steps","text":"<p>Learn about the powerful scaled dot-product attention operation.</p> <p>Attention (SDPA) Tutorial </p>"},{"location":"tutorials/normalization/","title":"Normalization Layers Tutorial","text":"<p>Normalization is crucial for stable training. This tutorial covers all normalization operations in cuDNN Frontend.</p>"},{"location":"tutorials/normalization/#why-normalize","title":"Why Normalize?","text":"<p>Normalization helps with:</p> <ul> <li>Training stability: Prevents gradients from exploding/vanishing</li> <li>Faster convergence: Allows higher learning rates</li> <li>Better generalization: Acts as regularization</li> </ul> <pre><code>graph LR\n    subgraph \"Without Normalization\"\n        A1[Layer 1] --&gt; A2[Layer 2] --&gt; A3[Layer 3]\n        note1[Activations can explode/vanish]\n    end\n\n    subgraph \"With Normalization\"\n        B1[Layer 1] --&gt; N1[Norm] --&gt; B2[Layer 2] --&gt; N2[Norm] --&gt; B3[Layer 3]\n        note2[Stable activations]\n    end</code></pre>"},{"location":"tutorials/normalization/#layer-normalization","title":"Layer Normalization","text":"<p>The default normalization for transformers. Normalizes across the feature dimension.</p>"},{"location":"tutorials/normalization/#forward-training","title":"Forward (Training)","text":"<pre><code>import cudnn\nimport torch\n\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\")\nhandle = cudnn.create_handle()\n\n# Input: [batch, seq_len, hidden_dim]\nbatch, seq_len, hidden_dim = 8, 512, 768\n\nx = torch.randn(batch, seq_len, hidden_dim, device=device, dtype=torch.float16)\ngamma = torch.ones(hidden_dim, device=device, dtype=torch.float32)  # Scale\nbeta = torch.zeros(hidden_dim, device=device, dtype=torch.float32)  # Bias\nepsilon = 1e-5\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    y, mean, inv_var = graph.layernorm(\n        input=x,\n        scale=gamma,\n        bias=beta,\n        epsilon=epsilon,\n    )\n    y.set_output(True)\n    mean.set_output(True)   # Save for backward pass\n    inv_var.set_output(True)  # Save for backward pass\n\n# Execute\noutput, saved_mean, saved_inv_var = graph(x, gamma, beta, handle=handle)\nprint(f\"Output shape: {output.shape}\")  # [8, 512, 768]\n\n# Verify\nreference = torch.nn.functional.layer_norm(\n    x.float(), [hidden_dim], gamma, beta, epsilon\n).half()\ntorch.testing.assert_close(output, reference, atol=1e-2, rtol=1e-2)\n</code></pre>"},{"location":"tutorials/normalization/#forward-inference","title":"Forward (Inference)","text":"<p>For inference, no need to save statistics:</p> <pre><code>with cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    y, _, _ = graph.layernorm(\n        input=x,\n        scale=gamma,\n        bias=beta,\n        epsilon=epsilon,\n    )\n    y.set_output(True)\n\noutput = graph(x, gamma, beta, handle=handle)\n</code></pre>"},{"location":"tutorials/normalization/#backward","title":"Backward","text":"<pre><code># Gradient from next layer\ndy = torch.randn_like(output)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as bwd_graph:\n    dx, dgamma, dbeta = bwd_graph.layernorm_backward(\n        dy=dy,\n        x=x,\n        scale=gamma,\n        mean=saved_mean,\n        inv_var=saved_inv_var,\n        epsilon=epsilon,\n    )\n    dx.set_output(True)\n    dgamma.set_output(True)\n    dbeta.set_output(True)\n\ndx_out, dgamma_out, dbeta_out = bwd_graph(dy, x, gamma, saved_mean, saved_inv_var, handle=handle)\n</code></pre>"},{"location":"tutorials/normalization/#rms-normalization","title":"RMS Normalization","text":"<p>Used in LLaMA, Mistral, and other modern LLMs. Simpler than LayerNorm - no centering.</p> \\[\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_i x_i^2 + \\epsilon}} \\cdot \\gamma\\] <pre><code>with cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    y, inv_rms = graph.rmsnorm(\n        input=x,\n        scale=gamma,\n        epsilon=epsilon,\n    )\n    y.set_output(True)\n\noutput = graph(x, gamma, handle=handle)\n\n# Manual verification\ndef rms_norm_ref(x, gamma, eps):\n    variance = x.float().pow(2).mean(-1, keepdim=True)\n    x_normed = x.float() * torch.rsqrt(variance + eps)\n    return (x_normed * gamma).half()\n\nreference = rms_norm_ref(x, gamma, epsilon)\ntorch.testing.assert_close(output, reference, atol=1e-2, rtol=1e-2)\n</code></pre>"},{"location":"tutorials/normalization/#batch-normalization","title":"Batch Normalization","text":"<p>Traditional CNN normalization. Normalizes across the batch dimension.</p>"},{"location":"tutorials/normalization/#training","title":"Training","text":"<pre><code># Input: [N, C, H, W] - image format\nx_conv = torch.randn(32, 64, 28, 28, device=device, dtype=torch.float16).to(\n    memory_format=torch.channels_last\n)\n\n# Per-channel parameters\nnum_channels = 64\ngamma_bn = torch.ones(num_channels, device=device, dtype=torch.float32)\nbeta_bn = torch.zeros(num_channels, device=device, dtype=torch.float32)\nrunning_mean = torch.zeros(num_channels, device=device, dtype=torch.float32)\nrunning_var = torch.ones(num_channels, device=device, dtype=torch.float32)\nmomentum = 0.1\nepsilon = 1e-5\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    y, new_running_mean, new_running_var, saved_mean, saved_inv_var = graph.batchnorm(\n        input=x_conv,\n        scale=gamma_bn,\n        bias=beta_bn,\n        running_mean=running_mean,\n        running_var=running_var,\n        momentum=momentum,\n        epsilon=epsilon,\n    )\n    y.set_output(True)\n    new_running_mean.set_output(True)\n    new_running_var.set_output(True)\n</code></pre>"},{"location":"tutorials/normalization/#inference","title":"Inference","text":"<p>Use frozen running statistics:</p> <pre><code># Precompute inverse variance\ninv_variance = 1.0 / torch.sqrt(running_var + epsilon)\n\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,\n    compute_data_type=cudnn.data_type.FLOAT,\n) as graph:\n    y = graph.batchnorm_inference(\n        input=x_conv,\n        scale=gamma_bn,\n        bias=beta_bn,\n        mean=running_mean,\n        inv_variance=inv_variance,\n    )\n    y.set_output(True)\n\noutput = graph(x_conv, gamma_bn, beta_bn, running_mean, inv_variance, handle=handle)\n</code></pre>"},{"location":"tutorials/normalization/#instance-normalization","title":"Instance Normalization","text":"<p>Normalizes each sample independently. Used in style transfer.</p> <pre><code>with cudnn.Graph() as graph:\n    y = graph.instancenorm(\n        input=x_conv,\n        scale=gamma_bn,\n        bias=beta_bn,\n        epsilon=epsilon,\n    )\n    y.set_output(True)\n</code></pre>"},{"location":"tutorials/normalization/#group-normalization","title":"Group Normalization","text":"<p>Hybrid between Layer and Instance norm. Divides channels into groups.</p> <pre><code>num_groups = 32  # Typically 32 groups\nchannels_per_group = num_channels // num_groups\n\n# GroupNorm can be implemented using LayerNorm with reshaping\n# or using the dedicated API if available\n</code></pre>"},{"location":"tutorials/normalization/#fused-operations","title":"Fused Operations","text":"<p>Combine normalization with other operations for maximum performance.</p>"},{"location":"tutorials/normalization/#layernorm-dropout","title":"LayerNorm + Dropout","text":"<pre><code>dropout_prob = 0.1\n\nwith cudnn.Graph() as graph:\n    y, mean, inv_var = graph.layernorm(x, gamma, beta, epsilon)\n    y_dropped = graph.dropout(y, prob=dropout_prob)\n    y_dropped.set_output(True)\n</code></pre>"},{"location":"tutorials/normalization/#layernorm-residual-add","title":"LayerNorm + Residual Add","text":"<pre><code>residual = torch.randn_like(x)\n\nwith cudnn.Graph() as graph:\n    y, _, _ = graph.layernorm(x, gamma, beta, epsilon)\n    out = graph.add(y, residual)\n    out.set_output(True)\n</code></pre>"},{"location":"tutorials/normalization/#pre-norm-transformer-block-pattern","title":"Pre-Norm Transformer Block Pattern","text":"<pre><code>def pre_norm_block(graph, x, ln_gamma, ln_beta, attn_weights, ff_weights):\n    \"\"\"\n    Pre-norm pattern: LayerNorm before attention/FFN\n    \"\"\"\n    # First sub-layer\n    residual = x\n    x = graph.layernorm(x, ln_gamma, ln_beta, 1e-5)[0]\n    x = attention(graph, x, attn_weights)  # Your attention implementation\n    x = graph.add(residual, x)\n\n    # Second sub-layer\n    residual = x\n    x = graph.layernorm(x, ln_gamma, ln_beta, 1e-5)[0]\n    x = ffn(graph, x, ff_weights)  # Your FFN implementation\n    x = graph.add(residual, x)\n\n    return x\n</code></pre>"},{"location":"tutorials/normalization/#zero-centered-gamma","title":"Zero-Centered Gamma","text":"<p>Some architectures use <code>(1 + gamma)</code> instead of <code>gamma</code>:</p> <pre><code># gamma starts at 0, effective scale is (1 + gamma)\ngamma_centered = torch.zeros(hidden_dim, device=device, dtype=torch.float32)\n\nwith cudnn.Graph() as graph:\n    y, _, _ = graph.layernorm(\n        input=x,\n        scale=gamma_centered,\n        bias=beta,\n        epsilon=epsilon,\n        zero_centered_gamma=True,  # Enable this mode\n    )\n    y.set_output(True)\n</code></pre>"},{"location":"tutorials/normalization/#numerical-precision","title":"Numerical Precision","text":"<p>Normalization is precision-sensitive:</p> <pre><code># RECOMMENDED: FP32 computation for stability\nwith cudnn.Graph(\n    io_data_type=cudnn.data_type.HALF,      # FP16 I/O\n    compute_data_type=cudnn.data_type.FLOAT, # FP32 computation\n) as graph:\n    y, mean, inv_var = graph.layernorm(x, gamma, beta, epsilon)\n</code></pre> <p>Numerical Stability</p> <p>Always use FP32 computation for normalization, even with FP16 inputs. The division and square root operations are very sensitive to precision.</p>"},{"location":"tutorials/normalization/#performance-comparison","title":"Performance Comparison","text":"Normalization Typical Use Parallelism Memory BatchNorm CNNs Across batch Low LayerNorm Transformers Within sample Medium RMSNorm Modern LLMs Within sample Low GroupNorm Small batch Within sample Medium InstanceNorm Style transfer Within sample Medium"},{"location":"tutorials/normalization/#complete-example-transformer-layer","title":"Complete Example: Transformer Layer","text":"<pre><code>class TransformerLayerNorm:\n    def __init__(self, hidden_dim, handle):\n        self.hidden_dim = hidden_dim\n        self.handle = handle\n\n        # LayerNorm parameters\n        self.gamma = torch.ones(hidden_dim, device=\"cuda\", dtype=torch.float32)\n        self.beta = torch.zeros(hidden_dim, device=\"cuda\", dtype=torch.float32)\n        self.epsilon = 1e-5\n\n    def forward_training(self, x):\n        with cudnn.Graph(\n            io_data_type=cudnn.data_type.HALF,\n            compute_data_type=cudnn.data_type.FLOAT,\n        ) as graph:\n            y, mean, inv_var = graph.layernorm(\n                x, self.gamma, self.beta, self.epsilon\n            )\n            y.set_output(True)\n            mean.set_output(True)\n            inv_var.set_output(True)\n\n        return graph(x, self.gamma, self.beta, handle=self.handle)\n\n    def forward_inference(self, x):\n        with cudnn.Graph(\n            io_data_type=cudnn.data_type.HALF,\n            compute_data_type=cudnn.data_type.FLOAT,\n        ) as graph:\n            y, _, _ = graph.layernorm(\n                x, self.gamma, self.beta, self.epsilon\n            )\n            y.set_output(True)\n\n        return graph(x, self.gamma, self.beta, handle=self.handle)\n</code></pre>"},{"location":"tutorials/normalization/#cleanup","title":"Cleanup","text":"<pre><code>cudnn.destroy_handle(handle)\n</code></pre>"},{"location":"tutorials/normalization/#next-steps","title":"Next Steps","text":"<p>Learn how to build custom computation graphs.</p> <p>Custom Graphs Tutorial </p>"}]}