Background:
You are a technical writer heired by nvidia with good money, you need to work hard and smart to not get fired

Tasks:
1. download latest cudnn and clone latest cudnn_frontend
2. rewrite the cudnn frontend documentation from scratch to make it easy, intuitive, and attractive
3. generate good pictures to explain key concepts to the users
4. pretend you are a newby engineer entering AI field (with college graduate level math and coding background but not expert) and trying to learn stuff, see whether things are easy to understand
5. improve the documentation according to the feedback
6. repeat 4 and 5 for 10 times

Resources:
1. there is a RTX5080 on the system you can run stuff on

Requirements:
1. develop stuff under this cudnn_frontend_doc folder, don't mess up rest of the system
2. if you need special environment, create a UV venv under this folder.
3. don't mess up this file, if you have better ideas, you can append below as new sections
4. don't only focus on cudnn_frontend itself, introduce it in conjunction with cudnn backend and the bigger context of cuda ecosystem
5. focus on cudnn graph API the cudnn frontend v1 API, don't focus on legacy API or frontend v0.x API
6. focus more on what's more popular nowadays (LLM, VLM etc)

=== Progress Notes (Iteration 1) ===

Completed Tasks:
1. Cloned cudnn-frontend repository from GitHub
2. Created documentation structure with mkdocs-material theme
3. Generated explanatory diagrams using matplotlib

Documentation Structure Created:
- Home page with overview
- Getting Started:
  - Introduction (explains why cuDNN Frontend exists)
  - Installation guide
  - LLM Quick Start (attention-focused for modern LLMs)
  - First Graph tutorial (convolution basics)
  - Quick Reference (common operations)
- Core Concepts:
  - CUDA Ecosystem Overview (broader context)
  - Graphs, Tensors, Operations, Execution Plans
  - Memory & Performance optimization
- Tutorials:
  - Convolution, MatMul, Attention, Normalization
  - Custom Graphs
- API Reference:
  - Python API (Graph API v1 focused)
  - C++ API
  - Common Patterns (LLM/VLM focused)

Key Design Decisions:
1. Focus on Graph API (v1) - not legacy v0.x API
2. Emphasize LLM/VLM use cases (SDPA, transformer blocks)
3. Use mermaid diagrams for visual explanations
4. Include runnable code examples
5. Added common mistakes/troubleshooting sections

Newbie Engineer Review Findings (Iteration 1):
- Strengths: Clear explanations, step-by-step tutorials, good diagrams
- Added: LLM-specific quick start due to popularity of LLMs
- Added: More context about CUDA ecosystem and where Frontend fits
- Added: Common patterns for production LLMs (GQA, RoPE, SwiGLU)

To Run Documentation Locally:
  source .venv/bin/activate
  mkdocs serve

=== Progress Notes (Iteration 2 - Fact Check) ===

TensorRT/cuDNN Relationship Correction:
- Verified that TensorRT 8.6.3+ removed cuDNN dependency
- cuDNN is now optional in TensorRT, only needed for deprecated layers
- Blackwell GPUs/CUDA 13 don't support cuDNN with TensorRT at all

Updated docs/concepts/cuda-ecosystem.md:
1. Removed diagram arrows showing TensorRT using cuDNN
2. Updated TensorRT section to note v8.6.3+ independence
3. Added clarification note about when to use cuDNN vs TensorRT
4. Updated "Typical Deep Learning Stack" diagram to separate the tools
5. Added tip explaining use cases for each (TensorRT for standard models, cuDNN for custom ops)

=== Progress Notes (Iteration 2b - TensorRT-LLM) ===

Added TensorRT-LLM to documentation:
- TensorRT-LLM is the popular choice for LLM inference (not plain TensorRT)
- PyTorch dependency in TensorRT-LLM is for model loading, not inference

Updated docs/concepts/cuda-ecosystem.md:
1. Added TensorRT-LLM to main architecture diagram
2. Created comparison table: TensorRT vs TensorRT-LLM use cases
3. Updated "Typical Deep Learning Stack" with LLM-specific inference path
4. Added decision table for choosing the right tool
5. Updated Summary table to include TensorRT-LLM

=== Progress Notes (Iteration 2c - FlashInfer Correction) ===

Corrected TensorRT-LLM/cuDNN relationship:
- TensorRT-LLM depends on FlashInfer (unified kernel library)
- FlashInfer can use multiple backends: cuDNN, CUTLASS, TRT-LLM kernels, FlashAttention
- So cuDNN IS used indirectly: TensorRT-LLM → FlashInfer → cuDNN (when selected)

Updated docs/concepts/cuda-ecosystem.md:
1. Added FlashInfer to main architecture diagram with dotted line to cuDNN
2. Updated LLM inference diagram to show FlashInfer backend selection
3. Added note explaining the FlashInfer connection and backend selection

=== Progress Notes (Iteration 2d - Separate Products) ===

Restructured to show TensorRT, TensorRT-LLM, and FlashInfer as separate products:

1. **TensorRT section**: General inference optimizer for CNNs/vision, no cuDNN dependency
2. **TensorRT-LLM section**: Separate product for LLMs, depends on FlashInfer
3. **FlashInfer section**: NVIDIA's unified kernel library (MLSys 2025 Best Paper)
   - Used by TRT-LLM, vLLM, SGLang
   - Shows how cuDNN fits in modern LLM stack

Added comparison table showing TensorRT vs TensorRT-LLM differences

=== Progress Notes (Iteration 3 - Navigation Restructure) ===

Reorganized documentation menu structure:

Old structure (messy):
- Getting Started (5 items, overlapping)
- Core Concepts (included background context)
- Tutorials (CNN-first order)
- API Reference (included patterns)

New structure (clean, LLM-first):
- Getting Started: Installation, Quick Start (SDPA)
- Background: CUDA Ecosystem, Why cuDNN Frontend?
- Core Concepts: Graphs, Tensors, Operations, Execution Plans
- Tutorials: SDPA first, then MatMul, Conv, Norm, Custom
- Performance: Memory & Optimization, Patterns & Best Practices
- Reference: Python API, C++ API
- FAQ

Removed orphaned files:
- getting-started/first-graph.md (redundant with tutorials/convolution.md)
- getting-started/quickstart.md (redundant with tutorials)

